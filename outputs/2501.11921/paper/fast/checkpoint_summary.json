{
  "content_type": "paper",
  "content": {
    "paper_info": "**Title**: Goal-oriented Transmission Scheduling: Structure-guided DRL with a Unified Dual On-policy and Off-policy Approach\n**Authors**: Jiazheng Chen, Wanchun Liu*",
    "figures": "",
    "tables": "",
    "equations": "",
    "motivation": "以下是基于您提供的文本整理的结构化研究动机：\n\n## 研究问题\n**多设备多信道无线通信系统中的面向目标（Goal-oriented）传输调度问题。**\n该研究聚焦于在 $N$ 个边缘设备共享 $M$ 个有限衰落信道（$M < N$）的场景下，如何优化调度策略以优先实现应用驱动的目标（如数据新鲜度和系统性能），而非传统的位级准确度。\n*   **核心目标：** 寻找最优调度策略 ($\\pi$)，以最小化所有设备的长期期望总成本。\n*   **具体案例：** 在远程状态估计系统中，这意味着最小化与信息年龄（AoI）直接相关的估计状态均方误差（MSE）。\n\n## 现有方法的局限性\n- **传统启发式与动态规划方法：**\n    *   **启发式方法（如 Whittle’s Index）：** 虽然计算效率高，但无法保证调度决策的最优性。\n    *   **传统动态规划（如值迭代和策略迭代）：** 在现代面向目标的系统中计算上不可行。由于高维状态和动作空间引发的“维度灾难”，即使对于小型系统（如10设备5信道，动作空间超3万个）也难以处理。\n- **标准深度强化学习（DRL）算法：**\n    *   **离策（Off-Policy）DRL（如 DQN, DDPG）：** 尽管样本效率高，但存在训练不稳定和偏差问题。当前策略与回放缓冲区中旧数据之间的差异会导致训练发散，尤其在大型动态系统中。在 40 设备 20 信道的大规模场景下无法收敛。\n    *   **同策（On-Policy）DRL（如 PPO, TRPO）：** 稳定性较好但样本效率极低。每次更新后丢弃数据导致探索不足，使智能体易陷入局部最优。在大规模系统中表现出明显的性能损失。\n- **现有的结构增强型 DRL 研究：**\n    *   **范围局限：** 大多仅依赖离策 DRL（如 DDPG），继承了其不稳定性，且仅限于小规模系统（通常最多 20 个传感器）。\n    *   **缺乏领域深度见解：** 许多应用采用“暴力优化”，未深入研究最优策略的数学性质，导致发现最优解的效率低下。\n    *   **理论属性缺失：** 在本研究之前，关于 AoI 状态的最优状态值函数的“渐近凸性”在传输调度领域尚未被探索或证明。\n\n## 研究空白\n- **计算可行性与最优性的平衡缺口：** 传统方法要么计算太慢（DP），要么不够精确（启发式），急需一种既能保证性能又能处理大规模问题的方案。\n- **DRL 稳定性与效率的权衡缺口：** 现有 DRL 在样本效率（离策）与训练稳定性（同策）之间存在天然矛盾，缺乏能结合两者优点的统一框架。\n- **理论导向学习的缺失：** 现有研究缺乏将通信问题的物理特性或数学结构（如单调性、凸性）融入 DRL 训练过程的有效手段。\n- **大规模系统的扩展性缺口：** 缺乏能在 40 设备 20 信道等复杂大规模环境下收敛并保持高性能的调度算法。\n\n## 关键挑战\n- **高维状态与动作空间：**\n    *   **状态空间：** 必须跟踪每个设备的 AoI 和每个设备-信道对的信道状态，随设备数量呈指数级增长。\n    *   **动作空间：** 调度器需决定设备与信道的匹配。对于 $N$ 设备 $M$ 信道，动作数为 $N! / (N-M)!$。10设备5信道即产生 30,240 种动作，使传统 MDP 求解器失效。\n- **标准 DRL 算法的固有缺陷：** 离策算法的训练不稳定/偏差问题与同策算法的样本低效/易陷入局部最优问题。\n- **缺乏领域特定指导：** 若不结合单调性或凸性等结构化属性，单纯依靠“暴力”优化难以在复杂环境中找到理论最优策略。\n- **可扩展性压力：** 当系统规模从 20 个传感器扩展到 40 个设备及 20 个信道时，基准算法往往面临不收敛或性能严重下降的挑战。\n\n## 背景背景\n- **研究背景：** 随着物联网和远程控制系统的发展，通信目标从单纯的“传输比特”转向“满足特定应用需求”（如 remote state estimation）。\n- **技术背景：** 现有的 DRL 虽被用于解决复杂调度，但由于忽略了通信问题的数学结构（如 AoI 的演进规律和信道衰落特性），在效率和可扩展性上遇到瓶颈。\n- **创新切入点：** 本研究通过数学证明最优解的结构属性（单调性、渐近凸性、贪婪结构），并以此引导 DRL 训练（SUDO-DRL 算法），旨在弥合理论最优性与实际 DRL 落地之间的鸿沟。",
    "solution": "## 框架概述\n**SUDO-DRL (Structure-guided Unified Dual On-off policy DRL)** 是一个混合深度强化学习框架，旨在解决大规模目标导向通信系统中的复杂传输调度问题。该框架通过结合 **PPO (Proximal Policy Optimization)** 的在线策略（on-policy）训练稳定性与 **SAC (Soft Actor-Critic)** 的离线策略（off-policy）采样效率，并利用推导出的最优调度策略理论结构属性（单调性、渐近凸性、贪婪结构）来指导神经网络的训练，从而在处理高维状态空间时实现更高的性能和更快的收敛。\n\n## 关键组成部分\n\n1.  **理论结构属性推导 (Theoretical Foundation)**：\n    *   作为算法设计的基石，推导了最优解的四个关键属性：V函数的单调性（随AoI和信道状态增加）、V函数的渐近凸性（随AoI状态）、最优策略的单调性（随信道状态）、以及针对共址设备的渐近贪婪结构（高AoI设备强制调度）。\n\n2.  **结构属性评估框架 (Structural Property Evaluation Framework)**：\n    *   将理论属性转化为量化指标，用于评分轨迹：\n        *   **CM Score (Critic-Monotonicity)**：评估Critic网络是否符合V函数单调性。\n        *   **CC Score (Critic-Convexity)**：评估Critic网络是否符合渐近凸性。\n        *   **AM Score (Actor-Monotonicity)**：评估Actor网络决策是否符合信道状态单调性。\n\n3.  **统一双损失函数 (Unified Dual Loss Function)**：\n    *   **Critic Loss**：结合了当前轨迹的TD误差（在线）和回放池数据的TD误差（离线），并加入结构违反惩罚项（Penalty terms）。\n    *   **Actor Loss**：结合了PPO的裁剪损失（在线）与熵正则化损失（离线）。\n\n4.  **结构引导的回放池管理 (Structure-Guided Replay Buffer Management)**：\n    *   **选择性存储**：仅存储结构评分（CM, CC, AM）达到或超过历史平均水平的轨迹。\n    *   **优先级采样**：基于结构评分和数据新近度（Recency）计算优先级指标进行采样。\n\n5.  **结构引导的预训练 (Structure-Guided Pre-training)**：\n    *   利用“渐近贪婪结构”定理，在初始阶段选择强制调度集（高AoI设备）的动作，为模型提供高质量的初始策略。\n\n## 数学表述\n\n### 1. 系统模型与目标\n- **目标函数 (Problem 1)**：\n  $$\\min_{\\pi} \\lim_{T \\to \\infty} \\mathbb{E}^\\pi \\left[ \\sum_{t=1}^{T} \\sum_{n=1}^{N} \\gamma^t c_n(\\delta_{n,t}) \\right]$$\n  - $\\gamma$：折扣因子 $\\in (0,1)$。\n  - $c_n(\\delta_{n,t})$：基于AoI $\\delta$ 的应用特定成本函数。\n- **AoI 动态方程 (Eq. 4)**：\n  $$\\delta_{n,t+1} = \\begin{cases} 1, & \\text{若传输成功} \\\\ \\delta_{n,t} + 1, & \\text{否则} \\end{cases}$$\n- **信道约束 (Eq. 3)**：\n  $$\\sum_{n=1}^{N} \\mathbb{1}(a_{n,t} = m) = 1, \\quad \\sum_{m=1}^{M} \\mathbb{1}(a_{n,t} = m) \\leq 1$$\n  - $a_{n,t}$：设备 $n$ 在时间 $t$ 分配的信道。\n\n### 2. 结构属性评分与惩罚\n- **Critic 单调性惩罚 (AoI & Channel)**：\n  $$\\hat{V}_{\\mathrm{AoI}} = \\max(0, v(\\mathbf{s}; \\boldsymbol{\\nu}) - v(\\hat{\\mathbf{s}}_{(n)}; \\boldsymbol{\\nu}))$$\n  $$\\hat{V}_{\\mathrm{Ch}} = \\max(0, v(\\mathbf{s}; \\boldsymbol{\\nu}) - v(\\hat{\\mathbf{s}}_{(n,m)}; \\boldsymbol{\\nu}))$$\n- **Critic 凸性惩罚 (AoI)**：\n  $$\\check{V}_{\\mathrm{AoI}} = \\max(0, 2v(\\mathbf{s}; \\boldsymbol{\\nu}) - (v(\\check{\\mathbf{s}}_{(n)}; \\boldsymbol{\\nu}) + v(\\hat{\\mathbf{s}}_{(n)}; \\boldsymbol{\\nu})))$$\n- **Actor 单调性评分**：\n  $$\\acute{\\Lambda}_{\\mathrm{Ch},n} = \\mathbb{1}(a_n \\neq 0 \\text{ 且 } a_{\\mathrm{Ch},n} \\neq a_n)$$\n\n### 3. 统一损失函数\n- **Critic 统一损失 (Eq. 22 & 31)**：\n  $$L_{\\mathrm{SUDO}}(\\boldsymbol{\\nu}) = L_{\\mathrm{On}}(\\boldsymbol{\\nu}) + \\beta_1 L_{\\mathrm{Off}}(\\boldsymbol{\\nu})$$\n  $$L_{\\mathrm{On}}(\\boldsymbol{\\nu}) = \\frac{1}{B_1} \\sum_{l=1}^{B_1} \\mathrm{TD}_l^2 + \\text{Structural Penalties}$$\n- **Actor 统一损失 (Eq. 23 & 38)**：\n  $$L_{\\mathrm{SUDO}}(\\varphi) = L_{\\mathrm{On}}(\\varphi) + \\beta_2 L_{\\mathrm{Off}}(\\varphi)$$\n  $$L_{\\mathrm{Off}}(\\varphi) = \\frac{1}{B_2} \\sum_{b=1}^{B_2} \\left[ \\varpi \\log(\\pi(\\tilde{\\mathbf{a}}_b | \\mathbf{s}_b; \\varphi)) + (c_b + \\gamma v(\\tilde{\\mathbf{s}}_{b+1}; \\boldsymbol{\\nu})) \\right]$$\n\n### 4. 采样优先级 (Eq. 35 & 36)\n- **优先级指标**：$p_u = \\mathrm{CM}_u + \\mathrm{CC}_u + \\mathrm{AM}_u$\n- **采样概率**：$P_b \\triangleq \\frac{p_b \\cdot \\varrho^b}{\\sum_{b=1}^R (p_b \\cdot \\varrho^b)}$\n  - $\\varrho$：新近度衰减率。\n\n## 技术流程\n\n1.  **初始化**：设置环境参数（$N$ 设备, $M$ 信道）、神经网络权重（Actor $\\varphi$, Critic $\\nu$）及回放池。\n2.  **预训练阶段**：利用定理5（渐近贪婪结构）进行动作选择，收集高质量初始轨迹。\n3.  **轨迹采样**：Actor网络与环境交互生成轨迹；若在预训练后，则根据当前策略采样。\n4.  **结构评估**：对当前轨迹计算 CM、CC、AM 评分。\n5.  **回放池管理**：\n    *   对比当前评分与历史平均评分。\n    *   若评分优异，则存入回放池并更新优先级指标 $p$。\n6.  **损失计算**：\n    *   计算在线损失 $L_{On}$（含结构违反惩罚）。\n    *   从回放池按优先级采样，计算离线损失 $L_{Off}$。\n    *   合并为统一损失 $L_{SUDO}$。\n7.  **参数更新**：使用 Adam 优化器更新 $\\varphi$ 和 $\\nu$。\n8.  **重复**：循环执行步骤3-7直至收敛。\n\n## 实现细节\n\n*   **网络结构**：Actor 和 Critic 均为 3 层隐藏层的全连接神经网络。\n*   **超参数**：\n    *   折扣因子 $\\gamma = 0.99$。\n    *   PPO 裁剪参数 $\\epsilon = 0.2$。\n    *   统一损失权重 $\\beta_1, \\beta_2 = 0.9$。\n    *   学习率：Critic = 0.001, Actor = 0.0001。\n    *   Batch Size = 128，回放池容量 = 200 轨迹。\n*   **信道模型**：i.i.d. 块衰落信道，5 级量化状态，丢包率范围 0.01 至 0.2。\n*   **扩展性**：支持多达 40 设备和 20 信道的规模。\n\n## 关键创新\n\n*   **结构引导 (Structure-guided)**：首次将 AoI 调度问题的渐近凸性和单调性等数学证明直接嵌入到 DRL 的损失函数和数据筛选中。\n*   **统一双策略 (Unified Dual On-off)**：通过统一损失函数桥接了 PPO 和 SAC，解决了传统离线策略方法在大规模调度中难以收敛的问题。\n*   **渐近贪婪预训练**：利用理论发现的最优策略边界情况（定理5）加速冷启动过程，减少 40% 的收敛时间。",
    "results": "## 数据集 / 基准测试\n- 姓名：远程状态估计系统仿真环境 (Remote State Estimation System)\n- 规模：10,000 训练回合 (Episodes)，每回合 128 步 (Steps)；评估基于 20,000 步的经验模拟。\n- 类别/分组：\n  - 小规模 (Small-scale)：10 台设备, 5 个信道\n  - 中规模 (Medium-scale)：20 台设备, 10 个信道\n  - 大规模 (Large-scale)：30 台设备, 15 个信道\n  - 超大规模 (Very Large-scale)：40 台设备, 20 个信道\n- 其他详情：\n  - 状态维度：$N + (N \\times M)$，最大输入维度为 840 (40台设备场景)。\n  - 信道状态：量化为 $\\bar{g} = 5$ 级。\n  - 数据包丢弃率：$\\{0.2, 0.15, 0.1, 0.05, 0.01\\}$。\n  - 硬件：Intel Core i7 9700 CPU, 32GB RAM, NVIDIA RTX 3060Ti GPU。\n\n## 评估指标\n- Empirical Average Sum MSE Cost：衡量远程估计的平均和均方误差，是目标导向通信的核心指标。\n- Convergence Time/Speed：达到稳定成本所需的训练回合数。\n- Critic-Monotonicity (CM) Score：衡量 Critic 神经网络是否符合关于信道和 AoI 状态的单调性证明（0-100分）。\n- Critic-Convexity (CC) Score：衡量 Critic 神经网络是否符合价值函数关于 AoI 状态的渐进凸性证明（0-100分）。\n- Actor-Monotonicity (AM) Score：衡量策略关于信道状态的单调性（0-100分）。\n- Success Rate：算法在特定系统规模下是否能成功收敛。\n\n## 主要结果\n<table>\n<tr><th>Method</th><th>Small (10,5)</th><th>Medium (20,10)</th><th>Large (30,15)</th><th>Very Large (40,20)</th><th>Convergence Speed</th></tr>\n<tr><td>DDPG</td><td>Works</td><td>Fails (—)</td><td>Fails (—)</td><td>Fails (—)</td><td>Slow/Unstable</td></tr>\n<tr><td>SE-DDPG [21]</td><td>Best Performance</td><td>Works</td><td>Fails (—)</td><td>Fails (—)</td><td>Moderate</td></tr>\n<tr><td>MRI-DDPG [22]</td><td>High Performance</td><td>Works</td><td>Fails (—)</td><td>Fails (—)</td><td>Moderate</td></tr>\n<tr><td>PPO</td><td>Poor Performance</td><td>Poor Performance</td><td>Works (High Cost)</td><td>Works (High Cost)</td><td>Baseline</td></tr>\n<tr><td>SUDO-DRL</td><td>High Performance</td><td>Best Performance</td><td>Best Performance</td><td>Best Performance</td><td>40% Faster than PPO</td></tr>\n</table>\n\n## 按类别/分段的表现\n<table>\n<tr><th>System Scale (N, M)</th><th>Para. Setting</th><th>PPO (MSE Cost)</th><th>SUDO-DRL (MSE Cost)</th><th>Improvement (%)</th></tr>\n<tr><td>(10, 5)</td><td>Para. 1</td><td>119.63</td><td>85.52</td><td>~28.5%</td></tr>\n<tr><td>(20, 10)</td><td>Para. 5</td><td>569.96</td><td>370.63</td><td>~35%</td></tr>\n<tr><td>(30, 15)</td><td>Para. 11</td><td>900.71</td><td>518.03</td><td>~42.5%</td></tr>\n<tr><td>(40, 20)</td><td>Para. 14</td><td>971.35</td><td>689.81</td><td>~29%</td></tr>\n<tr><td>(40, 20)</td><td>Para. 15</td><td>1291.54</td><td>994.80</td><td>~23%</td></tr>\n</table>\n\n## 消融研究\n<table>\n<tr><th>Variant</th><th>Avg. Sum MSE Cost</th><th>Convergence Time (Episodes)</th><th>Structural Scores (CM/CC/AM)</th></tr>\n<tr><td>PPO (Baseline)</td><td>~1000</td><td>Slowest (>5000)</td><td>Low (CC<80, AM<75)</td></tr>\n<tr><td>SUDO-DRL (w/o pre-training)</td><td>~700</td><td>~5000</td><td>High (100)</td></tr>\n<tr><td>SUDO-DRL (Full)</td><td><700</td><td>~3000 (40% reduction)</td><td>Perfect (100, Faster)</td></tr>\n</table>\n\n## 详细发现\n- 系统性能提升：SUDO-DRL 相比 PPO 基准算法，将系统性能（MSE 成本）提升了 25% 至 45%。\n- 收敛效率：通过结构引导的预训练阶段，收敛时间缩短了约 40%。\n- 可扩展性突破：SUDO-DRL 成功处理了 40 个设备和 20 个信道的规模，而所有离策 (Off-policy) 基准算法（DDPG, SE-DDPG, MRI-DDPG）在此规模下均无法收敛。\n- 结构一致性：SUDO-DRL 在 200 回合内即可达到 100 分的 Critic 凸性 (CC) 评分，而 PPO 始终低于 80 分。\n- 预训练增益：全功能 SUDO-DRL 的初始成本从约 800 开始，远低于无预训练版本的起始点。\n\n## 比较分析\n- 相比 PPO：PPO 虽然具有稳定性，但在高维空间中由于缺乏结构引导，性能显著落后于 SUDO-DRL（成本高出 25%-40%）。\n- 相比 DDPG 变体：在小规模（10, 5）下，SE-DDPG 等离策方法由于采样效率极高表现略优，但随着规模扩大，这些方法因无法处理高维动作空间而彻底失效，而 SUDO-DRL 通过统一的双策 (Dual On-off policy) 架构兼顾了稳定性和效率。\n\n## 主要贡献\n1. 理论证明的结构属性：首次证明了最优价值函数关于 AoI 状态具有渐进凸性 (Asymptotic Convexity)，并证明了关于信道和 AoI 状态的单调性。\n2. SUDO-DRL 算法框架：提出了一种结构引导的统一双策深度强化学习方法，结合了在策 (On-policy) 的稳定性和离策 (Off-policy) 的样本效率。\n3. 结构引导的数据存储方案：设计了一种基于结构评分的经验回放机制，仅存储符合理论属性的高质量轨迹。\n4. 渐进贪婪预训练：利用理论推导出的渐进贪婪结构（Theorem 5）进行策略初始化，显著加速了学习过程。\n\n## 新奇与创新\n- 凸性属性的应用：以往工作仅关注单调性，本文首次将渐进凸性引入传输调度 DRL 的结构引导中。\n- 统一损失函数：不同于传统的混合方法，本文使用统一的损失函数整合了 PPO 的稳定性和 SAC 的样本效率。\n- 结构评分系统：引入了 CM、CC 和 AM 三种评分指标，量化神经网络对数学结构的遵循程度，并将其直接用于数据筛选。\n\n## 局限性\n- 线性系统假设：实验主要基于线性时不变 (LTI) 系统模型，对于高度非线性的动态过程可能需要进一步验证。\n- 状态量化：信道状态被量化为 5 级，在连续信道状态下的表现尚未深入探讨。\n\n## 未来方向\n- 非线性代价函数：探索 SUDO-DRL 在更复杂的非线性目标函数下的适用性。\n- 分布式实现：研究如何将该集中式调度框架扩展到分布式边缘计算场景。\n\n## 更广泛的影响\n- 目标导向通信：该研究为 6G 网络中的目标导向 (Goal-oriented) 和语义通信提供了高效的资源调度方案，尤其是在大规模物联网 (IoT) 监测和远程控制领域具有应用潜力。",
    "contributions": "",
    "raw_rag_results": {
      "paper_info": [
        {
          "query": "List the paper title, author names and their institutional affiliations.",
          "answer": "**Title**: Goal-oriented Transmission Scheduling: Structure-guided DRL with a Unified Dual On-policy and Off-policy Approach\n**Authors**: Jiazheng Chen, Wanchun Liu*",
          "mode": "direct_markdown_extraction",
          "success": true
        }
      ],
      "figures": [
        {
          "query": "Describe the architecture or framework diagram. What are the main components shown?",
          "answer": "Based on the provided document, the architecture and framework of the **SUDO-DRL** (Structure-guided Unified Dual On-off policy DRL) algorithm are primarily illustrated in **Figure 2** and **Figure 3**.\n\nThe architecture is a hybrid system that integrates **on-policy stability** (based on PPO) with **off-policy sample efficiency** (based on SAC), guided by **theoretical structural properties**.\n\n### 1. Theoretical Structural Property Evaluation Framework (Fig. 2)\nThis component serves as the \"guide\" for the neural networks. It evaluates whether the NNs are adhering to the mathematical properties derived in the paper:\n*   **Critic NN Evaluation:** Assesses the estimated state-value function $v(s)$ for:\n    *   **Monotonicity:** Ensuring the value function increases with AoI and channel states (based on Lemma 1 and Theorem 1).\n    *   **Convexity:** Ensuring the value function is asymptotically convex with respect to AoI states (based on Theorem 3).\n*   **Actor NN Evaluation:** Assesses the sampled actions for:\n    *   **Monotonicity:** Ensuring scheduling decisions are consistent with improved channel states (based on Theorem 4).\n*   **Output Scores:** It generates three specific metrics: **CM** (Critic-Monotonicity), **CC** (Critic-Convexity), and **AM** (Actor-Monotonicity).\n\n### 2. SUDO-DRL Main Architecture (Fig. 3)\nThe main framework is divided into three functional areas that interact to update the Actor and Critic NNs:\n\n*   **Trajectory Sampling & Pre-training:**\n    *   The **Actor NN** interacts with the **Wireless Environment** to generate experience trajectories.\n    *   **Pre-training Phase:** Includes an \"Additional Greedy Action Selection\" block (based on Theorem 5) to provide a high-quality initial policy by prioritizing devices in the \"mandatory scheduling set.\"\n\n*   **On-Policy Part (Green Block):**\n    *   Uses the current trajectory to calculate the **Advantage function ($A_t$)** and **Cost-to-go ($C_t$)**.\n    *   **Structure-guided On-policy Loss ($L_{On}$):** Unlike standard PPO, this loss function includes a penalty term derived from the structural property evaluation metrics to force the Critic NN to respect monotonicity and convexity.\n\n*   **Off-Policy Part (Blue Block):**\n    *   **Structure-Guided Data Storage:** A decision gate checks if the current trajectory's structural scores (CM, CC, AM) meet specific constraints. If they do, the data is stored in the **Replay Buffer ($R$)**; otherwise, it is discarded.\n    *   **Structural Priority-based Sampling:** Transitions are sampled from the buffer based on a priority indicator ($p$) that favors trajectories with higher structural alignment and recency.\n    *   **Off-policy Loss ($L_{Off}$):** Calculates losses based on the sampled transitions to improve sample efficiency.\n\n*   **Unified Update:**\n    *   The final component is the **Unified Dual On-Off Policy Loss Function ($L_{SUDO}$)**, which sums the weighted on-policy and off-policy losses. This unified gradient is used to update the parameters of both the **Actor NN ($\\varphi$)** and the **Critic NN ($\\nu$)**.",
          "mode": "fast_direct_with_vision",
          "success": true
        }
      ],
      "tables": [
        {
          "query": "Show the main performance comparison table with all methods and metrics. Include exact numbers.",
          "answer": "Based on the document provided, the main performance comparison is presented in **Table II**, which evaluates the **Empirical Average Cost** (specifically the average sum Mean Square Error) across 16 different parameter settings and 4 system scales.\n\n### **Table II: Empirical Average Cost Comparison**\n\nThe table compares the proposed **SUDO-DRL** against four benchmarks: DDPG, SE-DDPG, MRI-DDPG, and PPO. (Note: \"—\" indicates the algorithm failed to converge at that scale).\n\n| System Scale (N, M) | Para. | DDPG | SE-DDPG [21] | MRI-DDPG [22] | PPO | SUDO-DRL |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **(10, 5)** | 1 | 89.26 | 77.14 | 84.00 | 119.63 | **85.52** |\n| | 2 | 98.87 | 87.30 | 90.28 | 123.01 | **95.82** |\n| | 3 | — | 106.60 | 119.55 | 195.37 | **121.31** |\n| | 4 | 83.12 | 78.21 | 80.88 | 120.93 | **80.68** |\n| **(20, 10)** | 5 | — | 357.14 | 369.14 | 569.96 | **370.63** |\n| | 6 | — | — | 445.87 | 584.37 | **426.29** |\n| | 7 | — | 407.20 | 441.73 | 731.94 | **417.90** |\n| | 8 | — | 290.72 | 307.94 | 376.16 | **308.91** |\n| **(30, 15)** | 9 | — | — | — | 805.45 | **519.78** |\n| | 10 | — | — | — | 739.97 | **575.34** |\n| | 11 | — | — | — | 900.71 | **518.03** |\n| | 12 | — | — | — | 901.42 | **551.97** |\n| **(40, 20)** | 13 | — | — | — | 1057.05 | **719.45** |\n| | 14 | — | — | — | 971.35 | **689.81** |\n| | 15 | — | — | — | 1291.54 | **994.80** |\n| | 16 | — | — | — | 1012.20 | **771.91** |\n\n### **Key Observations from the Data:**\n*   **Scalability:** Off-policy methods (DDPG, SE-DDPG, MRI-DDPG) fail to converge as the system scale increases to 30 devices or more. SUDO-DRL and PPO are the only methods that remain functional at the largest scale (40 devices, 20 channels).\n*   **Performance Gain:** In large-scale scenarios (40, 20), SUDO-DRL consistently reduces the average MSE cost by **25% to 40%** compared to the standard PPO algorithm.\n*   **Small-Scale Performance:** For the smallest scale (10, 5), SUDO-DRL performs comparably to advanced off-policy methods but is slightly outperformed by SE-DDPG, as off-policy methods are highly efficient at smaller dimensions.\n*   **Convergence:** According to the text and Figure 4, SUDO-DRL achieves **40% faster convergence** than the version without pre-training and significantly outperforms PPO in stability and final cost.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "Show the ablation study table with all variants and their results.",
          "answer": "Based on the provided document, there is **no specific \"ablation study table\"** that explicitly lists and compares the individual components of the SUDO-DRL algorithm (such as removing only the score scheme or only the pre-training).\n\nHowever, the document provides a **performance comparison table** (Table II) and a **training performance graph** (Fig. 4) that function as a partial ablation study by comparing the full SUDO-DRL against its variants and benchmarks.\n\n### 1. Performance Comparison (Table II)\nThis table compares the empirical average MSE cost across 16 different parameter settings and 4 system scales. It demonstrates how the structure-guided approach (SUDO-DRL) compares to standard on-policy (PPO) and off-policy (DDPG) methods.\n\n| System Scale (N, M) | PPO (Standard On-Policy) | SUDO-DRL (Proposed) | Improvement |\n| :--- | :--- | :--- | :--- |\n| (10, 5) | 119.63 - 195.37 | **85.52 - 121.31** | ~30-40% |\n| (20, 10) | 376.16 - 731.94 | **308.91 - 426.29** | ~20-40% |\n| (30, 15) | 739.97 - 901.42 | **518.03 - 575.34** | ~35-40% |\n| (40, 20) | 971.35 - 1291.54 | **689.81 - 994.80** | ~25-35% |\n\n*Note: DDPG-based variants (SE-DDPG, MRI-DDPG) failed to converge for scales larger than (20, 10).*\n\n### 2. Variant Comparison (Figure 4)\nFigure 4 specifically compares three versions of the algorithm to show the impact of the proposed structural guidance and pre-training:\n\n*   **PPO (Baseline):** Exhibits the highest average sum MSE (approx. 1000) and the slowest, most unstable convergence.\n*   **SUDO-DRL (without pre-training):** Shows the impact of the **Unified Dual On-Off Policy** and **Structural Property Evaluation**. It achieves a significantly lower cost (approx. 700) than PPO but takes about 5000 episodes to stabilize.\n*   **SUDO-DRL (Full):** Includes the **Structure-Guided Action Selection (Pre-training)**. This variant starts at a much lower cost (approx. 800) and converges 40% faster than the version without pre-training, ultimately reaching the lowest MSE (below 700).\n\n### 3. Structural Score Results (Figures 5, 6, and 7)\nThe paper evaluates the \"internal\" effectiveness of the structural guidance by comparing scores between PPO and SUDO-DRL:\n*   **Critic Monotonicity (CM):** SUDO-DRL reaches a perfect score of 100 almost immediately; PPO takes 1000 episodes.\n*   **Critic Convexity (CC):** SUDO-DRL reaches 100 within 200 episodes; PPO never exceeds 80.\n*   **Actor Monotonicity (AM):** SUDO-DRL reaches 100 by 2000 episodes; PPO plateaus below 75.\n\n**Summary of Results:** The \"ablation\" logic in the text concludes that SUDO-DRL improves system performance by **25% to 45%** and reduces convergence time by **40%** compared to state-of-the-art methods.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "Show any dataset statistics table with sizes, splits, and other details.",
          "answer": "Based on the provided document, the dataset statistics and experimental parameters are primarily detailed in **Table I** and the **Numerical Experiments** section (Section VI).\n\nThe \"dataset\" in this context refers to the training data generated by the DRL agent through interactions with a simulated environment (a remote state estimation system).\n\n### **Dataset and Training Statistics (from Table I)**\n\n| Hyperparameter / Statistic | Value |\n| :--- | :--- |\n| **Total Number of Episodes ($I$)** | 10,000 |\n| **Pre-training Episodes ($I_1$)** | $10 \\times N$ (e.g., 400 for a 40-device system) |\n| **Time Horizon per Episode ($T$)** | 128 steps |\n| **On-policy Batch Size ($B_1$)** | 128 |\n| **Off-policy Batch Size ($B_2$)** | 128 |\n| **Replay Buffer Size ($R$)** | 200 trajectories |\n| **Sampled States for Score Scheme ($K$)** | 50 |\n| **Tested AoI and Channel States ($\\Xi$)** | 4 |\n| **Past Trajectories for Avg. Score ($\\bar{u}$)** | 50 |\n\n### **System Scale and Environment Details (from Section VI-A and Table II)**\nThe experiments evaluate the algorithm across various system dimensions, which dictates the complexity of the state-action space:\n\n*   **System Scales ($N$ devices, $M$ channels):**\n    *   Small: (10, 5)\n    *   Medium: (20, 10)\n    *   Large: (30, 15)\n    *   Very Large: (40, 20)\n*   **State Space Dimensions:** $N + (N \\times M)$. For the largest system (40, 20), the input dimension is $40 + 800 = 840$.\n*   **Channel States:** Quantized into $\\bar{g} = 5$ levels.\n*   **Packet Drop Rates:** $\\{0.2, 0.15, 0.1, 0.05, 0.01\\}$.\n*   **Evaluation Split:** Performance is verified using an empirical average MSE over **20,000-step simulations** across 16 different parameter settings (Para. 1-16).\n\n### **Neural Network Architecture**\n*   **Type:** Fully connected NNs with **three hidden layers**.\n*   **Actor Output:** $2N$ (for SUDO-DRL and PPO).\n*   **Critic Output:** 1 (estimated optimal V function).\n\nThe document notes that for the largest scale (40 devices, 20 channels), benchmark off-policy algorithms like DDPG fail to converge, whereas the SUDO-DRL dataset management (selective storage based on structural scores) allows for stable training.",
          "mode": "fast_direct_with_vision",
          "success": true
        }
      ],
      "equations": [
        {
          "query": "What is the core model formulation or main equation? Show the exact formula and notation.",
          "answer": "The core model formulation for the goal-oriented transmission scheduling problem is a **Markov Decision Process (MDP)** designed to minimize the long-term expected sum of cost functions across multiple devices.\n\n### 1. Objective Function\nThe primary goal is to find an optimal scheduling policy $\\pi$ that minimizes the infinite-horizon expected discounted cost:\n$$\\min_{\\pi} \\lim_{T \\to \\infty} \\mathbb{E}^{\\pi} \\left[ \\sum_{t=1}^{T} \\sum_{n=1}^{N} \\gamma^{t} c_{n}(\\delta_{n,t}) \\right]$$\n**Notation:**\n*   $N$: Number of edge devices.\n*   $\\gamma \\in (0,1)$: Discount factor.\n*   $\\delta_{n,t}$: Age of Information (AoI) for device $n$ at time $t$.\n*   $c_{n}(\\delta_{n,t})$: Application-specific cost function (non-decreasing with respect to AoI).\n\n---\n\n### 2. MDP Components\nThe system is modeled using the following state, action, and transition definitions:\n\n*   **State ($\\mathbf{s}_t$):** Comprises the AoI of all devices and the current channel states.\n    $$\\mathbf{s}_t \\triangleq (\\delta_t, \\mathbf{G}_t) \\in \\mathcal{S}$$\n    Where $\\delta_t = \\{\\delta_{1,t}, \\dots, \\delta_{N,t}\\}$ and $\\mathbf{G}_t$ is an $N \\times M$ channel state matrix.\n\n*   **Action ($\\mathbf{a}_t$):** The channel assignment for each device.\n    $$\\mathbf{a}_t = (a_{1,t}, \\dots, a_{N,t}) \\in \\mathcal{A}$$\n    Subject to constraints: $\\sum_{n=1}^{N} \\mathbb{1}(a_{n,t}=m) = 1$ (each channel assigned to one device) and $\\sum_{m=1}^{M} \\mathbb{1}(a_{n,t}=m) \\leq 1$ (each device gets at most one channel).\n\n*   **Transitions:** The probability of moving to the next state is:\n    $$\\mathrm{P}(\\mathbf{s}^{+} | \\mathbf{s}, \\mathbf{a}) = \\mathrm{P}(\\delta^{+} | \\delta, \\mathbf{G}, \\mathbf{a}) \\mathrm{P}(\\mathbf{G}^{+})$$\n    The AoI transition for a specific device $n$ is:\n    $$\\mathrm{P}(\\delta_{n}^{+} | \\delta_{n}, \\mathbf{g}_{n}, a_{n}) = \\begin{cases} 1 - \\psi_{n,m}, & \\text{if } \\delta_{n}^{+} = 1, a_{n} = m \\\\ \\psi_{n,m}, & \\text{if } \\delta_{n}^{+} = \\delta_{n} + 1, a_{n} = m \\\\ 1, & \\text{if } \\delta_{n}^{+} = \\delta_{n} + 1, a_{n} = 0 \\\\ 0, & \\text{otherwise} \\end{cases}$$\n    Where $\\psi_{n,m}$ is the packet drop rate for device $n$ on channel $m$.\n\n---\n\n### 3. Bellman Optimality Equation\nThe solution is characterized by the **Optimal Action-Value Function (Q-function)**, which represents the expected cumulative discounted cost:\n$$Q(\\mathbf{s}_t, \\mathbf{a}_t) = c(\\mathbf{s}_t) + \\gamma \\sum_{\\mathbf{s}_{t+1}} \\mathrm{Pr}(\\mathbf{s}_{t+1} | \\mathbf{s}_t, \\mathbf{a}_t) \\min_{\\mathbf{a}_{t+1} \\in \\mathcal{A}} Q(\\mathbf{s}_{t+1}, \\mathbf{a}_{t+1})$$\nThe optimal policy $\\pi^*(\\mathbf{s}_t)$ is then derived as:\n$$\\mathbf{a}_t^* = \\arg \\min_{\\mathbf{a}_t \\in \\mathcal{A}} Q(\\mathbf{s}_t, \\mathbf{a}_t)$$",
          "mode": "fast_direct_with_vision",
          "success": true
        }
      ],
      "motivation": [
        {
          "query": "What problem or task does this paper aim to solve? Describe the specific challenges.",
          "answer": "Based on the provided paper, here is a detailed description of the problem the authors aim to solve and the specific challenges involved:\n\n### **The Core Problem**\nThe paper addresses the **Goal-oriented Transmission Scheduling** problem in a multi-device, multi-channel wireless communication system. Unlike traditional communications that focus on bit-level accuracy, this work prioritizes **application-driven objectives** (such as data freshness and system performance) by optimizing how $N$ edge devices share $M$ limited fading channels ($M < N$).\n\nThe primary goal is to find an optimal scheduling policy ($\\pi$) that minimizes the long-term expected sum of cost functions across all devices. In the context of the provided example (a remote state estimation system), this means minimizing the **Mean Square Error (MSE)** of the estimated states, which is directly tied to the **Age of Information (AoI)**.\n\n### **Specific Challenges**\n\nThe authors identify several critical challenges that make this problem difficult to solve using standard methods:\n\n#### **1. High-Dimensional State and Action Spaces**\n*   **State Space:** The system must track the AoI of every device and the channel states for every device-channel pair. This leads to a state space that grows exponentially with the number of devices.\n*   **Action Space:** The scheduler must decide which device is assigned to which channel. For a system with $N$ devices and $M$ channels, the number of possible actions is $N! / (N-M)!$. For a relatively small 10-device-5-channel system, this already results in 30,240 possible actions, making conventional Markov Decision Process (MDP) solvers like value or policy iteration computationally infeasible.\n\n#### **2. Limitations of Standard DRL Approaches**\n*   **Off-Policy DRL (e.g., DQN, DDPG):** While sample-efficient because they reuse past data, these methods suffer from **training instability and bias**. Discrepancies between the current policy and the \"old\" data in the replay buffer can cause the training to diverge, especially in large-scale dynamic systems.\n*   **On-Policy DRL (e.g., PPO, TRPO):** These are more stable but have **poor data efficiency**. They discard data after every update, which often leads to insufficient exploration and causes the agent to get stuck in **local minima**, resulting in suboptimal performance.\n\n#### **3. Lack of Domain-Specific Guidance**\nMost existing DRL applications use \"brute-force\" optimization without considering the underlying physics or mathematical structure of the communication problem. Without incorporating **structural properties** (like monotonicity or convexity), these algorithms struggle to find the theoretical optimal policy in complex environments.\n\n#### **4. Scalability**\nThe paper notes that previous structure-enhanced DRL works were limited to small-scale systems (e.g., up to 20 sensors). Scaling to larger systems (e.g., 40 devices and 20 channels) causes benchmark off-policy algorithms to fail to converge and standard on-policy algorithms to suffer significant performance loss.\n\n### **Summary of the Paper's Approach**\nTo overcome these, the authors:\n1.  **Mathematically prove structural properties** of the optimal solution (monotonicity and asymptotic convexity of the value function).\n2.  **Propose SUDO-DRL**, a hybrid algorithm that uses these properties to guide a \"dual\" on-policy and off-policy training process.\n3.  **Introduce a Structural Property Evaluation Framework** to score and filter training data, ensuring the agent learns from \"favorable\" trajectories that align with theoretical optimality.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What are the limitations or drawbacks of existing approaches mentioned in the introduction or related work?",
          "answer": "Based on the provided document, the authors categorize existing approaches into three main groups and identify specific limitations for each:\n\n### 1. Conventional Heuristic and Dynamic Programming Methods\n*   **Heuristic Methods (e.g., Whittle’s Index):** While these are computationally efficient, the authors note that they **cannot guarantee optimality** in scheduling decisions.\n*   **Conventional Dynamic Programming (e.g., Value and Policy Iteration):** These methods are described as **computationally infeasible** for modern goal-oriented systems. The high-dimensional state and action spaces (the \"curse of dimensionality\") make them impractical even for relatively small systems (e.g., a 10-device-5-channel system has over 30,000 possible actions).\n\n### 2. Standard Deep Reinforcement Learning (DRL) Algorithms\nThe authors distinguish between off-policy and on-policy DRL, noting drawbacks in both:\n*   **Off-Policy DRL (e.g., DQN, DDPG):** \n    *   **Instability and Bias:** These methods update the current policy using data from past policies, which introduces training instability and bias due to discrepancies between old and new behaviors.\n    *   **Scalability Issues:** The paper explicitly states that benchmark off-policy algorithms **fail to converge** in large-scale scenarios (e.g., systems with 40 devices and 20 channels).\n*   **On-Policy DRL (e.g., TRPO, PPO):**\n    *   **Sample Inefficiency:** These methods discard data after each update to ensure stability, leading to poor data efficiency.\n    *   **Suboptimal Exploration:** Discarding data can hinder the agent's ability to explore the environment thoroughly, often resulting in the policy getting stuck in **local minima**.\n    *   **Performance Loss:** In large-scale systems, state-of-the-art on-policy methods exhibit \"significant performance loss\" compared to the proposed structure-guided approach.\n\n### 3. Initial Structure-Enhanced DRL Studies\nWhile some recent works have begun incorporating structural properties (like monotonicity or threshold structures), they suffer from the following:\n*   **Limited Scope:** Most existing structure-aware algorithms rely exclusively on off-policy DRL (like DDPG). As a result, they inherit the instability of off-policy methods and are limited to small-scale systems (typically up to 20 sensors and 10 channels).\n*   **Lack of Domain Insights:** Many general DRL applications use \"brute-force optimization\" without investigating the mathematical properties of the optimal policy, leading to less effective discovery of optimal solutions.\n*   **Missing Theoretical Properties:** Prior to this paper, the authors claim that **asymptotic convexity** with respect to Age of Information (AoI) states had not been explored or proven in the context of transmission scheduling.\n\n### Summary Table of Limitations\n| Approach | Key Drawbacks |\n| :--- | :--- |\n| **Heuristics** | No guarantee of optimality. |\n| **Dynamic Programming** | Computationally infeasible for large-scale systems. |\n| **Off-Policy DRL** | Training instability, bias, and failure to converge in large systems. |\n| **On-Policy DRL** | High sample inefficiency and tendency to hit local minima. |\n| **Existing Structure-DRL** | Limited to small scales; lacks comprehensive structural insights (like convexity). |",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What gap or unmet need motivates this research? How is it different from prior work?",
          "answer": "Based on the provided document, the research is motivated by several critical gaps in the field of goal-oriented communications and transmission scheduling. The unmet needs and the ways this work differentiates itself from prior research are detailed below:\n\n### 1. The Gap: Limitations of Conventional Scheduling Methods\n*   **Computational Infeasibility:** Traditional dynamic programming methods (like value or policy iteration) are computationally impossible to implement for large-scale systems due to the \"curse of dimensionality\" in high-dimensional state and action spaces.\n*   **Suboptimality of Heuristics:** While heuristic methods (like Whittle’s Index) are efficient, they cannot guarantee optimality and often lead to significant performance loss in complex scenarios.\n\n### 2. The Gap: Weaknesses in Existing DRL Approaches\n*   **Instability vs. Inefficiency:** \n    *   **Off-policy DRL** (e.g., DQN, DDPG) is sample-efficient but suffers from training instability and bias because it updates the current policy using data from past, different policies.\n    *   **On-policy DRL** (e.g., PPO, TRPO) is stable but has poor data efficiency because it discards data after each update, often failing to explore the state space sufficiently.\n*   **Lack of Domain-Specific Insights:** Most existing works apply \"brute-force\" DRL without considering the underlying structural properties of the problem. This often results in agents getting stuck in local minima.\n\n### 3. How This Research is Different from Prior Work\nThe authors differentiate their work through three primary innovations:\n\n#### A. Advanced Theoretical Framework (The \"Structure\")\nUnlike previous studies that only looked at basic monotonicity, this paper derives more complex structural properties:\n*   **Asymptotic Convexity:** This is the first work in the literature to prove the asymptotic convexity of the optimal state value function with respect to Age of Information (AoI) states.\n*   **Channel State Monotonicity:** It establishes that the optimal value function and policy are monotonic with respect to channel states, complementing earlier work that focused primarily on AoI.\n*   **Greedy Structure:** It proves that for co-located devices, the optimal policy follows a \"greedy\" structure where devices with the highest AoI must be scheduled.\n\n#### B. SUDO-DRL: A Hybrid \"Dual\" Approach\nWhile prior \"structure-enhanced\" DRL works (like SE-DDPG or MRI-DDPG) relied exclusively on off-policy methods, this paper proposes **SUDO-DRL (Structure-guided Unified dual On-off policy DRL)**. \n*   **Unified Loss Function:** It combines the stability of on-policy training (PPO) with the efficiency of off-policy methods (SAC-like) into a single framework.\n*   **Structure-Guided Replay Buffer:** Instead of a standard buffer, it uses a \"Structure-Guided Data Storage Scheme\" that only saves high-quality transitions that align with the proven theoretical properties (monotonicity and convexity).\n\n#### C. Scalability to Large-Scale Systems\nPrior structure-guided DRL algorithms were limited to small systems (e.g., 20 sensors and 10 channels). This research successfully addresses systems with **40 devices and 20 channels**—a scale where benchmark off-policy algorithms fail to converge and standard on-policy methods show significant performance degradation.\n\n### Summary of Motivation\nThe research is driven by the need for a scheduling solution that is **simultaneously optimal, stable, sample-efficient, and scalable.** By \"guiding\" the learning process with mathematical proofs of how an optimal controller *should* behave (the structure), the authors bridge the gap between theoretical optimality and practical DRL implementation.",
          "mode": "fast_direct_with_vision",
          "success": true
        }
      ],
      "solution": [
        {
          "query": "What method, approach, or framework does this paper propose? Provide an overview.",
          "answer": "Based on the provided paper, the authors propose a novel deep reinforcement learning (DRL) framework called **SUDO-DRL (Structure-guided Unified Dual On-off policy DRL)**. This framework is designed to solve complex transmission scheduling problems in large-scale goal-oriented communication systems.\n\nBelow is a detailed overview of the proposed method, approach, and framework:\n\n### 1. Theoretical Foundation (Structural Properties)\nBefore designing the algorithm, the paper derives several key structural properties of the optimal solution to the goal-oriented scheduling problem (which incorporates Age of Information (AoI) and channel states):\n*   **Monotonicity of the Optimal V-function:** Proven with respect to both AoI states and channel states.\n*   **Asymptotic Convexity:** The first result in literature to prove the asymptotic convexity of the state value function with respect to AoI states.\n*   **Monotonicity of the Optimal Policy:** Established with respect to channel states.\n*   **Asymptotic Greedy Structure:** For co-located devices, the optimal policy follows a greedy structure where devices with the highest AoI (above a certain threshold) must be scheduled.\n\n### 2. The SUDO-DRL Framework\nSUDO-DRL is a hybrid algorithm that combines the **stability of on-policy training** (based on Proximal Policy Optimization - PPO) with the **sample efficiency of off-policy methods** (based on Soft Actor-Critic - SAC).\n\n#### A. Unified Dual Loss Function\nThe framework utilizes a unified loss function for both the Actor and Critic neural networks (NNs), balancing on-policy and off-policy components:\n*   **Critic Loss:** Combines Temporal Difference (TD) error from current trajectories (on-policy) and sampled past data (off-policy), while adding **penalty terms** for any violations of the derived structural properties (monotonicity and convexity).\n*   **Actor Loss:** Combines the PPO clipping loss (on-policy) with an entropy-regularized loss (off-policy) to improve exploration and stability.\n\n#### B. Structural Property Evaluation Framework (See Fig. 2)\nThe paper introduces a framework to score trajectories based on how well they align with the theoretical properties:\n*   **CM Score:** Critic-Monotonicity.\n*   **CC Score:** Critic-Convexity.\n*   **AM Score:** Actor-Monotonicity.\nThese scores are used to guide the learning process and ensure the NNs approximate the \"true\" optimal functions more accurately.\n\n#### C. Structure-Guided Replay Buffer Management\nUnlike standard off-policy DRL that stores all data, SUDO-DRL uses a **selective storage scheme**:\n*   **Data Storage:** Only transitions from trajectories that meet or exceed average structural scores (CM, CC, AM) are stored in the replay buffer.\n*   **Priority Sampling:** Transitions are sampled from the buffer based on a priority indicator derived from their structural scores and recency.\n\n#### D. Structure-Guided Pre-training (See Fig. 3)\nTo avoid starting from scratch, the authors propose a pre-training stage. It uses the **Asymptotic Greedy Structure** (Theorem 5) to select initial actions. This provides a \"good\" starting policy, which the paper shows reduces convergence time by 40%.\n\n### 3. Key Results and Scalability\nThe proposed framework is specifically designed to handle high-dimensional spaces. Numerical results demonstrate:\n*   **Performance:** A 25% to 45% improvement in system performance (reducing Mean Square Error) compared to standard PPO.\n*   **Scalability:** SUDO-DRL successfully manages systems with **40 devices and 20 channels**, a scale where traditional off-policy DRL (like DDPG) fails to converge and standard on-policy methods (PPO) show significant performance loss.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What are the main components, modules, or steps of the proposed method?",
          "answer": "Based on the provided document, the proposed method is **SUDO-DRL (Structure-guided Unified Dual On-off policy DRL)**. It is a hybrid algorithm designed to solve large-scale goal-oriented transmission scheduling problems by combining the stability of on-policy learning with the sample efficiency of off-policy methods, all guided by derived theoretical structural properties.\n\nThe main components and steps of the method are as follows:\n\n### 1. Theoretical Structural Property Derivation\nThe foundation of the method is the mathematical proof of four key properties of the optimal solution:\n*   **Monotonicity of the V-function:** The optimal state-value function increases with both Age of Information (AoI) states (Lemma 1) and channel states (Theorem 1).\n*   **Asymptotic Convexity:** The V-function is asymptotically convex with respect to AoI states (Theorem 3).\n*   **Monotonicity of the Optimal Policy:** If a device is scheduled on a channel, it should remain scheduled if its channel quality improves (Theorem 4).\n*   **Greedy Structure:** For co-located devices, the optimal policy follows a greedy structure for devices with very high AoI (Theorem 5).\n\n### 2. Structural Property Evaluation Framework\nThis module (illustrated in Fig. 2) translates the theoretical proofs into quantifiable metrics to guide the Neural Networks (NNs):\n*   **Critic-Monotonicity (CM) Score:** Measures how well the Critic NN respects the proven monotonicity.\n*   **Critic-Convexity (CC) Score:** Measures the alignment of the Critic NN with the proven convexity.\n*   **Actor-Monotonicity (AM) Score:** Evaluates if the Actor NN's scheduling decisions follow the channel-state monotonicity.\n\n### 3. Unified Dual On-Off Policy Training\nThe algorithm maintains an Actor NN ($\\varphi$) and a Critic NN ($\\nu$) and updates them using a unified loss function that combines two parts:\n\n*   **On-Policy Part (based on PPO):**\n    *   **Experience Generation:** Generates trajectories using the current policy.\n    *   **Structure-Guided Loss:** The standard PPO loss is augmented with penalty terms derived from the structural property evaluation (CM and CC scores) to ensure the Critic NN adheres to theoretical bounds.\n*   **Off-Policy Part (based on SAC principles):**\n    *   **Structure-Guided Data Storage:** Transitions are only stored in the Replay Buffer if the trajectory's structural scores (CM, CC, AM) exceed the average of past trajectories.\n    *   **Priority-Based Sampling:** Transitions are sampled from the buffer based on a priority indicator ($p$) that combines the structural scores and the recency of the data.\n\n### 4. Structure-Guided Pre-training\nBefore formal training begins, the method uses a **Structure-guided Action Selection** step. It leverages the \"Greedy Structure\" (Theorem 5) to select initial actions that prioritize devices in the \"mandatory scheduling set\" (those with the highest AoI). This provides a \"good\" initial policy, reducing convergence time by approximately 40%.\n\n### 5. Implementation Steps (Algorithm 1)\n1.  **Initialization:** Set environment parameters and random weights for NNs.\n2.  **Trajectory Sampling:** Collect data; use greedy action selection if in the pre-training phase.\n3.  **Structural Evaluation:** Calculate CM, CC, and AM scores for the current trajectory.\n4.  **Buffer Management:** Store high-scoring trajectories in the replay buffer with priority indicators.\n5.  **Loss Calculation:** Compute the unified loss ($L_{SUDO} = L_{On} + \\beta L_{Off}$) for both the Actor and Critic.\n6.  **Parameter Update:** Use the Adam optimizer to update NN weights based on the unified loss.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "Describe the system design, model structure, or theoretical framework. Include any architecture diagrams or tables if present.",
          "answer": "Based on the provided document, the system design, model structure, and theoretical framework for goal-oriented transmission scheduling are detailed as follows:\n\n### 1. System Model\nThe paper considers a wireless goal-oriented communication system consisting of:\n*   **Edge Devices ($N$):** Cameras, sensors, or robots that generate local data.\n*   **Channels ($M$):** Limited wireless subcarriers where $M < N$, requiring efficient scheduling.\n*   **Remote Destination:** A base station or remote estimator that receives data to achieve specific application goals (e.g., remote state estimation).\n*   **Communication Model:** Uses independent and identically distributed (i.i.d.) block fading channels. Channel states are quantized into $\\bar{g}$ levels, each with a specific packet drop rate ($\\psi$).\n*   **Performance Metric:** The primary metric is the **Age of Information (AoI)**, which measures data freshness. The system aims to minimize a long-term discounted cost function $c_n(\\delta_{n,t})$ based on AoI.\n\n### 2. Theoretical Framework\nThe authors formulate the scheduling problem as a **Markov Decision Process (MDP)** and derive several key structural properties of the optimal solution:\n*   **Monotonicity of the Value Function:** Proven with respect to both AoI states (fresher is better) and channel states (better channels reduce long-term cost).\n*   **Asymptotic Convexity:** The first result in literature proving that the optimal state value function is asymptotically convex regarding AoI states.\n*   **Optimal Policy Monotonicity:** If it is optimal to schedule a device on a specific channel, it remains optimal if that channel's quality improves.\n*   **Greedy Structure:** For co-located devices, the optimal policy follows a greedy approach for devices with very high AoI (the \"mandatory scheduling set\").\n\n### 3. SUDO-DRL Architecture\nThe **Structure-guided Unified Dual On-off policy DRL (SUDO-DRL)** is a hybrid algorithm designed to combine the stability of on-policy learning (PPO) with the efficiency of off-policy learning (SAC).\n\n**Architecture Components (as shown in Fig. 3):**\n*   **Trajectory Sampling:** Generates experience data. During pre-training, it uses a \"Greedy Action Selection\" based on Theorem 5.\n*   **Structural Property Evaluation:** A framework (Fig. 2) that calculates scores for:\n    *   **Critic-Monotonicity (CM)**\n    *   **Critic-Convexity (CC)**\n    *   **Actor-Monotonicity (AM)**\n*   **On-Policy Part:** Uses a structure-guided loss function ($L_{On}$) that penalizes the Critic NN if it violates the proven monotonicity or convexity properties.\n*   **Off-Policy Part:** Features a **Structure-Guided Data Storage Scheme**. It only stores transitions in the replay buffer if the trajectory's structural scores exceed the historical average. It also uses **Structural Priority-based Sampling**.\n*   **Unified Loss Function:** Combines both parts: $L_{SUDO} = L_{On} + \\beta L_{Off}$.\n\n### 4. Key Tables and Data\n*   **Table I (Training Hyperparameters):** Lists critical values such as a discount factor ($\\gamma$) of 0.99, clipping parameter ($\\epsilon$) of 0.2, and a unified policy weight ($\\beta$) of 0.9.\n*   **Table II (Empirical Average Cost):** Demonstrates that SUDO-DRL scales to large systems (40 devices, 20 channels) where traditional off-policy methods like DDPG fail to converge. It shows a performance improvement of 25% to 45% over the standard PPO algorithm.\n*   **Performance Visuals:** Figures 5, 6, and 7 show that SUDO-DRL reaches near-perfect structural property scores (CM, CC, AM) much faster and more consistently than standard PPO, explaining its superior scheduling efficiency.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What are the key equations, formulas, or mathematical formulations? Show the notation and mathematical expressions.",
          "answer": "Based on the provided document, the key mathematical formulations, equations, and notations for the goal-oriented transmission scheduling problem are detailed below:\n\n### 1. System Model and Communication Parameters\n*   **Channel State:** $g_{n,m,t} \\in \\mathcal{G} \\triangleq \\{1,\\ldots,\\bar{g}\\}$ represents the state between device $n$ and destination on channel $m$.\n*   **Channel Distribution:** $\\mathrm{P}(g_{n,m,t} = i) = q_{n,m}^i$, where $\\sum_{i=1}^{\\bar{g}} q_{n,m}^i = 1$.\n*   **Channel Assignment Constraint (Eq. 3):**\n    $$\\sum_{n=1}^{N} \\mathbb{1}(a_{n,t} = m) = 1, \\quad \\sum_{m=1}^{M} \\mathbb{1}(a_{n,t} = m) \\leq 1$$\n    This ensures each channel is assigned to one device, and each device gets at most one channel.\n*   **Age of Information (AoI) Dynamics (Eq. 4):**\n    $$\\delta_{n,t+1} = \\begin{cases} 1, & \\text{if successful reception} \\\\ \\delta_{n,t} + 1, & \\text{otherwise} \\end{cases}$$\n\n### 2. Goal-Oriented Cost Functions\n*   **General Objective (Problem 1):** Minimize the infinite-horizon expected discounted cost:\n    $$\\min_{\\pi} \\lim_{T \\to \\infty} \\mathbb{E}^\\pi \\left[ \\sum_{t=1}^{T} \\sum_{n=1}^{N} \\gamma^t c_n(\\delta_{n,t}) \\right]$$\n*   **Remote State Estimation MSE (Example 1):**\n    $$c_n(\\delta_{n,t}) \\triangleq \\operatorname{Tr}(\\mathbf{P}_{n,t}) = \\operatorname{Tr}\\left( h_n^{\\delta_{n,t}}(\\bar{\\mathbf{P}}_n) \\right)$$\n    Where $h_n(\\mathbf{X}) = \\mathbf{A}_n\\mathbf{X}\\mathbf{A}_n^\\top + \\mathbf{W}_n$.\n\n### 3. MDP and Value Functions\n*   **State Transition Probability (Eq. 6):**\n    $$\\mathrm{P}(\\mathbf{s}^+ | \\mathbf{s}, \\mathbf{a}) = \\mathrm{P}(\\delta^+ | \\delta, \\mathbf{G}, \\mathbf{a}) \\mathrm{P}(\\mathbf{G}^+)$$\n*   **Bellman Optimality Equation (Eq. 7):**\n    $$Q(\\mathbf{s}_t, \\mathbf{a}_t) = c(\\mathbf{s}_t) + \\gamma \\sum_{\\mathbf{s}_{t+1}} \\mathrm{Pr}(\\mathbf{s}_{t+1} | \\mathbf{s}_t, \\mathbf{a}_t) \\min_{\\mathbf{a}_{t+1} \\in \\mathcal{A}} Q(\\mathbf{s}_{t+1}, \\mathbf{a}_{t+1})$$\n*   **Optimal State-Value Function (Eq. 9):** $v^*(\\mathbf{s}_t) = \\min_{\\mathbf{a}_t \\in \\mathcal{A}} Q(\\mathbf{s}_t, \\mathbf{a}_t)$.\n\n### 4. SUDO-DRL Structural Property Evaluation\nThe algorithm evaluates the Critic and Actor NNs using the following penalty metrics:\n*   **Critic Monotonicity (AoI & Channel):**\n    $$\\hat{V}_{\\mathrm{AoI}} = \\max(0, v(\\mathbf{s}; \\boldsymbol{\\nu}) - v(\\hat{\\mathbf{s}}_{(n)}; \\boldsymbol{\\nu}))$$\n    $$\\hat{V}_{\\mathrm{Ch}} = \\max(0, v(\\mathbf{s}; \\boldsymbol{\\nu}) - v(\\hat{\\mathbf{s}}_{(n,m)}; \\boldsymbol{\\nu}))$$\n*   **Critic Convexity (AoI):**\n    $$\\check{V}_{\\mathrm{AoI}} = \\max(0, 2v(\\mathbf{s}; \\boldsymbol{\\nu}) - (v(\\check{\\mathbf{s}}_{(n)}; \\boldsymbol{\\nu}) + v(\\hat{\\mathbf{s}}_{(n)}; \\boldsymbol{\\nu})))$$\n*   **Actor Monotonicity (Channel):**\n    $$\\acute{\\Lambda}_{\\mathrm{Ch},n} = \\mathbb{1}(a_n \\neq 0 \\text{ and } a_{\\mathrm{Ch},n} \\neq a_n)$$\n\n### 5. Unified Loss Functions\nSUDO-DRL combines on-policy (On) and off-policy (Off) losses:\n*   **Critic Loss (Eq. 22 & 31):**\n    $$L_{\\mathrm{SUDO}}(\\boldsymbol{\\nu}) = L_{\\mathrm{On}}(\\boldsymbol{\\nu}) + \\beta_1 L_{\\mathrm{Off}}(\\boldsymbol{\\nu})$$\n    $$L_{\\mathrm{On}}(\\boldsymbol{\\nu}) = \\frac{1}{B_1} \\sum_{l=1}^{B_1} \\mathrm{TD}_l^2 + \\frac{1}{K\\Xi} \\sum_{k=1}^{K} \\sum_{\\xi=1}^{\\Xi} (\\hat{V}_{\\mathrm{AoI},k,\\xi} + \\hat{V}_{\\mathrm{Ch},k,\\xi} + \\check{V}_{\\mathrm{AoI},k,\\xi})$$\n*   **Actor Loss (Eq. 23 & 38):**\n    $$L_{\\mathrm{SUDO}}(\\varphi) = L_{\\mathrm{On}}(\\varphi) + \\beta_2 L_{\\mathrm{Off}}(\\varphi)$$\n    $$L_{\\mathrm{Off}}(\\varphi) = \\frac{1}{B_2} \\sum_{b=1}^{B_2} \\left[ \\varpi \\log(\\pi(\\tilde{\\mathbf{a}}_b | \\mathbf{s}_b; \\varphi)) + (c_b + \\gamma v(\\tilde{\\mathbf{s}}_{b+1}; \\boldsymbol{\\nu})) \\right]$$\n\n### 6. Off-Policy Replay Buffer Sampling\n*   **Priority Indicator (Eq. 35):** $p_u = \\mathrm{CM}_u + \\mathrm{CC}_u + \\mathrm{AM}_u$ (Sum of Critic Monotonicity, Critic Convexity, and Actor Monotonicity scores).\n*   **Sampling Probability (Eq. 36):**\n    $$P_b \\triangleq \\frac{p_b \\cdot \\varrho^b}{\\sum_{b=1}^R (p_b \\cdot \\varrho^b)}$$\n    Where $\\varrho \\in (0,1]$ is the decay rate for recency.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What objective function, optimization goal, or theoretical derivation is used?",
          "answer": "Based on the provided document, the objective function, optimization goal, and theoretical derivations for the goal-oriented transmission scheduling problem are detailed as follows:\n\n### 1. Objective Function\nThe problem is formulated as an infinite-horizon Markov Decision Process (MDP). The objective function is the **expected discounted sum of cost functions** across all $N$ devices:\n$$\\min_{\\pi} \\lim_{T \\to \\infty} \\mathbb{E}^{\\pi} \\left[ \\sum_{t=1}^{T} \\sum_{n=1}^{N} \\gamma^{t} c_{n}(\\delta_{n,t}) \\right]$$\n*   **$c_n(\\delta_{n,t})$**: An application-specific cost function (e.g., Mean Square Error in remote estimation) that is non-decreasing with respect to the Age of Information (AoI).\n*   **$\\delta_{n,t}$**: The AoI state of device $n$ at time $t$.\n*   **$\\gamma$**: The discount factor $\\in (0,1)$.\n\n### 2. Optimization Goal\nThe primary goal is to determine an optimal dynamic scheduling policy $\\pi^*$ that minimizes the long-term system cost by deciding which devices should transmit over $M$ available channels ($M < N$) at each time step $t$, given the current AoI states ($\\delta_t$) and channel states ($\\mathbf{G}_t$).\n\n### 3. Theoretical Derivations (Structural Properties)\nThe authors derive several key properties of the optimal Value function ($v^*$) and Policy ($\\pi^*$) to guide the DRL agent:\n\n*   **Monotonicity of $v^*$ w.r.t. Channel States (Theorem 1):** The authors prove that the optimal state-value function is monotonically decreasing with respect to channel quality (higher channel states/better quality lead to lower long-term costs).\n*   **Asymptotic Convexity of $v^*$ w.r.t. AoI (Theorem 3 & Proposition 1):** They establish that $v^*$ is asymptotically convex relative to AoI states. This means that as information becomes increasingly stale, the \"penalty\" or cost to the system grows at an increasing rate.\n*   **Monotonicity of Optimal Policy w.r.t. Channel States (Theorem 4):** If it is optimal to schedule a device on a specific channel, it remains optimal if that channel's quality improves, provided all other variables remain constant.\n*   **Asymptotic Greedy Structure (Theorem 5):** For co-located devices (identical channels), the optimal policy follows a \"greedy\" structure where it is mandatory to schedule devices with the highest AoI (those in the \"mandatory scheduling set\").\n\n### 4. SUDO-DRL Optimization Approach\nThe paper uses these derivations to create the **SUDO-DRL** algorithm, which optimizes via a **Unified Dual On-Off Policy Loss Function**:\n*   **On-Policy Component:** Uses a PPO-based loss but adds a **structural penalty term** (Equation 31) that punishes the Critic NN if its outputs violate the proven monotonicity or convexity properties.\n*   **Off-Policy Component:** Employs a **Structure-Guided Data Storage Scheme**. Only trajectories that align with the theoretical properties (high CM, CC, and AM scores) are stored in the replay buffer.\n*   **Pre-training:** Uses the **Greedy Structure (Theorem 5)** to select actions during an initial phase, providing a \"good\" starting policy rather than training from scratch.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What is the algorithm, procedure, or workflow? Describe the key steps.",
          "answer": "Based on the provided document, the authors propose **SUDO-DRL (Structure-guided Unified Dual On-off policy DRL)**. This hybrid algorithm combines the stability of on-policy training (based on PPO) with the sample efficiency of off-policy methods (based on SAC), while being guided by the theoretical structural properties of the optimal scheduling policy.\n\nThe workflow is divided into three main phases: **Pre-training**, **Experience Generation**, and **Unified Parameter Updating**.\n\n### 1. Pre-training Phase (Structure-Guided Initialization)\nTo avoid starting from scratch, the algorithm uses a \"good\" initial policy derived from the proven **asymptotic greedy structure** (Theorem 5).\n*   **Procedure:** During the first $I_1$ episodes, the agent identifies a \"mandatory scheduling set\" (devices with the highest Age of Information (AoI) exceeding a certain threshold).\n*   **Action Selection:** It prioritizes scheduling these devices regardless of channel state variations. This generates high-quality initial trajectories to populate the replay buffer and stabilize early training.\n\n### 2. Experience Generation & Structural Evaluation\nThe algorithm generates data through a dual-path process:\n*   **On-policy Trajectory Sampling:** The actor network generates a trajectory $\\mathcal{T}_{\\text{On}}$ of length $T$.\n*   **Structural Property Evaluation Framework:** This is a key innovation. The sampled data is evaluated against three derived theoretical properties:\n    1.  **Critic-Monotonicity (CM):** V-function should increase with AoI and channel states.\n    2.  **Critic-Convexity (CC):** V-function should be asymptotically convex w.r.t. AoI.\n    3.  **Actor-Monotonicity (AM):** If a device is scheduled to a channel, it should remain scheduled if that channel's quality improves.\n*   **Score Calculation:** CM, CC, and AM scores are calculated. These scores determine if a trajectory is \"good\" enough to be stored in the off-policy replay buffer.\n\n### 3. Unified Dual On-Off Policy Training\nThe core of the algorithm is the **Unified Loss Function**, which updates the Actor ($\\varphi$) and Critic ($\\nu$) networks by combining two sources:\n\n#### A. On-policy Path (Stability)\n*   Uses the current trajectory to calculate the Advantage function ($A_t$) and Cost-to-go ($C_t$).\n*   **Critic Update:** Includes a standard Temporal Difference (TD) loss plus a **penalty term** for any violations of the structural properties (monotonicity and convexity) detected during evaluation.\n*   **Actor Update:** Uses the standard PPO clipped objective function to ensure stable policy updates.\n\n#### B. Off-policy Path (Efficiency)\n*   **Structure-Guided Storage:** Only trajectories that meet or exceed the average CM, CC, and AM scores are stored in the Replay Buffer ($\\mathcal{R}$).\n*   **Priority-Based Sampling:** Transitions are sampled from the buffer based on a priority indicator ($p$) derived from their structural scores and recency.\n*   **Loss Calculation:** Off-policy losses are calculated using an approximation of the Q-value (based on the V-function) to reduce complexity.\n\n#### C. Parameter Update\nThe final gradients are computed using the unified loss:\n*   $L_{\\text{SUDO}} = L_{\\text{On}} + \\beta L_{\\text{Off}}$\nThe weights $\\beta_1, \\beta_2$ balance the two approaches. The networks are then updated using the Adam optimizer.\n\n### Summary of the Workflow (as per Fig. 3)\n1.  **Sample Trajectory** (using greedy guidance if in pre-training).\n2.  **Evaluate Structural Scores** (CM, CC, AM).\n3.  **Filter & Store** high-score transitions into the **Replay Buffer**.\n4.  **Sample Mini-batches** from both the current trajectory (On-policy) and the Replay Buffer (Off-policy).\n5.  **Compute Unified Loss** (incorporating structural penalties in the on-policy critic loss).\n6.  **Update Neural Networks** and repeat.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What are the key parameters, settings, or implementation details mentioned?",
          "answer": "Based on the provided document, the key parameters, settings, and implementation details for the goal-oriented transmission scheduling system and the SUDO-DRL algorithm are detailed below:\n\n### 1. System Model & Communication Parameters\n*   **Network Scale:** The experiments evaluate various scales, ranging from small **(10 devices, 5 channels)** to large-scale scenarios with **40 devices and 20 channels**.\n*   **Channel Model:** Independent and identically distributed (i.i.d.) block fading channels.\n*   **Channel States ($\\mathcal{G}$):** Quantized into **$\\bar{g} = 5$ levels**.\n*   **Packet Drop Rates:** Corresponding to the 5 channel levels, the rates are set at **0.2, 0.15, 0.1, 0.05, and 0.01**. These are derived from a Rayleigh distribution with a scale parameter between 0.5 and 2.\n*   **Goal-Oriented Metric:** Age of Information (AoI) and specifically **Estimation Mean-Square Error (MSE)** for a remote state estimation system.\n*   **LTI System Dynamics:** For each device $n$, the state dimension $r_n = 2$ and measurement dimension $e_n = 1$. System matrices $\\mathbf{A}_n$ have spectral radii uniformly drawn from **(1, 1.3)**.\n\n### 2. SUDO-DRL Algorithm Architecture\nThe algorithm is a hybrid \"Unified Dual On-Off Policy\" approach combining **PPO (on-policy)** and **SAC (off-policy)** elements.\n*   **Neural Networks:** \n    *   **Actor NN:** Approximates the policy $\\pi$; input dimension is $N + (N \\times M)$. Output is $2N$ (for SUDO and PPO).\n    *   **Critic NN:** Approximates the state-value function $v$; input dimension is $N + (N \\times M)$. Output is 1.\n    *   **Structure:** Both are fully connected NNs with **three hidden layers**.\n*   **Loss Functions:** \n    *   **Critic Loss:** Combines Temporal Difference (TD) error with structural penalties for violations of **monotonicity** (w.r.t. AoI and channel) and **asymptotic convexity** (w.r.t. AoI).\n    *   **Actor Loss:** Combines the PPO clipped objective with an off-policy SAC-based entropy term.\n\n### 3. Training Hyperparameters (Table I)\n*   **Learning Rates:** Critic = **0.001**, Actor = **0.0001** (with a decay rate of 0.001).\n*   **Optimization:** **Adam Optimizer**.\n*   **Discount Factor ($\\gamma$):** 0.99.\n*   **GAE Parameter ($\\lambda$):** 0.99.\n*   **PPO Clipping ($\\epsilon$):** 0.2.\n*   **Batch Sizes:** Both On-policy ($B_1$) and Off-policy ($B_2$) are set to **128**.\n*   **Replay Buffer ($R$):** Size of **200** trajectories.\n*   **Training Duration:** Total of **10,000 episodes**; Time horizon ($T$) per episode is **128**.\n\n### 4. Structural Guidance Implementation\n*   **Pre-training:** Uses a **Structure-Guided Action Selection** (based on Theorem 5) for the first $10 \\times N$ episodes to provide a \"good\" initial policy.\n*   **Score-Based Storage:** Trajectories are only stored in the off-policy replay buffer if they meet or exceed the average **Critic-Monotonicity (CM)**, **Critic-Convexity (CC)**, and **Actor-Monotonicity (AM)** scores of the past 50 trajectories.\n*   **Priority Sampling:** Sampling from the buffer is weighted by a priority indicator ($p = CM + CC + AM$) and a recency decay rate ($\\varrho = 0.95$).\n*   **Sampling for Evaluation:** $K=50$ state-action pairs are sampled per trajectory, testing $\\Xi=4$ AoI and channel states to calculate structural scores.\n\n### 5. Performance Results\n*   **Efficiency:** SUDO-DRL reduces convergence time by **40%** compared to versions without pre-training.\n*   **Effectiveness:** Improves system performance (MSE reduction) by **25% to 45%** over the benchmark PPO.\n*   **Scalability:** Successfully converges in a **40-device/20-channel** environment where standard off-policy DRL (DDPG) fails to converge.",
          "mode": "fast_direct_with_vision",
          "success": true
        }
      ],
      "results": [
        {
          "query": "What datasets, benchmarks, or experimental setups are used for evaluation?",
          "answer": "Based on the provided document, the evaluation of the proposed SUDO-DRL algorithm is conducted using the following datasets, benchmarks, and experimental setups:\n\n### 1. Experimental Setup\nThe experiments are modeled on a **Remote State Estimation System** (as described in Example 1 of the paper).\n*   **System Model:** $N$ edge devices (sensors) transmitting to a remote destination through $M$ channels ($M < N$).\n*   **Dynamic Processes:** Each sensor measures a process modeled as a discrete-time linear time-invariant (LTI) system.\n    *   **State/Measurement Dimensions:** Process state $r_n = 2$; measurement $c_n = 1$.\n    *   **System Matrices ($A_n$):** Randomly generated with spectral radii uniformly drawn from $(1, 1.3)$.\n*   **Communication Model:**\n    *   **Channels:** Independent and identically distributed (i.i.d.) block fading channels.\n    *   **Quantization:** Channel states are quantized into $\\bar{g} = 5$ levels.\n    *   **Packet Drop Rates:** Set to $0.2, 0.15, 0.1, 0.05,$ and $0.01$ (derived from a Rayleigh distribution with scale parameters between $0.5$ and $2$).\n*   **Hardware/Software:** Intel Core i7 9700 CPU, 32GB RAM, and an NVIDIA RTX 3060Ti GPU.\n\n### 2. Benchmarks\nThe paper compares SUDO-DRL against several state-of-the-art Deep Reinforcement Learning (DRL) algorithms:\n*   **On-policy Benchmark:**\n    *   **PPO (Proximal Policy Optimization):** The standard on-policy algorithm which SUDO-DRL builds upon.\n*   **Off-policy Benchmarks:**\n    *   **DDPG (Deep Deterministic Policy Gradient):** A fundamental off-policy actor-critic method.\n    *   **SE-DDPG (Structure-Enhanced DDPG):** A previous state-of-the-art method that incorporates monotonicity properties.\n    *   **MRI-DDPG (Monotonicity-Regularized DDPG):** A state-of-the-art approach for goal-oriented scheduling.\n\n### 3. Evaluation Scenarios (System Scales)\nThe performance is tested across **16 different parameter settings** (Para. 1-16) and four primary system scales $(N, M)$:\n*   **Small-scale:** (10 devices, 5 channels)\n*   **Medium-scale:** (20 devices, 10 channels)\n*   **Large-scale:** (30 devices, 15 channels) and **(40 devices, 20 channels)**.\n\n### 4. Performance Metrics\nThe algorithms are evaluated using:\n*   **Empirical Average Sum MSE Cost:** The primary goal-oriented metric (Mean Square Error of the remote estimation).\n*   **Convergence Time/Speed:** Measured by the number of episodes required to reach a stable cost.\n*   **Structural Property Scores:**\n    *   **CM (Critic-Monotonicity):** Measures how well the critic NN follows proven monotonicity w.r.t. channel and AoI states.\n    *   **CC (Critic-Convexity):** Measures adherence to the proven asymptotic convexity of the value function.\n    *   **AM (Actor-Monotonicity):** Measures the monotonicity of the policy w.r.t. channel states.\n\n### 5. Key Findings from Evaluation\n*   **Scalability:** Off-policy benchmarks (DDPG, SE-DDPG, MRI-DDPG) failed to converge in systems larger than 20 devices.\n*   **Efficiency:** SUDO-DRL improved system performance by **25% to 45%** and reduced convergence time by **40%** compared to the PPO benchmark.\n*   **Structural Adherence:** As shown in Figures 5, 6, and 7, SUDO-DRL reaches near-perfect structural scores (100) much faster than PPO, which struggles particularly with convexity and actor monotonicity.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What evaluation metrics or criteria are used to measure performance?",
          "answer": "Based on the provided document, the performance of the transmission scheduling system and the SUDO-DRL algorithm is measured using several distinct categories of metrics:\n\n### 1. Primary System Performance Metric\n*   **Sum of Cost Functions (MSE):** The central objective is to minimize the infinite-horizon expected sum of cost functions across all $N$ devices. \n*   **Remote State Estimation (Example 1):** In the specific case of remote state estimation, the cost function $c_n(\\delta_{n,t})$ is defined as the **Estimation Mean-Square Error (MSE)**. This measures the error covariance of the remote estimate, which is a function of the **Age of Information (AoI)**.\n*   **Age of Information (AoI):** Defined as the time elapsed since the last successful packet reception ($\\delta_{n,t}$). It serves as a proxy for information freshness and directly influences the cost function.\n\n### 2. DRL Training and Efficiency Metrics\nThe paper evaluates the effectiveness of the SUDO-DRL algorithm using the following:\n*   **Average Sum MSE Cost:** Used to plot learning curves (Fig. 4) to compare the final performance of different algorithms.\n*   **Convergence Time:** Measured by the number of episodes required to reach a stable, minimum cost. SUDO-DRL is noted to reduce convergence time by approximately 40%.\n*   **Scalability:** The ability of the algorithm to converge in large-scale systems (e.g., 40 devices and 20 channels), where traditional off-policy methods like DDPG fail.\n\n### 3. Structural Property Evaluation Scores\nA novel framework (illustrated in **Fig. 2**) is used to measure how well the neural networks adhere to theoretically proven optimal properties. These are expressed as scores from 0 to 100:\n*   **Critic-Monotonicity (CM) Score:** Measures if the Critic NN respects the property that the value function should increase as AoI or channel states worsen (based on Lemma 1 and Theorem 1).\n*   **Critic-Convexity (CC) Score:** Measures if the Critic NN follows the proven asymptotic convexity of the value function with respect to AoI states (Theorem 3).\n*   **Actor-Monotonicity (AM) Score:** Measures if the Actor NN follows the property that if a device is scheduled on a channel, it should remain scheduled if that channel's quality improves (Theorem 4).\n\n### 4. Algorithmic Stability and Efficiency\n*   **Sample Efficiency:** The ability to learn effectively from fewer data points by combining on-policy stability with off-policy data reuse (via the replay buffer).\n*   **Training Stability:** Measured by the lack of bias and fluctuations during the policy update process, primarily attributed to the unified dual on-off policy approach.\n\n### 5. Numerical Comparison Criteria\nIn **Table II**, the algorithms are compared across 16 different parameter settings (\"Para. 1-16\") based on:\n*   **Empirical Average Cost:** The average MSE calculated over 20,000-step simulations after training is complete.\n*   **Success Rate:** Whether an algorithm can successfully converge (\"—\" indicates failure to converge) for a given system scale $(N, M)$.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What are the main results shown in the main results table?",
          "answer": "Based on the document provided, the main results are presented in **Table II: Empirical Average Cost of the SUDO-DRL Algorithm and the Benchmarks**. The table compares the performance of the proposed **SUDO-DRL** against four state-of-the-art methods across 16 different parameter settings and four system scales.\n\nThe key findings from this table are:\n\n### 1. Superior Scalability\nThe table demonstrates that SUDO-DRL is significantly more robust as system complexity increases:\n*   **Small Scale (10 devices, 5 channels):** All algorithms (DDPG, SE-DDPG, MRI-DDPG, PPO, and SUDO-DRL) are generally able to converge.\n*   **Medium Scale (20 devices, 10 channels):** Standard DDPG fails to converge (indicated by \"—\"), while structure-enhanced versions (SE-DDPG, MRI-DDPG) still function.\n*   **Large Scale (30+ devices, 15-20 channels):** All off-policy benchmarks (DDPG, SE-DDPG, and MRI-DDPG) **fail to converge**. Only the on-policy PPO and the proposed SUDO-DRL are capable of handling these high-dimensional spaces.\n\n### 2. Significant Performance Gains\nSUDO-DRL consistently achieves the lowest (best) empirical average MSE cost in large-scale scenarios:\n*   **Compared to PPO:** In the largest tested scenario (40 devices, 20 channels), SUDO-DRL reduces the average cost by approximately **25% to 40%** compared to the standard PPO algorithm.\n*   **Consistency:** Across all 16 parameter settings, SUDO-DRL (highlighted in bold in the table) consistently outperforms the on-policy benchmark (PPO).\n\n### 3. Comparison with Specialized Off-Policy Methods\n*   In small-scale systems (10, 5), the table shows that SUDO-DRL is comparable to, though sometimes slightly behind, specialized off-policy methods like SE-DDPG. The authors note this is because off-policy methods are highly efficient in smaller, less complex environments.\n*   However, as the system grows, those specialized methods become non-functional, whereas SUDO-DRL maintains high performance.\n\n### 4. Summary of Numerical Improvements\nThe text accompanying the table (and the abstract) synthesizes these results, noting that SUDO-DRL:\n*   Improves system performance by **up to 45%**.\n*   Reduces convergence time by **40%** (as supported by the training curves in Fig. 4).\n*   Successfully manages systems with up to **40 devices and 20 channels**, a scale where previous structure-guided off-policy DRL methods were limited to roughly 20 sensors.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "How does the proposed method compare to baseline methods? Show the comparison.",
          "answer": "Based on the provided document, the proposed **SUDO-DRL (Structure-guided Unified Dual On-off policy DRL)** method significantly outperforms baseline methods in terms of performance, scalability, and training efficiency.\n\n### 1. Performance and Scalability Comparison\nThe paper compares SUDO-DRL against several state-of-the-art algorithms: **PPO** (on-policy benchmark), **DDPG** (off-policy benchmark), **SE-DDPG**, and **MRI-DDPG** (structure-guided off-policy benchmarks).\n\n*   **System Performance:** SUDO-DRL improves system performance (measured by Mean Square Error/MSE) by **25% to 45%** compared to the PPO benchmark.\n*   **Scalability:** SUDO-DRL demonstrates superior scalability. It effectively handles large-scale systems with up to **40 devices and 20 channels**. \n    *   **Off-policy baselines (DDPG, SE-DDPG, MRI-DDPG)** fail to converge entirely in these large-scale setups.\n    *   **On-policy baseline (PPO)** converges but exhibits significant performance loss (higher MSE) compared to SUDO-DRL.\n*   **Small-Scale Systems:** In smaller systems (e.g., 10 devices, 5 channels), SUDO-DRL achieves performance comparable to advanced off-policy methods like SE-DDPG, though it is noted to be slightly worse because off-policy methods are highly specialized for small-scale optimality.\n\n### 2. Training Efficiency\n*   **Convergence Time:** SUDO-DRL reduces convergence time by approximately **40%** compared to versions without its specific structure-guided enhancements.\n*   **Stability:** By combining on-policy stability with off-policy sample efficiency, it avoids the training instability and bias often found in pure off-policy methods (like DDPG) while overcoming the data inefficiency of pure on-policy methods (like PPO).\n\n### 3. Structural Property Alignment\nThe paper uses \"Structure Scores\" (CM, CC, and AM) to show how well the NNs adhere to theoretical optimal properties (monotonicity and convexity). The comparison in **Figures 5, 6, and 7** shows:\n*   **Critic Monotonicity (CM):** SUDO-DRL reaches a perfect score (100) almost immediately, whereas PPO takes 1,000 episodes.\n*   **Critic Convexity (CC):** SUDO-DRL reaches a perfect score within 200 episodes, while PPO never exceeds a score of 80.\n*   **Actor Monotonicity (AM):** SUDO-DRL reaches a perfect score by 2,000 episodes, while PPO plateaus below 75.\n\n### Summary Table (Based on Table II)\n| System Scale (N, M) | DDPG | SE-DDPG / MRI-DDPG | PPO | SUDO-DRL |\n| :--- | :--- | :--- | :--- | :--- |\n| **Small (10, 5)** | Works | **Best Performance** | Poor Performance | High Performance |\n| **Medium (20, 10)** | Fails | Works | Poor Performance | **Best Performance** |\n| **Large (40, 20)** | Fails | **Fails** | Works (High Cost) | **Best Performance (Low Cost)** |\n\n### Key Advantages of the Proposed Method\n1.  **Hybrid Approach:** It uses a unified loss function that integrates the stability of PPO with the sample efficiency of SAC (Soft Actor-Critic).\n2.  **Structure-Guided Storage:** It selectively stores \"high-quality\" data in the replay buffer only if the trajectory aligns with proven structural properties (monotonicity/convexity).\n3.  **Pre-training:** It uses a greedy structure (Theorem 5) to initialize the policy, leading to much faster initial learning.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What performance does the method achieve? Report the exact numbers from experiments.",
          "answer": "Based on the numerical experiments detailed in Section VI and the corresponding figures, the proposed **SUDO-DRL** method achieves the following performance results:\n\n### **1. Overall System Performance (MSE Cost)**\n*   **Reduction in Cost:** SUDO-DRL improves system performance by **25% to 45%** compared to state-of-the-art methods (specifically the benchmark on-policy PPO algorithm).\n*   **Large-Scale Scenarios:** In a system with **40 devices and 20 channels** (the largest scale tested), SUDO-DRL reduced the average sum MSE cost by approximately **35%** compared to PPO (as shown in Fig. 4).\n*   **Empirical Average Cost (Table II):** Across 16 different parameter settings, SUDO-DRL consistently outperformed PPO. For example:\n    *   **Scale (30, 15):** Achieved costs as low as **518.03** compared to PPO's **900.71**.\n    *   **Scale (40, 20):** Achieved costs as low as **689.81** compared to PPO's **971.35**.\n\n### **2. Training Efficiency and Convergence**\n*   **Convergence Speed:** The method reduces convergence time by approximately **40%** compared to state-of-the-art methods.\n*   **Impact of Pre-training:** The use of the structure-guided pre-training stage allowed the algorithm to converge in significantly fewer episodes—achieving over **40% faster convergence** than SUDO-DRL without pre-training (Fig. 4).\n\n### **3. Scalability**\n*   **Robustness:** SUDO-DRL successfully handles large-scale systems (up to **40 devices and 20 channels**). \n*   **Benchmark Failure:** In these large-scale setups, off-policy benchmarks like **DDPG, SE-DDPG, and MRI-DDPG failed to converge**, while the standard on-policy PPO exhibited significant performance loss.\n\n### **4. Structural Property Alignment (Scores)**\nThe method ensures that the neural networks adhere to theoretical optimal properties, measured on a scale of 0–100:\n*   **Critic Monotonicity (CM):** Achieved a full score of **100** very quickly (Fig. 5).\n*   **Critic Convexity (CC):** Guaranteed a full score of **100** after approximately **200 episodes**, whereas the PPO benchmark remained below 80 (Fig. 6).\n*   **Actor Monotonicity (AM):** Reached a score of **100** after **2000 episodes**, while PPO failed to exceed 75 (Fig. 7).",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What ablation study or sensitivity analysis is conducted? What are the findings?",
          "answer": "Based on the provided document, the authors conduct a comprehensive evaluation of the **SUDO-DRL** algorithm, which serves as a form of structural ablation and sensitivity analysis by comparing it against standard on-policy and off-policy benchmarks.\n\nThe analysis focuses on three primary areas: the contribution of the **pre-training stage**, the impact of **system scale (scalability)**, and the effectiveness of the **structural property scores**.\n\n### 1. Ablation of the Pre-training Stage\nThe authors specifically analyze the impact of the \"Structure-Guided Action Selection\" used during pre-training (based on the greedy structure derived in Theorem 5).\n*   **The Study:** They compare SUDO-DRL with a version of itself that excludes the pre-training phase (labeled \"SUDO-DRL without pre-training\" in Fig. 4).\n*   **Findings:** \n    *   **Convergence Speed:** The pre-training stage reduces convergence time by approximately **40%**.\n    *   **Final Performance:** The version with pre-training achieves a lower average cost (MSE), suggesting that the structural guidance helps the agent avoid local minima and find a better initialization point for formal training.\n\n### 2. Sensitivity to System Scale (Scalability Analysis)\nThe study tests the algorithms across 16 different parameter settings and four system scales: (10 devices, 5 channels), (20, 10), (30, 15), and (40, 20).\n*   **The Study:** Comparing SUDO-DRL against off-policy (DDPG, SE-DDPG, MRI-DDPG) and on-policy (PPO) benchmarks as the state/action space dimensions increase.\n*   **Findings:**\n    *   **Off-policy Failure:** Standard off-policy methods (DDPG) only work at the smallest scale. State-of-the-art variants (SE-DDPG, MRI-DDPG) fail to converge once the system reaches 30 devices.\n    *   **On-policy Performance Loss:** While the on-policy benchmark (PPO) converges in large systems, it exhibits significant performance loss.\n    *   **SUDO-DRL Superiority:** SUDO-DRL is the only algorithm that remains both stable and efficient at the largest scale (40 devices/20 channels), improving system performance by **25% to 45%** over PPO (Table II).\n\n### 3. Structural Property Score Evaluation\nThe authors evaluate how well the NNs adhere to the derived theoretical properties (Monotonicity and Convexity) during the training process.\n*   **The Study:** They track three metrics: **Critic-Monotonicity (CM)**, **Critic-Convexity (CC)**, and **Actor-Monotonicity (AM)** scores (Figs. 5, 6, and 7).\n*   **Findings:**\n    *   **Critic Monotonicity:** SUDO-DRL reaches a perfect score (100) almost immediately, whereas PPO takes 1,000 episodes.\n    *   **Critic Convexity:** SUDO-DRL guarantees a full score after 200 episodes, while PPO never rises above 80.\n    *   **Actor Monotonicity:** SUDO-DRL reaches a full score after 2,000 episodes, while PPO plateaus below 75.\n*   **Conclusion:** This analysis proves that standard DRL (PPO) struggles to learn the underlying mathematical structure of the optimal policy (especially convexity), which explains the performance gap.\n\n### Summary of Findings\nThe analysis demonstrates that **SUDO-DRL** successfully bridges the gap between on-policy stability and off-policy efficiency. By enforcing structural properties (monotonicity and convexity) through its unified loss function and using a structure-guided replay buffer, it achieves **faster convergence, better scalability, and higher accuracy** in goal-oriented communication tasks.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What analysis, case study, or discussion of the results is provided?",
          "answer": "Based on the provided document, the analysis and discussion of results are centered on the theoretical derivation of structural properties and the empirical validation of the proposed **SUDO-DRL** algorithm.\n\n### 1. Theoretical Analysis of Structural Properties\nThe authors provide a rigorous mathematical analysis of the optimal scheduling policy, which serves as the foundation for their DRL algorithm. Key findings include:\n*   **Monotonicity of the Value Function:** The authors prove that the optimal state-value function ($v^*$) is monotonically increasing with respect to both **Age of Information (AoI) states** (Lemma 1) and **channel states** (Theorem 1). This implies that system performance degrades as data becomes older or channel conditions worsen.\n*   **Asymptotic Convexity:** For the first time in transmission scheduling literature, the authors prove the **asymptotic convexity** of the value function with respect to AoI states (Theorem 3). This suggests that the \"cost\" of outdated information increases at an accelerating rate as AoI grows.\n*   **Optimal Policy Structure:** Theorem 4 establishes the monotonicity of the optimal policy regarding channel states, while Theorem 5 identifies an **asymptotic greedy structure** for co-located devices. This means that in certain conditions, the optimal policy aligns with a \"greedy\" approach that prioritizes devices with the highest immediate cost.\n\n### 2. Case Study: Remote State Estimation System\nThe paper uses a **Remote State Estimation System** (Example 1) as its primary case study to evaluate the goal-oriented framework.\n*   **Setup:** The system involves $N$ sensors measuring linear time-invariant (LTI) processes and sending estimates to a remote estimator.\n*   **Metric:** The performance goal is to minimize the **Mean Square Error (MSE)** of the remote estimates.\n*   **Linkage:** The authors prove in Lemma 2 that the MSE cost function in this system is asymptotically convex, justifying the use of convexity-based guidance in their DRL model.\n\n### 3. Discussion of Numerical Results\nThe results are discussed through comparisons with state-of-the-art algorithms (PPO, DDPG, SE-DDPG, and MRI-DDPG) across 16 different parameter settings:\n\n*   **Performance and Scalability:**\n    *   SUDO-DRL outperforms the benchmark PPO by **25% to 45%** in terms of reducing the average sum MSE.\n    *   **Scalability:** SUDO-DRL successfully handles large-scale systems (up to **40 devices and 20 channels**). In contrast, off-policy benchmarks like DDPG fail to converge at this scale, and PPO exhibits significant performance loss (Fig. 4 and Table II).\n*   **Convergence Efficiency:**\n    *   The inclusion of a **pre-training stage** (guided by the greedy structure in Theorem 5) reduces convergence time by approximately **40%** (Fig. 4).\n*   **Structural Property Scores (Figures 5, 6, and 7):**\n    *   The authors use a \"score\" system (CM, CC, and AM) to track how well the NNs adhere to the proven theoretical properties during training.\n    *   **Critic Monotonicity (CM):** SUDO-DRL reaches a perfect score almost immediately, whereas PPO takes 1,000 episodes.\n    *   **Critic Convexity (CC) and Actor Monotonicity (AM):** SUDO-DRL maintains near-perfect scores, while PPO struggles significantly (staying below 80 for CC and 75 for AM). This confirms that SUDO-DRL’s superior performance is directly linked to its ability to enforce these theoretical structures.\n\n### 4. Summary of SUDO-DRL Innovation\nThe discussion highlights that the algorithm's success stems from its **Unified Dual Approach**:\n1.  **On-policy:** Provides training stability via a structure-guided loss function.\n2.  **Off-policy:** Enhances sample efficiency by using a **Structure-Guided Data Storage Scheme**, which only keeps high-quality transitions in the replay buffer based on their structural property scores.",
          "mode": "fast_direct_with_vision",
          "success": true
        }
      ],
      "contributions": [
        {
          "query": "What are the main contributions listed in the introduction or conclusion?",
          "answer": "Based on the provided document, the main contributions of this work are detailed in the introduction and summarized in the conclusion. They can be categorized into three primary areas:\n\n### 1. Theoretical Derivation of Structural Properties\nThe authors establish several key structural properties of the optimal solution for goal-oriented transmission scheduling (which considers both Age of Information (AoI) and channel states):\n*   **Monotonicity of the Optimal V Function:** They prove that the optimal state-value function is monotonically increasing with respect to **channel states** (Theorem 1), complementing previous work that showed monotonicity regarding AoI states.\n*   **Asymptotic Convexity:** They provide the first result in the literature to prove the **asymptotic convexity** of the state-value function with respect to AoI states in transmission scheduling problems (Theorem 3 and Proposition 1).\n*   **Monotonicity of the Optimal Policy:** They derive the monotonicity of the optimal scheduling policy with respect to channel states (Theorem 4).\n*   **Greedy Structure:** They establish an **asymptotic greedy structure** for the optimal policy in systems with co-located devices (Theorem 5), suggesting that devices with the highest AoI (above a certain threshold) must be scheduled.\n\n### 2. Development of the SUDO-DRL Algorithm\nThe paper proposes the **Structure-guided Unified Dual On-off policy DRL (SUDO-DRL)**, a hybrid algorithm designed to solve large-scale MDPs:\n*   **Hybrid Approach:** It combines the **stability of on-policy training** (based on PPO) with the **sample efficiency of off-policy methods** (based on SAC) through a unified loss function.\n*   **Structural Property Evaluation Framework:** As shown in **Fig. 2**, the algorithm calculates \"scores\" for critic-monotonicity (CM), critic-convexity (CC), and actor-monotonicity (AM).\n*   **Structure-Guided Loss and Buffer Management:** \n    *   The **on-policy loss function** incorporates penalties for violating the derived structural properties.\n    *   The **off-policy replay buffer** uses a selective storage scheme (storing only high-quality data that aligns with structural properties) and a priority-based sampling method.\n*   **Pre-training Stage:** It utilizes the \"greedy structure\" derived in the theory to guide action selection during an initial pre-training phase to find a better starting policy.\n\n### 3. Numerical Validation and Scalability\nThe authors demonstrate the efficacy of SUDO-DRL through extensive experiments (as seen in **Table II** and **Figs. 4-7**):\n*   **Performance Gains:** SUDO-DRL improves system performance (reduces MSE cost) by **25% to 45%** compared to state-of-the-art methods like PPO.\n*   **Efficiency:** It reduces convergence time by approximately **40%**.\n*   **Scalability:** The algorithm successfully handles large-scale systems (up to **40 devices and 20 channels**). This is a significant contribution because benchmark off-policy algorithms (like DDPG) fail to converge at this scale, and standard on-policy benchmarks (like PPO) exhibit significant performance loss.",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What is novel or new about this work compared to existing methods?",
          "answer": "Based on the provided document, this work introduces several novel theoretical and algorithmic contributions to the field of goal-oriented transmission scheduling. The key innovations are detailed below:\n\n### 1. New Theoretical Structural Properties\nWhile previous works (including the authors' own earlier research) identified basic monotonicity, this paper significantly advances the theoretical framework by proving three new properties of the optimal solution:\n*   **Asymptotic Convexity of the Value Function:** This is the first result in the literature to explore and prove the convexity of the optimal state-value function ($v^*$) with respect to Age of Information (AoI) states. The authors prove this for two-device systems (Theorem 2) and establish asymptotic convexity for multi-device-multi-channel systems (Theorem 3).\n*   **Monotonicity w.r.t. Channel States:** The authors prove that the optimal $v^*$ function is monotonically decreasing with respect to channel states (Theorem 1), complementing existing knowledge of monotonicity regarding AoI.\n*   **Asymptotic Greedy Structure:** The paper proves that for co-located devices (identical channel conditions), the optimal policy follows a \"greedy\" structure where devices in a \"mandatory scheduling set\" (those with the highest AoI) must be scheduled (Theorem 5).\n\n### 2. SUDO-DRL: A Hybrid \"Dual\" Architecture\nMost existing methods use either **on-policy** DRL (stable but sample-inefficient, like PPO) or **off-policy** DRL (efficient but unstable in large scales, like DQN/DDPG). This work proposes the **Structure-guided Unified Dual On-off policy DRL (SUDO-DRL)**, which is novel in its hybrid approach:\n*   **Unified Loss Function:** It combines the stability of on-policy training with the sample efficiency of off-policy methods through a single unified loss function ($L_{SUDO} = L_{On} + \\beta L_{Off}$).\n*   **Structural Property Evaluation Framework:** As shown in **Fig. 2**, the algorithm introduces a framework that calculates \"scores\" for Critic-Monotonicity (CM), Critic-Convexity (CC), and Actor-Monotonicity (AM). These scores are used to penalize the NNs when they violate the proven theoretical properties.\n\n### 3. Novel Replay Buffer Management\nUnlike standard off-policy methods that store all experiences, SUDO-DRL uses a **Structure-Guided Data Storage Scheme**:\n*   **Selective Storage:** It only stores transitions in the replay buffer if the trajectory's structural scores (CM, CC, AM) meet specific constraints (Equation 34).\n*   **Priority-Based Sampling:** It uses a priority indicator ($p$) based on these structural scores to sample \"high-quality\" data that aligns with theoretical optimality, rather than just sampling based on temporal recency or TD-error.\n\n### 4. Structure-Guided Pre-training\nThe algorithm introduces a specific pre-training stage (Algorithm 1, Step 6) that uses the **Greedy Structure** derived in Theorem 5. This allows the agent to find a \"good\" initial policy quickly, rather than starting from a random state. This was shown to reduce convergence time by 40% (**Fig. 4**).\n\n### 5. Superior Scalability\nA major practical novelty is the demonstrated scalability. Existing off-policy methods (DDPG, SE-DDPG) failed to converge in large-scale scenarios (e.g., 40 devices, 20 channels). SUDO-DRL not only converges in these environments but outperforms the standard on-policy benchmark (PPO) by **25% to 45%** in system performance (**Table II**).",
          "mode": "fast_direct_with_vision",
          "success": true
        },
        {
          "query": "What limitations does the paper acknowledge? What future directions are suggested?",
          "answer": "Based on the provided document, the paper acknowledges several limitations and suggests specific future research directions to build upon the SUDO-DRL framework.\n\n### **Acknowledged Limitations**\n\nThe paper identifies limitations in both existing methodologies and the specific scope of the current study:\n\n*   **Computational Infeasibility of Conventional Methods:** The authors note that traditional dynamic programming methods (like value and policy iteration) are computationally infeasible for large-scale systems due to high-dimensional state and action spaces.\n*   **Suboptimality of Heuristics:** Heuristic methods, while efficient, cannot guarantee optimality in goal-oriented scheduling.\n*   **DRL Algorithm Trade-offs:**\n    *   **Off-policy DRL (e.g., DQN, DDPG):** While sample-efficient, these methods suffer from training instability and bias. The paper specifically notes that state-of-the-art off-policy algorithms (like SE-DDPG) fail to converge in large-scale setups (e.g., 40 devices and 20 channels).\n    *   **On-policy DRL (e.g., PPO):** These methods are stable but exhibit poor data efficiency and insufficient exploration, often getting stuck in local minima.\n*   **Theoretical Constraints:**\n    *   **Convexity Proofs:** While the authors prove asymptotic convexity for multi-device systems, they acknowledge that proving absolute convexity for general multi-device-multi-channel systems is extremely challenging due to the increased dimensionality of state and action spaces.\n    *   **Channel State Convexity:** The authors explicitly state they did not derive convexity for channel states because it is \"neither meaningful nor necessary,\" as the cost function fundamentally depends on AoI rather than channel quality.\n    *   **Policy Monotonicity:** The paper mentions that general results for policy monotonicity in terms of AoI have not yet been derived for multi-device systems (only for special cases like two-device-single-channel).\n*   **Scenario Specificity:** Some theoretical results (like the greedy structure of the optimal policy) were only derived for the special case of \"co-located devices\" with identical channel states.\n\n### **Suggested Future Directions**\n\nThe paper outlines several avenues for future research to enhance goal-oriented communication systems:\n\n*   **Expansion of Theoretical Framework:** The authors suggest exploring additional structural properties of the optimal solution to further refine the guidance provided to DRL algorithms.\n*   **Comprehensive Resource Allocation:** Future work aims to extend the SUDO-DRL algorithm beyond simple transmission scheduling to include:\n    *   **Power Allocation:** Optimizing the energy used for transmissions alongside scheduling.\n    *   **Advanced Multiple Access Schemes:** Integrating techniques like Non-Orthogonal Multiple Access (NOMA) into the scheduling framework.\n*   **Broader Application Goals:** While this paper focuses heavily on Age of Information (AoI) and remote state estimation, future directions involve applying these structure-guided methods to other goal-oriented metrics and application-specific objectives in 6G networks.\n*   **Scalability Enhancements:** Continuing to improve the scalability of these algorithms to handle even larger and more complex network environments where current benchmarks exhibit performance loss.",
          "mode": "fast_direct_with_vision",
          "success": true
        }
      ]
    }
  },
  "origin": {
    "tables": [
      {
        "id": "Doc Table 1",
        "caption": "",
        "html": "<table><tr><th>Hyperparameters of SUDO-DRL and benchmarks</th><th>Value</th></tr><tr><td>Critic NN learning rate</td><td>0.001</td></tr><tr><td>Actor NN learning rate</td><td>0.0001</td></tr><tr><td>Decay rate of learning rate</td><td>0.001</td></tr><tr><td>Discount factor, <b>γ</b></td><td>0.99</td></tr><tr><td>GAE parameter, <b>λ</b></td><td>0.99</td></tr><tr><td>Clipping parameter, <b>ε</b></td><td>0.2</td></tr><tr><td>Policy entropy loss weight, <b>ω</b></td><td>0.01</td></tr><tr><td>Decay rate of sampling priority, <b>ρ</b></td><td>0.95</td></tr><tr><td>Unified on-off policy loss function hyperparameter, <b>β</b><sub>1</sub>, <b>β</b><sub>2</sub></td><td>0.9</td></tr><tr><td>On-policy and off-policy batch size, <b>B</b><sub>1</sub>, <b>B</b><sub>2</sub></td><td>128</td></tr><tr><td>Number of sampled states for score scheme, <b>K</b></td><td>50</td></tr><tr><td>Number of tested AoI and channel state, <b>Xi</b></td><td>4</td></tr><tr><td>Number of past trajectories for average score computing, <b>bar</b></td><td>50</td></tr><tr><td>Time horizon of each episode, <b>T</b></td><td>128</td></tr><tr><td>Size of replay buffer, <b>R</b></td><td>200</td></tr><tr><td>Number of episodes for pre-training, <b>I</b><sub>1</sub></td><td>10 × <b>N</b></td></tr><tr><td>Total number of episodes for training, <b>I</b></td><td>10000</td></tr><tr><td>Optimizer during training</td><td>Adam</td></tr></table>"
      },
      {
        "id": "Doc Table 2",
        "caption": "",
        "html": "<table><tr><th>System Scale (N, M)</th><th>Para.</th><th>DDPG</th><th>SE-DDPG [21]</th><th>MRI-DDPG [22]</th><th>PPO</th><th>SUDO-DRL</th></tr><tr><td>(10, 5)</td><td>1</td><td>89.26</td><td>77.14</td><td>84.00</td><td>119.63</td><td><b>85.52</b></td></tr><tr><td></td><td>2</td><td>98.87</td><td>87.30</td><td>90.28</td><td>123.01</td><td><b>95.82</b></td></tr><tr><td></td><td>3</td><td>—</td><td>106.60</td><td>119.55</td><td>195.37</td><td><b>121.31</b></td></tr><tr><td></td><td>4</td><td>83.12</td><td>78.21</td><td>80.88</td><td>120.93</td><td><b>80.68</b></td></tr><tr><td>(20, 10)</td><td>5</td><td>—</td><td>357.14</td><td>369.14</td><td>569.96</td><td><b>370.63</b></td></tr><tr><td></td><td>6</td><td>—</td><td>—</td><td>445.87</td><td>584.37</td><td><b>426.29</b></td></tr><tr><td></td><td>7</td><td>—</td><td>407.20</td><td>441.73</td><td>731.94</td><td><b>417.90</b></td></tr><tr><td></td><td>8</td><td>—</td><td>290.72</td><td>307.94</td><td>376.16</td><td><b>308.91</b></td></tr><tr><td>(30, 15)</td><td>9</td><td>—</td><td>—</td><td>—</td><td>805.45</td><td><b>519.78</b></td></tr><tr><td></td><td>10</td><td>—</td><td>—</td><td>—</td><td>739.97</td><td><b>575.34</b></td></tr><tr><td></td><td>11</td><td>—</td><td>—</td><td>—</td><td>900.71</td><td><b>518.03</b></td></tr><tr><td></td><td>12</td><td>—</td><td>—</td><td>—</td><td>901.42</td><td><b>551.97</b></td></tr><tr><td>(40, 20)</td><td>13</td><td>—</td><td>—</td><td>—</td><td>1057.05</td><td><b>719.45</b></td></tr><tr><td></td><td>14</td><td>—</td><td>—</td><td>—</td><td>971.35</td><td><b>689.81</b></td></tr><tr><td></td><td>15</td><td>—</td><td>—</td><td>—</td><td>1291.54</td><td><b>994.80</b></td></tr><tr><td></td><td>16</td><td>—</td><td>—</td><td>—</td><td>1012.20</td><td><b>771.91</b></td></tr></table>"
      }
    ],
    "figures": [],
    "base_path": "Z:\\PythonWorkSpace\\sophyverse-platform-backend\\src\\slide-svc\\data\\temp\\1b9a609e-fce7-49de-a3fc-cb828a2de68b"
  },
  "markdown_paths": [
    "Z:\\PythonWorkSpace\\sophyverse-platform-backend\\src\\slide-svc\\data\\temp\\1b9a609e-fce7-49de-a3fc-cb828a2de68b\\2501.11921.md"
  ]
}