{
  "rag_results": {
    "paper_info": [
      {
        "query": "List the paper title, author names and their institutional affiliations.",
        "answer": "Based on the provided document, here are the details requested:\n\n*   **Paper Title:** Goal-oriented Transmission Scheduling: Structure-guided DRL with a Unified Dual On-policy and Off-policy Approach\n*   **Author Names:**\n    *   Jiazheng Chen\n    *   Wanchun Liu\n*   **Institutional Affiliation:**\n    *   Both authors are affiliated with the **IEEE** (Institute of Electrical and Electronics Engineers). Specifically, Jiazheng Chen is a Graduate Student Member, and Wanchun Liu is a Member.\n    *   *Note: While the specific university is not explicitly named in the header text, the document mentions the authors' IEEE membership status and provides an asterisk indicating Wanchun Liu is the corresponding author.*",
        "mode": "fast_direct_with_vision",
        "success": true
      }
    ],
    "figures": [
      {
        "query": "Describe the architecture or framework diagram. What are the main components shown?",
        "answer": "Based on the provided document, the architecture and framework of the **SUDO-DRL** (Structure-guided Unified Dual On-off policy DRL) algorithm are primarily illustrated in **Figure 2** and **Figure 3**.\n\nThe architecture is a hybrid system that integrates **on-policy stability** (based on PPO) with **off-policy sample efficiency** (based on SAC), guided by **theoretical structural properties**.\n\n### 1. Theoretical Structural Property Evaluation Framework (Fig. 2)\nThis component serves as the \"guide\" for the neural networks. It evaluates whether the NNs are adhering to the mathematical properties derived in the paper:\n*   **Critic NN Evaluation:** Assesses the estimated state-value function $v(s)$ for:\n    *   **Monotonicity:** Ensuring the value function increases with AoI and channel states (based on Lemma 1 and Theorem 1).\n    *   **Convexity:** Ensuring the value function is asymptotically convex with respect to AoI states (based on Theorem 3).\n*   **Actor NN Evaluation:** Assesses the sampled actions for:\n    *   **Monotonicity:** Ensuring scheduling decisions are consistent with improved channel states (based on Theorem 4).\n*   **Output Scores:** It generates three specific metrics: **CM** (Critic-Monotonicity), **CC** (Critic-Convexity), and **AM** (Actor-Monotonicity).\n\n### 2. SUDO-DRL Main Architecture (Fig. 3)\nThe main framework is divided into three functional areas that interact to update the Actor and Critic NNs:\n\n*   **Trajectory Sampling & Pre-training:**\n    *   The **Actor NN** interacts with the **Wireless Environment** to generate experience trajectories.\n    *   **Pre-training Phase:** Includes an \"Additional Greedy Action Selection\" block (based on Theorem 5) to provide a high-quality initial policy by prioritizing devices in the \"mandatory scheduling set.\"\n\n*   **On-Policy Part (Green Block):**\n    *   Uses the current trajectory to calculate the **Advantage function ($A_t$)** and **Cost-to-go ($C_t$)**.\n    *   **Structure-guided On-policy Loss ($L_{On}$):** Unlike standard PPO, this loss function includes a penalty term derived from the structural property evaluation metrics to force the Critic NN to respect monotonicity and convexity.\n\n*   **Off-Policy Part (Blue Block):**\n    *   **Structure-Guided Data Storage:** A decision gate checks if the current trajectory's structural scores (CM, CC, AM) meet specific constraints. If they do, the data is stored in the **Replay Buffer ($R$)**; otherwise, it is discarded.\n    *   **Structural Priority-based Sampling:** Transitions are sampled from the buffer based on a priority indicator ($p$) that favors trajectories with higher structural alignment and recency.\n    *   **Off-policy Loss ($L_{Off}$):** Calculates losses based on the sampled transitions to improve sample efficiency.\n\n*   **Unified Update:**\n    *   The final component is the **Unified Dual On-Off Policy Loss Function ($L_{SUDO}$)**, which sums the weighted on-policy and off-policy losses. This unified gradient is used to update the parameters of both the **Actor NN ($\\varphi$)** and the **Critic NN ($\\nu$)**.",
        "mode": "fast_direct_with_vision",
        "success": true
      }
    ],
    "tables": [
      {
        "query": "Show the main performance comparison table with all methods and metrics. Include exact numbers.",
        "answer": "Based on the document provided, the main performance comparison is presented in **Table II**, which evaluates the **Empirical Average Cost** (specifically the average sum Mean Square Error) across 16 different parameter settings and 4 system scales.\n\n### **Table II: Empirical Average Cost Comparison**\n\nThe table compares the proposed **SUDO-DRL** against four benchmarks: DDPG, SE-DDPG, MRI-DDPG, and PPO. (Note: \"—\" indicates the algorithm failed to converge at that scale).\n\n| System Scale (N, M) | Para. | DDPG | SE-DDPG [21] | MRI-DDPG [22] | PPO | SUDO-DRL |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **(10, 5)** | 1 | 89.26 | 77.14 | 84.00 | 119.63 | **85.52** |\n| | 2 | 98.87 | 87.30 | 90.28 | 123.01 | **95.82** |\n| | 3 | — | 106.60 | 119.55 | 195.37 | **121.31** |\n| | 4 | 83.12 | 78.21 | 80.88 | 120.93 | **80.68** |\n| **(20, 10)** | 5 | — | 357.14 | 369.14 | 569.96 | **370.63** |\n| | 6 | — | — | 445.87 | 584.37 | **426.29** |\n| | 7 | — | 407.20 | 441.73 | 731.94 | **417.90** |\n| | 8 | — | 290.72 | 307.94 | 376.16 | **308.91** |\n| **(30, 15)** | 9 | — | — | — | 805.45 | **519.78** |\n| | 10 | — | — | — | 739.97 | **575.34** |\n| | 11 | — | — | — | 900.71 | **518.03** |\n| | 12 | — | — | — | 901.42 | **551.97** |\n| **(40, 20)** | 13 | — | — | — | 1057.05 | **719.45** |\n| | 14 | — | — | — | 971.35 | **689.81** |\n| | 15 | — | — | — | 1291.54 | **994.80** |\n| | 16 | — | — | — | 1012.20 | **771.91** |\n\n### **Key Observations from the Data:**\n*   **Scalability:** Off-policy methods (DDPG, SE-DDPG, MRI-DDPG) fail to converge as the system scale increases to 30 devices or more. SUDO-DRL and PPO are the only methods that remain functional at the largest scale (40 devices, 20 channels).\n*   **Performance Gain:** In large-scale scenarios (40, 20), SUDO-DRL consistently reduces the average MSE cost by **25% to 40%** compared to the standard PPO algorithm.\n*   **Small-Scale Performance:** For the smallest scale (10, 5), SUDO-DRL performs comparably to advanced off-policy methods but is slightly outperformed by SE-DDPG, as off-policy methods are highly efficient at smaller dimensions.\n*   **Convergence:** According to the text and Figure 4, SUDO-DRL achieves **40% faster convergence** than the version without pre-training and significantly outperforms PPO in stability and final cost.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "Show the ablation study table with all variants and their results.",
        "answer": "Based on the provided document, there is **no specific \"ablation study table\"** that explicitly lists and compares the individual components of the SUDO-DRL algorithm (such as removing only the score scheme or only the pre-training).\n\nHowever, the document provides a **performance comparison table** (Table II) and a **training performance graph** (Fig. 4) that function as a partial ablation study by comparing the full SUDO-DRL against its variants and benchmarks.\n\n### 1. Performance Comparison (Table II)\nThis table compares the empirical average MSE cost across 16 different parameter settings and 4 system scales. It demonstrates how the structure-guided approach (SUDO-DRL) compares to standard on-policy (PPO) and off-policy (DDPG) methods.\n\n| System Scale (N, M) | PPO (Standard On-Policy) | SUDO-DRL (Proposed) | Improvement |\n| :--- | :--- | :--- | :--- |\n| (10, 5) | 119.63 - 195.37 | **85.52 - 121.31** | ~30-40% |\n| (20, 10) | 376.16 - 731.94 | **308.91 - 426.29** | ~20-40% |\n| (30, 15) | 739.97 - 901.42 | **518.03 - 575.34** | ~35-40% |\n| (40, 20) | 971.35 - 1291.54 | **689.81 - 994.80** | ~25-35% |\n\n*Note: DDPG-based variants (SE-DDPG, MRI-DDPG) failed to converge for scales larger than (20, 10).*\n\n### 2. Variant Comparison (Figure 4)\nFigure 4 specifically compares three versions of the algorithm to show the impact of the proposed structural guidance and pre-training:\n\n*   **PPO (Baseline):** Exhibits the highest average sum MSE (approx. 1000) and the slowest, most unstable convergence.\n*   **SUDO-DRL (without pre-training):** Shows the impact of the **Unified Dual On-Off Policy** and **Structural Property Evaluation**. It achieves a significantly lower cost (approx. 700) than PPO but takes about 5000 episodes to stabilize.\n*   **SUDO-DRL (Full):** Includes the **Structure-Guided Action Selection (Pre-training)**. This variant starts at a much lower cost (approx. 800) and converges 40% faster than the version without pre-training, ultimately reaching the lowest MSE (below 700).\n\n### 3. Structural Score Results (Figures 5, 6, and 7)\nThe paper evaluates the \"internal\" effectiveness of the structural guidance by comparing scores between PPO and SUDO-DRL:\n*   **Critic Monotonicity (CM):** SUDO-DRL reaches a perfect score of 100 almost immediately; PPO takes 1000 episodes.\n*   **Critic Convexity (CC):** SUDO-DRL reaches 100 within 200 episodes; PPO never exceeds 80.\n*   **Actor Monotonicity (AM):** SUDO-DRL reaches 100 by 2000 episodes; PPO plateaus below 75.\n\n**Summary of Results:** The \"ablation\" logic in the text concludes that SUDO-DRL improves system performance by **25% to 45%** and reduces convergence time by **40%** compared to state-of-the-art methods.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "Show any dataset statistics table with sizes, splits, and other details.",
        "answer": "Based on the provided document, the dataset statistics and experimental parameters are primarily detailed in **Table I** and the **Numerical Experiments** section (Section VI).\n\nThe \"dataset\" in this context refers to the training data generated by the DRL agent through interactions with a simulated environment (a remote state estimation system).\n\n### **Dataset and Training Statistics (from Table I)**\n\n| Hyperparameter / Statistic | Value |\n| :--- | :--- |\n| **Total Number of Episodes ($I$)** | 10,000 |\n| **Pre-training Episodes ($I_1$)** | $10 \\times N$ (e.g., 400 for a 40-device system) |\n| **Time Horizon per Episode ($T$)** | 128 steps |\n| **On-policy Batch Size ($B_1$)** | 128 |\n| **Off-policy Batch Size ($B_2$)** | 128 |\n| **Replay Buffer Size ($R$)** | 200 trajectories |\n| **Sampled States for Score Scheme ($K$)** | 50 |\n| **Tested AoI and Channel States ($\\Xi$)** | 4 |\n| **Past Trajectories for Avg. Score ($\\bar{u}$)** | 50 |\n\n### **System Scale and Environment Details (from Section VI-A and Table II)**\nThe experiments evaluate the algorithm across various system dimensions, which dictates the complexity of the state-action space:\n\n*   **System Scales ($N$ devices, $M$ channels):**\n    *   Small: (10, 5)\n    *   Medium: (20, 10)\n    *   Large: (30, 15)\n    *   Very Large: (40, 20)\n*   **State Space Dimensions:** $N + (N \\times M)$. For the largest system (40, 20), the input dimension is $40 + 800 = 840$.\n*   **Channel States:** Quantized into $\\bar{g} = 5$ levels.\n*   **Packet Drop Rates:** $\\{0.2, 0.15, 0.1, 0.05, 0.01\\}$.\n*   **Evaluation Split:** Performance is verified using an empirical average MSE over **20,000-step simulations** across 16 different parameter settings (Para. 1-16).\n\n### **Neural Network Architecture**\n*   **Type:** Fully connected NNs with **three hidden layers**.\n*   **Actor Output:** $2N$ (for SUDO-DRL and PPO).\n*   **Critic Output:** 1 (estimated optimal V function).\n\nThe document notes that for the largest scale (40 devices, 20 channels), benchmark off-policy algorithms like DDPG fail to converge, whereas the SUDO-DRL dataset management (selective storage based on structural scores) allows for stable training.",
        "mode": "fast_direct_with_vision",
        "success": true
      }
    ],
    "equations": [
      {
        "query": "What is the core model formulation or main equation? Show the exact formula and notation.",
        "answer": "The core model formulation for the goal-oriented transmission scheduling problem is a **Markov Decision Process (MDP)** designed to minimize the long-term expected sum of cost functions across multiple devices.\n\n### 1. Objective Function\nThe primary goal is to find an optimal scheduling policy $\\pi$ that minimizes the infinite-horizon expected discounted cost:\n$$\\min_{\\pi} \\lim_{T \\to \\infty} \\mathbb{E}^{\\pi} \\left[ \\sum_{t=1}^{T} \\sum_{n=1}^{N} \\gamma^{t} c_{n}(\\delta_{n,t}) \\right]$$\n**Notation:**\n*   $N$: Number of edge devices.\n*   $\\gamma \\in (0,1)$: Discount factor.\n*   $\\delta_{n,t}$: Age of Information (AoI) for device $n$ at time $t$.\n*   $c_{n}(\\delta_{n,t})$: Application-specific cost function (non-decreasing with respect to AoI).\n\n---\n\n### 2. MDP Components\nThe system is modeled using the following state, action, and transition definitions:\n\n*   **State ($\\mathbf{s}_t$):** Comprises the AoI of all devices and the current channel states.\n    $$\\mathbf{s}_t \\triangleq (\\delta_t, \\mathbf{G}_t) \\in \\mathcal{S}$$\n    Where $\\delta_t = \\{\\delta_{1,t}, \\dots, \\delta_{N,t}\\}$ and $\\mathbf{G}_t$ is an $N \\times M$ channel state matrix.\n\n*   **Action ($\\mathbf{a}_t$):** The channel assignment for each device.\n    $$\\mathbf{a}_t = (a_{1,t}, \\dots, a_{N,t}) \\in \\mathcal{A}$$\n    Subject to constraints: $\\sum_{n=1}^{N} \\mathbb{1}(a_{n,t}=m) = 1$ (each channel assigned to one device) and $\\sum_{m=1}^{M} \\mathbb{1}(a_{n,t}=m) \\leq 1$ (each device gets at most one channel).\n\n*   **Transitions:** The probability of moving to the next state is:\n    $$\\mathrm{P}(\\mathbf{s}^{+} | \\mathbf{s}, \\mathbf{a}) = \\mathrm{P}(\\delta^{+} | \\delta, \\mathbf{G}, \\mathbf{a}) \\mathrm{P}(\\mathbf{G}^{+})$$\n    The AoI transition for a specific device $n$ is:\n    $$\\mathrm{P}(\\delta_{n}^{+} | \\delta_{n}, \\mathbf{g}_{n}, a_{n}) = \\begin{cases} 1 - \\psi_{n,m}, & \\text{if } \\delta_{n}^{+} = 1, a_{n} = m \\\\ \\psi_{n,m}, & \\text{if } \\delta_{n}^{+} = \\delta_{n} + 1, a_{n} = m \\\\ 1, & \\text{if } \\delta_{n}^{+} = \\delta_{n} + 1, a_{n} = 0 \\\\ 0, & \\text{otherwise} \\end{cases}$$\n    Where $\\psi_{n,m}$ is the packet drop rate for device $n$ on channel $m$.\n\n---\n\n### 3. Bellman Optimality Equation\nThe solution is characterized by the **Optimal Action-Value Function (Q-function)**, which represents the expected cumulative discounted cost:\n$$Q(\\mathbf{s}_t, \\mathbf{a}_t) = c(\\mathbf{s}_t) + \\gamma \\sum_{\\mathbf{s}_{t+1}} \\mathrm{Pr}(\\mathbf{s}_{t+1} | \\mathbf{s}_t, \\mathbf{a}_t) \\min_{\\mathbf{a}_{t+1} \\in \\mathcal{A}} Q(\\mathbf{s}_{t+1}, \\mathbf{a}_{t+1})$$\nThe optimal policy $\\pi^*(\\mathbf{s}_t)$ is then derived as:\n$$\\mathbf{a}_t^* = \\arg \\min_{\\mathbf{a}_t \\in \\mathcal{A}} Q(\\mathbf{s}_t, \\mathbf{a}_t)$$",
        "mode": "fast_direct_with_vision",
        "success": true
      }
    ],
    "motivation": [
      {
        "query": "What problem or task does this paper aim to solve? Describe the specific challenges.",
        "answer": "Based on the provided paper, here is a detailed description of the problem the authors aim to solve and the specific challenges involved:\n\n### **The Core Problem**\nThe paper addresses the **Goal-oriented Transmission Scheduling** problem in a multi-device, multi-channel wireless communication system. Unlike traditional communications that focus on bit-level accuracy, this work prioritizes **application-driven objectives** (such as data freshness and system performance) by optimizing how $N$ edge devices share $M$ limited fading channels ($M < N$).\n\nThe primary goal is to find an optimal scheduling policy ($\\pi$) that minimizes the long-term expected sum of cost functions across all devices. In the context of the provided example (a remote state estimation system), this means minimizing the **Mean Square Error (MSE)** of the estimated states, which is directly tied to the **Age of Information (AoI)**.\n\n### **Specific Challenges**\n\nThe authors identify several critical challenges that make this problem difficult to solve using standard methods:\n\n#### **1. High-Dimensional State and Action Spaces**\n*   **State Space:** The system must track the AoI of every device and the channel states for every device-channel pair. This leads to a state space that grows exponentially with the number of devices.\n*   **Action Space:** The scheduler must decide which device is assigned to which channel. For a system with $N$ devices and $M$ channels, the number of possible actions is $N! / (N-M)!$. For a relatively small 10-device-5-channel system, this already results in 30,240 possible actions, making conventional Markov Decision Process (MDP) solvers like value or policy iteration computationally infeasible.\n\n#### **2. Limitations of Standard DRL Approaches**\n*   **Off-Policy DRL (e.g., DQN, DDPG):** While sample-efficient because they reuse past data, these methods suffer from **training instability and bias**. Discrepancies between the current policy and the \"old\" data in the replay buffer can cause the training to diverge, especially in large-scale dynamic systems.\n*   **On-Policy DRL (e.g., PPO, TRPO):** These are more stable but have **poor data efficiency**. They discard data after every update, which often leads to insufficient exploration and causes the agent to get stuck in **local minima**, resulting in suboptimal performance.\n\n#### **3. Lack of Domain-Specific Guidance**\nMost existing DRL applications use \"brute-force\" optimization without considering the underlying physics or mathematical structure of the communication problem. Without incorporating **structural properties** (like monotonicity or convexity), these algorithms struggle to find the theoretical optimal policy in complex environments.\n\n#### **4. Scalability**\nThe paper notes that previous structure-enhanced DRL works were limited to small-scale systems (e.g., up to 20 sensors). Scaling to larger systems (e.g., 40 devices and 20 channels) causes benchmark off-policy algorithms to fail to converge and standard on-policy algorithms to suffer significant performance loss.\n\n### **Summary of the Paper's Approach**\nTo overcome these, the authors:\n1.  **Mathematically prove structural properties** of the optimal solution (monotonicity and asymptotic convexity of the value function).\n2.  **Propose SUDO-DRL**, a hybrid algorithm that uses these properties to guide a \"dual\" on-policy and off-policy training process.\n3.  **Introduce a Structural Property Evaluation Framework** to score and filter training data, ensuring the agent learns from \"favorable\" trajectories that align with theoretical optimality.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What are the limitations or drawbacks of existing approaches mentioned in the introduction or related work?",
        "answer": "Based on the provided document, the authors categorize existing approaches into three main groups and identify specific limitations for each:\n\n### 1. Conventional Heuristic and Dynamic Programming Methods\n*   **Heuristic Methods (e.g., Whittle’s Index):** While these are computationally efficient, the authors note that they **cannot guarantee optimality** in scheduling decisions.\n*   **Conventional Dynamic Programming (e.g., Value and Policy Iteration):** These methods are described as **computationally infeasible** for modern goal-oriented systems. The high-dimensional state and action spaces (the \"curse of dimensionality\") make them impractical even for relatively small systems (e.g., a 10-device-5-channel system has over 30,000 possible actions).\n\n### 2. Standard Deep Reinforcement Learning (DRL) Algorithms\nThe authors distinguish between off-policy and on-policy DRL, noting drawbacks in both:\n*   **Off-Policy DRL (e.g., DQN, DDPG):** \n    *   **Instability and Bias:** These methods update the current policy using data from past policies, which introduces training instability and bias due to discrepancies between old and new behaviors.\n    *   **Scalability Issues:** The paper explicitly states that benchmark off-policy algorithms **fail to converge** in large-scale scenarios (e.g., systems with 40 devices and 20 channels).\n*   **On-Policy DRL (e.g., TRPO, PPO):**\n    *   **Sample Inefficiency:** These methods discard data after each update to ensure stability, leading to poor data efficiency.\n    *   **Suboptimal Exploration:** Discarding data can hinder the agent's ability to explore the environment thoroughly, often resulting in the policy getting stuck in **local minima**.\n    *   **Performance Loss:** In large-scale systems, state-of-the-art on-policy methods exhibit \"significant performance loss\" compared to the proposed structure-guided approach.\n\n### 3. Initial Structure-Enhanced DRL Studies\nWhile some recent works have begun incorporating structural properties (like monotonicity or threshold structures), they suffer from the following:\n*   **Limited Scope:** Most existing structure-aware algorithms rely exclusively on off-policy DRL (like DDPG). As a result, they inherit the instability of off-policy methods and are limited to small-scale systems (typically up to 20 sensors and 10 channels).\n*   **Lack of Domain Insights:** Many general DRL applications use \"brute-force optimization\" without investigating the mathematical properties of the optimal policy, leading to less effective discovery of optimal solutions.\n*   **Missing Theoretical Properties:** Prior to this paper, the authors claim that **asymptotic convexity** with respect to Age of Information (AoI) states had not been explored or proven in the context of transmission scheduling.\n\n### Summary Table of Limitations\n| Approach | Key Drawbacks |\n| :--- | :--- |\n| **Heuristics** | No guarantee of optimality. |\n| **Dynamic Programming** | Computationally infeasible for large-scale systems. |\n| **Off-Policy DRL** | Training instability, bias, and failure to converge in large systems. |\n| **On-Policy DRL** | High sample inefficiency and tendency to hit local minima. |\n| **Existing Structure-DRL** | Limited to small scales; lacks comprehensive structural insights (like convexity). |",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What gap or unmet need motivates this research? How is it different from prior work?",
        "answer": "Based on the provided document, the research is motivated by several critical gaps in the field of goal-oriented communications and transmission scheduling. The unmet needs and the ways this work differentiates itself from prior research are detailed below:\n\n### 1. The Gap: Limitations of Conventional Scheduling Methods\n*   **Computational Infeasibility:** Traditional dynamic programming methods (like value or policy iteration) are computationally impossible to implement for large-scale systems due to the \"curse of dimensionality\" in high-dimensional state and action spaces.\n*   **Suboptimality of Heuristics:** While heuristic methods (like Whittle’s Index) are efficient, they cannot guarantee optimality and often lead to significant performance loss in complex scenarios.\n\n### 2. The Gap: Weaknesses in Existing DRL Approaches\n*   **Instability vs. Inefficiency:** \n    *   **Off-policy DRL** (e.g., DQN, DDPG) is sample-efficient but suffers from training instability and bias because it updates the current policy using data from past, different policies.\n    *   **On-policy DRL** (e.g., PPO, TRPO) is stable but has poor data efficiency because it discards data after each update, often failing to explore the state space sufficiently.\n*   **Lack of Domain-Specific Insights:** Most existing works apply \"brute-force\" DRL without considering the underlying structural properties of the problem. This often results in agents getting stuck in local minima.\n\n### 3. How This Research is Different from Prior Work\nThe authors differentiate their work through three primary innovations:\n\n#### A. Advanced Theoretical Framework (The \"Structure\")\nUnlike previous studies that only looked at basic monotonicity, this paper derives more complex structural properties:\n*   **Asymptotic Convexity:** This is the first work in the literature to prove the asymptotic convexity of the optimal state value function with respect to Age of Information (AoI) states.\n*   **Channel State Monotonicity:** It establishes that the optimal value function and policy are monotonic with respect to channel states, complementing earlier work that focused primarily on AoI.\n*   **Greedy Structure:** It proves that for co-located devices, the optimal policy follows a \"greedy\" structure where devices with the highest AoI must be scheduled.\n\n#### B. SUDO-DRL: A Hybrid \"Dual\" Approach\nWhile prior \"structure-enhanced\" DRL works (like SE-DDPG or MRI-DDPG) relied exclusively on off-policy methods, this paper proposes **SUDO-DRL (Structure-guided Unified dual On-off policy DRL)**. \n*   **Unified Loss Function:** It combines the stability of on-policy training (PPO) with the efficiency of off-policy methods (SAC-like) into a single framework.\n*   **Structure-Guided Replay Buffer:** Instead of a standard buffer, it uses a \"Structure-Guided Data Storage Scheme\" that only saves high-quality transitions that align with the proven theoretical properties (monotonicity and convexity).\n\n#### C. Scalability to Large-Scale Systems\nPrior structure-guided DRL algorithms were limited to small systems (e.g., 20 sensors and 10 channels). This research successfully addresses systems with **40 devices and 20 channels**—a scale where benchmark off-policy algorithms fail to converge and standard on-policy methods show significant performance degradation.\n\n### Summary of Motivation\nThe research is driven by the need for a scheduling solution that is **simultaneously optimal, stable, sample-efficient, and scalable.** By \"guiding\" the learning process with mathematical proofs of how an optimal controller *should* behave (the structure), the authors bridge the gap between theoretical optimality and practical DRL implementation.",
        "mode": "fast_direct_with_vision",
        "success": true
      }
    ],
    "solution": [
      {
        "query": "What method, approach, or framework does this paper propose? Provide an overview.",
        "answer": "Based on the provided paper, the authors propose a novel deep reinforcement learning (DRL) framework called **SUDO-DRL (Structure-guided Unified Dual On-off policy DRL)**. This framework is designed to solve complex transmission scheduling problems in large-scale goal-oriented communication systems.\n\nBelow is a detailed overview of the proposed method, approach, and framework:\n\n### 1. Theoretical Foundation (Structural Properties)\nBefore designing the algorithm, the paper derives several key structural properties of the optimal solution to the goal-oriented scheduling problem (which incorporates Age of Information (AoI) and channel states):\n*   **Monotonicity of the Optimal V-function:** Proven with respect to both AoI states and channel states.\n*   **Asymptotic Convexity:** The first result in literature to prove the asymptotic convexity of the state value function with respect to AoI states.\n*   **Monotonicity of the Optimal Policy:** Established with respect to channel states.\n*   **Asymptotic Greedy Structure:** For co-located devices, the optimal policy follows a greedy structure where devices with the highest AoI (above a certain threshold) must be scheduled.\n\n### 2. The SUDO-DRL Framework\nSUDO-DRL is a hybrid algorithm that combines the **stability of on-policy training** (based on Proximal Policy Optimization - PPO) with the **sample efficiency of off-policy methods** (based on Soft Actor-Critic - SAC).\n\n#### A. Unified Dual Loss Function\nThe framework utilizes a unified loss function for both the Actor and Critic neural networks (NNs), balancing on-policy and off-policy components:\n*   **Critic Loss:** Combines Temporal Difference (TD) error from current trajectories (on-policy) and sampled past data (off-policy), while adding **penalty terms** for any violations of the derived structural properties (monotonicity and convexity).\n*   **Actor Loss:** Combines the PPO clipping loss (on-policy) with an entropy-regularized loss (off-policy) to improve exploration and stability.\n\n#### B. Structural Property Evaluation Framework (See Fig. 2)\nThe paper introduces a framework to score trajectories based on how well they align with the theoretical properties:\n*   **CM Score:** Critic-Monotonicity.\n*   **CC Score:** Critic-Convexity.\n*   **AM Score:** Actor-Monotonicity.\nThese scores are used to guide the learning process and ensure the NNs approximate the \"true\" optimal functions more accurately.\n\n#### C. Structure-Guided Replay Buffer Management\nUnlike standard off-policy DRL that stores all data, SUDO-DRL uses a **selective storage scheme**:\n*   **Data Storage:** Only transitions from trajectories that meet or exceed average structural scores (CM, CC, AM) are stored in the replay buffer.\n*   **Priority Sampling:** Transitions are sampled from the buffer based on a priority indicator derived from their structural scores and recency.\n\n#### D. Structure-Guided Pre-training (See Fig. 3)\nTo avoid starting from scratch, the authors propose a pre-training stage. It uses the **Asymptotic Greedy Structure** (Theorem 5) to select initial actions. This provides a \"good\" starting policy, which the paper shows reduces convergence time by 40%.\n\n### 3. Key Results and Scalability\nThe proposed framework is specifically designed to handle high-dimensional spaces. Numerical results demonstrate:\n*   **Performance:** A 25% to 45% improvement in system performance (reducing Mean Square Error) compared to standard PPO.\n*   **Scalability:** SUDO-DRL successfully manages systems with **40 devices and 20 channels**, a scale where traditional off-policy DRL (like DDPG) fails to converge and standard on-policy methods (PPO) show significant performance loss.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What are the main components, modules, or steps of the proposed method?",
        "answer": "Based on the provided document, the proposed method is **SUDO-DRL (Structure-guided Unified Dual On-off policy DRL)**. It is a hybrid algorithm designed to solve large-scale goal-oriented transmission scheduling problems by combining the stability of on-policy learning with the sample efficiency of off-policy methods, all guided by derived theoretical structural properties.\n\nThe main components and steps of the method are as follows:\n\n### 1. Theoretical Structural Property Derivation\nThe foundation of the method is the mathematical proof of four key properties of the optimal solution:\n*   **Monotonicity of the V-function:** The optimal state-value function increases with both Age of Information (AoI) states (Lemma 1) and channel states (Theorem 1).\n*   **Asymptotic Convexity:** The V-function is asymptotically convex with respect to AoI states (Theorem 3).\n*   **Monotonicity of the Optimal Policy:** If a device is scheduled on a channel, it should remain scheduled if its channel quality improves (Theorem 4).\n*   **Greedy Structure:** For co-located devices, the optimal policy follows a greedy structure for devices with very high AoI (Theorem 5).\n\n### 2. Structural Property Evaluation Framework\nThis module (illustrated in Fig. 2) translates the theoretical proofs into quantifiable metrics to guide the Neural Networks (NNs):\n*   **Critic-Monotonicity (CM) Score:** Measures how well the Critic NN respects the proven monotonicity.\n*   **Critic-Convexity (CC) Score:** Measures the alignment of the Critic NN with the proven convexity.\n*   **Actor-Monotonicity (AM) Score:** Evaluates if the Actor NN's scheduling decisions follow the channel-state monotonicity.\n\n### 3. Unified Dual On-Off Policy Training\nThe algorithm maintains an Actor NN ($\\varphi$) and a Critic NN ($\\nu$) and updates them using a unified loss function that combines two parts:\n\n*   **On-Policy Part (based on PPO):**\n    *   **Experience Generation:** Generates trajectories using the current policy.\n    *   **Structure-Guided Loss:** The standard PPO loss is augmented with penalty terms derived from the structural property evaluation (CM and CC scores) to ensure the Critic NN adheres to theoretical bounds.\n*   **Off-Policy Part (based on SAC principles):**\n    *   **Structure-Guided Data Storage:** Transitions are only stored in the Replay Buffer if the trajectory's structural scores (CM, CC, AM) exceed the average of past trajectories.\n    *   **Priority-Based Sampling:** Transitions are sampled from the buffer based on a priority indicator ($p$) that combines the structural scores and the recency of the data.\n\n### 4. Structure-Guided Pre-training\nBefore formal training begins, the method uses a **Structure-guided Action Selection** step. It leverages the \"Greedy Structure\" (Theorem 5) to select initial actions that prioritize devices in the \"mandatory scheduling set\" (those with the highest AoI). This provides a \"good\" initial policy, reducing convergence time by approximately 40%.\n\n### 5. Implementation Steps (Algorithm 1)\n1.  **Initialization:** Set environment parameters and random weights for NNs.\n2.  **Trajectory Sampling:** Collect data; use greedy action selection if in the pre-training phase.\n3.  **Structural Evaluation:** Calculate CM, CC, and AM scores for the current trajectory.\n4.  **Buffer Management:** Store high-scoring trajectories in the replay buffer with priority indicators.\n5.  **Loss Calculation:** Compute the unified loss ($L_{SUDO} = L_{On} + \\beta L_{Off}$) for both the Actor and Critic.\n6.  **Parameter Update:** Use the Adam optimizer to update NN weights based on the unified loss.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "Describe the system design, model structure, or theoretical framework. Include any architecture diagrams or tables if present.",
        "answer": "Based on the provided document, the system design, model structure, and theoretical framework for goal-oriented transmission scheduling are detailed as follows:\n\n### 1. System Model\nThe paper considers a wireless goal-oriented communication system consisting of:\n*   **Edge Devices ($N$):** Cameras, sensors, or robots that generate local data.\n*   **Channels ($M$):** Limited wireless subcarriers where $M < N$, requiring efficient scheduling.\n*   **Remote Destination:** A base station or remote estimator that receives data to achieve specific application goals (e.g., remote state estimation).\n*   **Communication Model:** Uses independent and identically distributed (i.i.d.) block fading channels. Channel states are quantized into $\\bar{g}$ levels, each with a specific packet drop rate ($\\psi$).\n*   **Performance Metric:** The primary metric is the **Age of Information (AoI)**, which measures data freshness. The system aims to minimize a long-term discounted cost function $c_n(\\delta_{n,t})$ based on AoI.\n\n### 2. Theoretical Framework\nThe authors formulate the scheduling problem as a **Markov Decision Process (MDP)** and derive several key structural properties of the optimal solution:\n*   **Monotonicity of the Value Function:** Proven with respect to both AoI states (fresher is better) and channel states (better channels reduce long-term cost).\n*   **Asymptotic Convexity:** The first result in literature proving that the optimal state value function is asymptotically convex regarding AoI states.\n*   **Optimal Policy Monotonicity:** If it is optimal to schedule a device on a specific channel, it remains optimal if that channel's quality improves.\n*   **Greedy Structure:** For co-located devices, the optimal policy follows a greedy approach for devices with very high AoI (the \"mandatory scheduling set\").\n\n### 3. SUDO-DRL Architecture\nThe **Structure-guided Unified Dual On-off policy DRL (SUDO-DRL)** is a hybrid algorithm designed to combine the stability of on-policy learning (PPO) with the efficiency of off-policy learning (SAC).\n\n**Architecture Components (as shown in Fig. 3):**\n*   **Trajectory Sampling:** Generates experience data. During pre-training, it uses a \"Greedy Action Selection\" based on Theorem 5.\n*   **Structural Property Evaluation:** A framework (Fig. 2) that calculates scores for:\n    *   **Critic-Monotonicity (CM)**\n    *   **Critic-Convexity (CC)**\n    *   **Actor-Monotonicity (AM)**\n*   **On-Policy Part:** Uses a structure-guided loss function ($L_{On}$) that penalizes the Critic NN if it violates the proven monotonicity or convexity properties.\n*   **Off-Policy Part:** Features a **Structure-Guided Data Storage Scheme**. It only stores transitions in the replay buffer if the trajectory's structural scores exceed the historical average. It also uses **Structural Priority-based Sampling**.\n*   **Unified Loss Function:** Combines both parts: $L_{SUDO} = L_{On} + \\beta L_{Off}$.\n\n### 4. Key Tables and Data\n*   **Table I (Training Hyperparameters):** Lists critical values such as a discount factor ($\\gamma$) of 0.99, clipping parameter ($\\epsilon$) of 0.2, and a unified policy weight ($\\beta$) of 0.9.\n*   **Table II (Empirical Average Cost):** Demonstrates that SUDO-DRL scales to large systems (40 devices, 20 channels) where traditional off-policy methods like DDPG fail to converge. It shows a performance improvement of 25% to 45% over the standard PPO algorithm.\n*   **Performance Visuals:** Figures 5, 6, and 7 show that SUDO-DRL reaches near-perfect structural property scores (CM, CC, AM) much faster and more consistently than standard PPO, explaining its superior scheduling efficiency.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What are the key equations, formulas, or mathematical formulations? Show the notation and mathematical expressions.",
        "answer": "Based on the provided document, the key mathematical formulations, equations, and notations for the goal-oriented transmission scheduling problem are detailed below:\n\n### 1. System Model and Communication Parameters\n*   **Channel State:** $g_{n,m,t} \\in \\mathcal{G} \\triangleq \\{1,\\ldots,\\bar{g}\\}$ represents the state between device $n$ and destination on channel $m$.\n*   **Channel Distribution:** $\\mathrm{P}(g_{n,m,t} = i) = q_{n,m}^i$, where $\\sum_{i=1}^{\\bar{g}} q_{n,m}^i = 1$.\n*   **Channel Assignment Constraint (Eq. 3):**\n    $$\\sum_{n=1}^{N} \\mathbb{1}(a_{n,t} = m) = 1, \\quad \\sum_{m=1}^{M} \\mathbb{1}(a_{n,t} = m) \\leq 1$$\n    This ensures each channel is assigned to one device, and each device gets at most one channel.\n*   **Age of Information (AoI) Dynamics (Eq. 4):**\n    $$\\delta_{n,t+1} = \\begin{cases} 1, & \\text{if successful reception} \\\\ \\delta_{n,t} + 1, & \\text{otherwise} \\end{cases}$$\n\n### 2. Goal-Oriented Cost Functions\n*   **General Objective (Problem 1):** Minimize the infinite-horizon expected discounted cost:\n    $$\\min_{\\pi} \\lim_{T \\to \\infty} \\mathbb{E}^\\pi \\left[ \\sum_{t=1}^{T} \\sum_{n=1}^{N} \\gamma^t c_n(\\delta_{n,t}) \\right]$$\n*   **Remote State Estimation MSE (Example 1):**\n    $$c_n(\\delta_{n,t}) \\triangleq \\operatorname{Tr}(\\mathbf{P}_{n,t}) = \\operatorname{Tr}\\left( h_n^{\\delta_{n,t}}(\\bar{\\mathbf{P}}_n) \\right)$$\n    Where $h_n(\\mathbf{X}) = \\mathbf{A}_n\\mathbf{X}\\mathbf{A}_n^\\top + \\mathbf{W}_n$.\n\n### 3. MDP and Value Functions\n*   **State Transition Probability (Eq. 6):**\n    $$\\mathrm{P}(\\mathbf{s}^+ | \\mathbf{s}, \\mathbf{a}) = \\mathrm{P}(\\delta^+ | \\delta, \\mathbf{G}, \\mathbf{a}) \\mathrm{P}(\\mathbf{G}^+)$$\n*   **Bellman Optimality Equation (Eq. 7):**\n    $$Q(\\mathbf{s}_t, \\mathbf{a}_t) = c(\\mathbf{s}_t) + \\gamma \\sum_{\\mathbf{s}_{t+1}} \\mathrm{Pr}(\\mathbf{s}_{t+1} | \\mathbf{s}_t, \\mathbf{a}_t) \\min_{\\mathbf{a}_{t+1} \\in \\mathcal{A}} Q(\\mathbf{s}_{t+1}, \\mathbf{a}_{t+1})$$\n*   **Optimal State-Value Function (Eq. 9):** $v^*(\\mathbf{s}_t) = \\min_{\\mathbf{a}_t \\in \\mathcal{A}} Q(\\mathbf{s}_t, \\mathbf{a}_t)$.\n\n### 4. SUDO-DRL Structural Property Evaluation\nThe algorithm evaluates the Critic and Actor NNs using the following penalty metrics:\n*   **Critic Monotonicity (AoI & Channel):**\n    $$\\hat{V}_{\\mathrm{AoI}} = \\max(0, v(\\mathbf{s}; \\boldsymbol{\\nu}) - v(\\hat{\\mathbf{s}}_{(n)}; \\boldsymbol{\\nu}))$$\n    $$\\hat{V}_{\\mathrm{Ch}} = \\max(0, v(\\mathbf{s}; \\boldsymbol{\\nu}) - v(\\hat{\\mathbf{s}}_{(n,m)}; \\boldsymbol{\\nu}))$$\n*   **Critic Convexity (AoI):**\n    $$\\check{V}_{\\mathrm{AoI}} = \\max(0, 2v(\\mathbf{s}; \\boldsymbol{\\nu}) - (v(\\check{\\mathbf{s}}_{(n)}; \\boldsymbol{\\nu}) + v(\\hat{\\mathbf{s}}_{(n)}; \\boldsymbol{\\nu})))$$\n*   **Actor Monotonicity (Channel):**\n    $$\\acute{\\Lambda}_{\\mathrm{Ch},n} = \\mathbb{1}(a_n \\neq 0 \\text{ and } a_{\\mathrm{Ch},n} \\neq a_n)$$\n\n### 5. Unified Loss Functions\nSUDO-DRL combines on-policy (On) and off-policy (Off) losses:\n*   **Critic Loss (Eq. 22 & 31):**\n    $$L_{\\mathrm{SUDO}}(\\boldsymbol{\\nu}) = L_{\\mathrm{On}}(\\boldsymbol{\\nu}) + \\beta_1 L_{\\mathrm{Off}}(\\boldsymbol{\\nu})$$\n    $$L_{\\mathrm{On}}(\\boldsymbol{\\nu}) = \\frac{1}{B_1} \\sum_{l=1}^{B_1} \\mathrm{TD}_l^2 + \\frac{1}{K\\Xi} \\sum_{k=1}^{K} \\sum_{\\xi=1}^{\\Xi} (\\hat{V}_{\\mathrm{AoI},k,\\xi} + \\hat{V}_{\\mathrm{Ch},k,\\xi} + \\check{V}_{\\mathrm{AoI},k,\\xi})$$\n*   **Actor Loss (Eq. 23 & 38):**\n    $$L_{\\mathrm{SUDO}}(\\varphi) = L_{\\mathrm{On}}(\\varphi) + \\beta_2 L_{\\mathrm{Off}}(\\varphi)$$\n    $$L_{\\mathrm{Off}}(\\varphi) = \\frac{1}{B_2} \\sum_{b=1}^{B_2} \\left[ \\varpi \\log(\\pi(\\tilde{\\mathbf{a}}_b | \\mathbf{s}_b; \\varphi)) + (c_b + \\gamma v(\\tilde{\\mathbf{s}}_{b+1}; \\boldsymbol{\\nu})) \\right]$$\n\n### 6. Off-Policy Replay Buffer Sampling\n*   **Priority Indicator (Eq. 35):** $p_u = \\mathrm{CM}_u + \\mathrm{CC}_u + \\mathrm{AM}_u$ (Sum of Critic Monotonicity, Critic Convexity, and Actor Monotonicity scores).\n*   **Sampling Probability (Eq. 36):**\n    $$P_b \\triangleq \\frac{p_b \\cdot \\varrho^b}{\\sum_{b=1}^R (p_b \\cdot \\varrho^b)}$$\n    Where $\\varrho \\in (0,1]$ is the decay rate for recency.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What objective function, optimization goal, or theoretical derivation is used?",
        "answer": "Based on the provided document, the objective function, optimization goal, and theoretical derivations for the goal-oriented transmission scheduling problem are detailed as follows:\n\n### 1. Objective Function\nThe problem is formulated as an infinite-horizon Markov Decision Process (MDP). The objective function is the **expected discounted sum of cost functions** across all $N$ devices:\n$$\\min_{\\pi} \\lim_{T \\to \\infty} \\mathbb{E}^{\\pi} \\left[ \\sum_{t=1}^{T} \\sum_{n=1}^{N} \\gamma^{t} c_{n}(\\delta_{n,t}) \\right]$$\n*   **$c_n(\\delta_{n,t})$**: An application-specific cost function (e.g., Mean Square Error in remote estimation) that is non-decreasing with respect to the Age of Information (AoI).\n*   **$\\delta_{n,t}$**: The AoI state of device $n$ at time $t$.\n*   **$\\gamma$**: The discount factor $\\in (0,1)$.\n\n### 2. Optimization Goal\nThe primary goal is to determine an optimal dynamic scheduling policy $\\pi^*$ that minimizes the long-term system cost by deciding which devices should transmit over $M$ available channels ($M < N$) at each time step $t$, given the current AoI states ($\\delta_t$) and channel states ($\\mathbf{G}_t$).\n\n### 3. Theoretical Derivations (Structural Properties)\nThe authors derive several key properties of the optimal Value function ($v^*$) and Policy ($\\pi^*$) to guide the DRL agent:\n\n*   **Monotonicity of $v^*$ w.r.t. Channel States (Theorem 1):** The authors prove that the optimal state-value function is monotonically decreasing with respect to channel quality (higher channel states/better quality lead to lower long-term costs).\n*   **Asymptotic Convexity of $v^*$ w.r.t. AoI (Theorem 3 & Proposition 1):** They establish that $v^*$ is asymptotically convex relative to AoI states. This means that as information becomes increasingly stale, the \"penalty\" or cost to the system grows at an increasing rate.\n*   **Monotonicity of Optimal Policy w.r.t. Channel States (Theorem 4):** If it is optimal to schedule a device on a specific channel, it remains optimal if that channel's quality improves, provided all other variables remain constant.\n*   **Asymptotic Greedy Structure (Theorem 5):** For co-located devices (identical channels), the optimal policy follows a \"greedy\" structure where it is mandatory to schedule devices with the highest AoI (those in the \"mandatory scheduling set\").\n\n### 4. SUDO-DRL Optimization Approach\nThe paper uses these derivations to create the **SUDO-DRL** algorithm, which optimizes via a **Unified Dual On-Off Policy Loss Function**:\n*   **On-Policy Component:** Uses a PPO-based loss but adds a **structural penalty term** (Equation 31) that punishes the Critic NN if its outputs violate the proven monotonicity or convexity properties.\n*   **Off-Policy Component:** Employs a **Structure-Guided Data Storage Scheme**. Only trajectories that align with the theoretical properties (high CM, CC, and AM scores) are stored in the replay buffer.\n*   **Pre-training:** Uses the **Greedy Structure (Theorem 5)** to select actions during an initial phase, providing a \"good\" starting policy rather than training from scratch.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What is the algorithm, procedure, or workflow? Describe the key steps.",
        "answer": "Based on the provided document, the authors propose **SUDO-DRL (Structure-guided Unified Dual On-off policy DRL)**. This hybrid algorithm combines the stability of on-policy training (based on PPO) with the sample efficiency of off-policy methods (based on SAC), while being guided by the theoretical structural properties of the optimal scheduling policy.\n\nThe workflow is divided into three main phases: **Pre-training**, **Experience Generation**, and **Unified Parameter Updating**.\n\n### 1. Pre-training Phase (Structure-Guided Initialization)\nTo avoid starting from scratch, the algorithm uses a \"good\" initial policy derived from the proven **asymptotic greedy structure** (Theorem 5).\n*   **Procedure:** During the first $I_1$ episodes, the agent identifies a \"mandatory scheduling set\" (devices with the highest Age of Information (AoI) exceeding a certain threshold).\n*   **Action Selection:** It prioritizes scheduling these devices regardless of channel state variations. This generates high-quality initial trajectories to populate the replay buffer and stabilize early training.\n\n### 2. Experience Generation & Structural Evaluation\nThe algorithm generates data through a dual-path process:\n*   **On-policy Trajectory Sampling:** The actor network generates a trajectory $\\mathcal{T}_{\\text{On}}$ of length $T$.\n*   **Structural Property Evaluation Framework:** This is a key innovation. The sampled data is evaluated against three derived theoretical properties:\n    1.  **Critic-Monotonicity (CM):** V-function should increase with AoI and channel states.\n    2.  **Critic-Convexity (CC):** V-function should be asymptotically convex w.r.t. AoI.\n    3.  **Actor-Monotonicity (AM):** If a device is scheduled to a channel, it should remain scheduled if that channel's quality improves.\n*   **Score Calculation:** CM, CC, and AM scores are calculated. These scores determine if a trajectory is \"good\" enough to be stored in the off-policy replay buffer.\n\n### 3. Unified Dual On-Off Policy Training\nThe core of the algorithm is the **Unified Loss Function**, which updates the Actor ($\\varphi$) and Critic ($\\nu$) networks by combining two sources:\n\n#### A. On-policy Path (Stability)\n*   Uses the current trajectory to calculate the Advantage function ($A_t$) and Cost-to-go ($C_t$).\n*   **Critic Update:** Includes a standard Temporal Difference (TD) loss plus a **penalty term** for any violations of the structural properties (monotonicity and convexity) detected during evaluation.\n*   **Actor Update:** Uses the standard PPO clipped objective function to ensure stable policy updates.\n\n#### B. Off-policy Path (Efficiency)\n*   **Structure-Guided Storage:** Only trajectories that meet or exceed the average CM, CC, and AM scores are stored in the Replay Buffer ($\\mathcal{R}$).\n*   **Priority-Based Sampling:** Transitions are sampled from the buffer based on a priority indicator ($p$) derived from their structural scores and recency.\n*   **Loss Calculation:** Off-policy losses are calculated using an approximation of the Q-value (based on the V-function) to reduce complexity.\n\n#### C. Parameter Update\nThe final gradients are computed using the unified loss:\n*   $L_{\\text{SUDO}} = L_{\\text{On}} + \\beta L_{\\text{Off}}$\nThe weights $\\beta_1, \\beta_2$ balance the two approaches. The networks are then updated using the Adam optimizer.\n\n### Summary of the Workflow (as per Fig. 3)\n1.  **Sample Trajectory** (using greedy guidance if in pre-training).\n2.  **Evaluate Structural Scores** (CM, CC, AM).\n3.  **Filter & Store** high-score transitions into the **Replay Buffer**.\n4.  **Sample Mini-batches** from both the current trajectory (On-policy) and the Replay Buffer (Off-policy).\n5.  **Compute Unified Loss** (incorporating structural penalties in the on-policy critic loss).\n6.  **Update Neural Networks** and repeat.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What are the key parameters, settings, or implementation details mentioned?",
        "answer": "Based on the provided document, the key parameters, settings, and implementation details for the goal-oriented transmission scheduling system and the SUDO-DRL algorithm are detailed below:\n\n### 1. System Model & Communication Parameters\n*   **Network Scale:** The experiments evaluate various scales, ranging from small **(10 devices, 5 channels)** to large-scale scenarios with **40 devices and 20 channels**.\n*   **Channel Model:** Independent and identically distributed (i.i.d.) block fading channels.\n*   **Channel States ($\\mathcal{G}$):** Quantized into **$\\bar{g} = 5$ levels**.\n*   **Packet Drop Rates:** Corresponding to the 5 channel levels, the rates are set at **0.2, 0.15, 0.1, 0.05, and 0.01**. These are derived from a Rayleigh distribution with a scale parameter between 0.5 and 2.\n*   **Goal-Oriented Metric:** Age of Information (AoI) and specifically **Estimation Mean-Square Error (MSE)** for a remote state estimation system.\n*   **LTI System Dynamics:** For each device $n$, the state dimension $r_n = 2$ and measurement dimension $e_n = 1$. System matrices $\\mathbf{A}_n$ have spectral radii uniformly drawn from **(1, 1.3)**.\n\n### 2. SUDO-DRL Algorithm Architecture\nThe algorithm is a hybrid \"Unified Dual On-Off Policy\" approach combining **PPO (on-policy)** and **SAC (off-policy)** elements.\n*   **Neural Networks:** \n    *   **Actor NN:** Approximates the policy $\\pi$; input dimension is $N + (N \\times M)$. Output is $2N$ (for SUDO and PPO).\n    *   **Critic NN:** Approximates the state-value function $v$; input dimension is $N + (N \\times M)$. Output is 1.\n    *   **Structure:** Both are fully connected NNs with **three hidden layers**.\n*   **Loss Functions:** \n    *   **Critic Loss:** Combines Temporal Difference (TD) error with structural penalties for violations of **monotonicity** (w.r.t. AoI and channel) and **asymptotic convexity** (w.r.t. AoI).\n    *   **Actor Loss:** Combines the PPO clipped objective with an off-policy SAC-based entropy term.\n\n### 3. Training Hyperparameters (Table I)\n*   **Learning Rates:** Critic = **0.001**, Actor = **0.0001** (with a decay rate of 0.001).\n*   **Optimization:** **Adam Optimizer**.\n*   **Discount Factor ($\\gamma$):** 0.99.\n*   **GAE Parameter ($\\lambda$):** 0.99.\n*   **PPO Clipping ($\\epsilon$):** 0.2.\n*   **Batch Sizes:** Both On-policy ($B_1$) and Off-policy ($B_2$) are set to **128**.\n*   **Replay Buffer ($R$):** Size of **200** trajectories.\n*   **Training Duration:** Total of **10,000 episodes**; Time horizon ($T$) per episode is **128**.\n\n### 4. Structural Guidance Implementation\n*   **Pre-training:** Uses a **Structure-Guided Action Selection** (based on Theorem 5) for the first $10 \\times N$ episodes to provide a \"good\" initial policy.\n*   **Score-Based Storage:** Trajectories are only stored in the off-policy replay buffer if they meet or exceed the average **Critic-Monotonicity (CM)**, **Critic-Convexity (CC)**, and **Actor-Monotonicity (AM)** scores of the past 50 trajectories.\n*   **Priority Sampling:** Sampling from the buffer is weighted by a priority indicator ($p = CM + CC + AM$) and a recency decay rate ($\\varrho = 0.95$).\n*   **Sampling for Evaluation:** $K=50$ state-action pairs are sampled per trajectory, testing $\\Xi=4$ AoI and channel states to calculate structural scores.\n\n### 5. Performance Results\n*   **Efficiency:** SUDO-DRL reduces convergence time by **40%** compared to versions without pre-training.\n*   **Effectiveness:** Improves system performance (MSE reduction) by **25% to 45%** over the benchmark PPO.\n*   **Scalability:** Successfully converges in a **40-device/20-channel** environment where standard off-policy DRL (DDPG) fails to converge.",
        "mode": "fast_direct_with_vision",
        "success": true
      }
    ],
    "results": [
      {
        "query": "What datasets, benchmarks, or experimental setups are used for evaluation?",
        "answer": "Based on the provided document, the evaluation of the proposed SUDO-DRL algorithm is conducted using the following datasets, benchmarks, and experimental setups:\n\n### 1. Experimental Setup\nThe experiments are modeled on a **Remote State Estimation System** (as described in Example 1 of the paper).\n*   **System Model:** $N$ edge devices (sensors) transmitting to a remote destination through $M$ channels ($M < N$).\n*   **Dynamic Processes:** Each sensor measures a process modeled as a discrete-time linear time-invariant (LTI) system.\n    *   **State/Measurement Dimensions:** Process state $r_n = 2$; measurement $c_n = 1$.\n    *   **System Matrices ($A_n$):** Randomly generated with spectral radii uniformly drawn from $(1, 1.3)$.\n*   **Communication Model:**\n    *   **Channels:** Independent and identically distributed (i.i.d.) block fading channels.\n    *   **Quantization:** Channel states are quantized into $\\bar{g} = 5$ levels.\n    *   **Packet Drop Rates:** Set to $0.2, 0.15, 0.1, 0.05,$ and $0.01$ (derived from a Rayleigh distribution with scale parameters between $0.5$ and $2$).\n*   **Hardware/Software:** Intel Core i7 9700 CPU, 32GB RAM, and an NVIDIA RTX 3060Ti GPU.\n\n### 2. Benchmarks\nThe paper compares SUDO-DRL against several state-of-the-art Deep Reinforcement Learning (DRL) algorithms:\n*   **On-policy Benchmark:**\n    *   **PPO (Proximal Policy Optimization):** The standard on-policy algorithm which SUDO-DRL builds upon.\n*   **Off-policy Benchmarks:**\n    *   **DDPG (Deep Deterministic Policy Gradient):** A fundamental off-policy actor-critic method.\n    *   **SE-DDPG (Structure-Enhanced DDPG):** A previous state-of-the-art method that incorporates monotonicity properties.\n    *   **MRI-DDPG (Monotonicity-Regularized DDPG):** A state-of-the-art approach for goal-oriented scheduling.\n\n### 3. Evaluation Scenarios (System Scales)\nThe performance is tested across **16 different parameter settings** (Para. 1-16) and four primary system scales $(N, M)$:\n*   **Small-scale:** (10 devices, 5 channels)\n*   **Medium-scale:** (20 devices, 10 channels)\n*   **Large-scale:** (30 devices, 15 channels) and **(40 devices, 20 channels)**.\n\n### 4. Performance Metrics\nThe algorithms are evaluated using:\n*   **Empirical Average Sum MSE Cost:** The primary goal-oriented metric (Mean Square Error of the remote estimation).\n*   **Convergence Time/Speed:** Measured by the number of episodes required to reach a stable cost.\n*   **Structural Property Scores:**\n    *   **CM (Critic-Monotonicity):** Measures how well the critic NN follows proven monotonicity w.r.t. channel and AoI states.\n    *   **CC (Critic-Convexity):** Measures adherence to the proven asymptotic convexity of the value function.\n    *   **AM (Actor-Monotonicity):** Measures the monotonicity of the policy w.r.t. channel states.\n\n### 5. Key Findings from Evaluation\n*   **Scalability:** Off-policy benchmarks (DDPG, SE-DDPG, MRI-DDPG) failed to converge in systems larger than 20 devices.\n*   **Efficiency:** SUDO-DRL improved system performance by **25% to 45%** and reduced convergence time by **40%** compared to the PPO benchmark.\n*   **Structural Adherence:** As shown in Figures 5, 6, and 7, SUDO-DRL reaches near-perfect structural scores (100) much faster than PPO, which struggles particularly with convexity and actor monotonicity.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What evaluation metrics or criteria are used to measure performance?",
        "answer": "Based on the provided document, the performance of the transmission scheduling system and the SUDO-DRL algorithm is measured using several distinct categories of metrics:\n\n### 1. Primary System Performance Metric\n*   **Sum of Cost Functions (MSE):** The central objective is to minimize the infinite-horizon expected sum of cost functions across all $N$ devices. \n*   **Remote State Estimation (Example 1):** In the specific case of remote state estimation, the cost function $c_n(\\delta_{n,t})$ is defined as the **Estimation Mean-Square Error (MSE)**. This measures the error covariance of the remote estimate, which is a function of the **Age of Information (AoI)**.\n*   **Age of Information (AoI):** Defined as the time elapsed since the last successful packet reception ($\\delta_{n,t}$). It serves as a proxy for information freshness and directly influences the cost function.\n\n### 2. DRL Training and Efficiency Metrics\nThe paper evaluates the effectiveness of the SUDO-DRL algorithm using the following:\n*   **Average Sum MSE Cost:** Used to plot learning curves (Fig. 4) to compare the final performance of different algorithms.\n*   **Convergence Time:** Measured by the number of episodes required to reach a stable, minimum cost. SUDO-DRL is noted to reduce convergence time by approximately 40%.\n*   **Scalability:** The ability of the algorithm to converge in large-scale systems (e.g., 40 devices and 20 channels), where traditional off-policy methods like DDPG fail.\n\n### 3. Structural Property Evaluation Scores\nA novel framework (illustrated in **Fig. 2**) is used to measure how well the neural networks adhere to theoretically proven optimal properties. These are expressed as scores from 0 to 100:\n*   **Critic-Monotonicity (CM) Score:** Measures if the Critic NN respects the property that the value function should increase as AoI or channel states worsen (based on Lemma 1 and Theorem 1).\n*   **Critic-Convexity (CC) Score:** Measures if the Critic NN follows the proven asymptotic convexity of the value function with respect to AoI states (Theorem 3).\n*   **Actor-Monotonicity (AM) Score:** Measures if the Actor NN follows the property that if a device is scheduled on a channel, it should remain scheduled if that channel's quality improves (Theorem 4).\n\n### 4. Algorithmic Stability and Efficiency\n*   **Sample Efficiency:** The ability to learn effectively from fewer data points by combining on-policy stability with off-policy data reuse (via the replay buffer).\n*   **Training Stability:** Measured by the lack of bias and fluctuations during the policy update process, primarily attributed to the unified dual on-off policy approach.\n\n### 5. Numerical Comparison Criteria\nIn **Table II**, the algorithms are compared across 16 different parameter settings (\"Para. 1-16\") based on:\n*   **Empirical Average Cost:** The average MSE calculated over 20,000-step simulations after training is complete.\n*   **Success Rate:** Whether an algorithm can successfully converge (\"—\" indicates failure to converge) for a given system scale $(N, M)$.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What are the main results shown in the main results table?",
        "answer": "Based on the document provided, the main results are presented in **Table II: Empirical Average Cost of the SUDO-DRL Algorithm and the Benchmarks**. The table compares the performance of the proposed **SUDO-DRL** against four state-of-the-art methods across 16 different parameter settings and four system scales.\n\nThe key findings from this table are:\n\n### 1. Superior Scalability\nThe table demonstrates that SUDO-DRL is significantly more robust as system complexity increases:\n*   **Small Scale (10 devices, 5 channels):** All algorithms (DDPG, SE-DDPG, MRI-DDPG, PPO, and SUDO-DRL) are generally able to converge.\n*   **Medium Scale (20 devices, 10 channels):** Standard DDPG fails to converge (indicated by \"—\"), while structure-enhanced versions (SE-DDPG, MRI-DDPG) still function.\n*   **Large Scale (30+ devices, 15-20 channels):** All off-policy benchmarks (DDPG, SE-DDPG, and MRI-DDPG) **fail to converge**. Only the on-policy PPO and the proposed SUDO-DRL are capable of handling these high-dimensional spaces.\n\n### 2. Significant Performance Gains\nSUDO-DRL consistently achieves the lowest (best) empirical average MSE cost in large-scale scenarios:\n*   **Compared to PPO:** In the largest tested scenario (40 devices, 20 channels), SUDO-DRL reduces the average cost by approximately **25% to 40%** compared to the standard PPO algorithm.\n*   **Consistency:** Across all 16 parameter settings, SUDO-DRL (highlighted in bold in the table) consistently outperforms the on-policy benchmark (PPO).\n\n### 3. Comparison with Specialized Off-Policy Methods\n*   In small-scale systems (10, 5), the table shows that SUDO-DRL is comparable to, though sometimes slightly behind, specialized off-policy methods like SE-DDPG. The authors note this is because off-policy methods are highly efficient in smaller, less complex environments.\n*   However, as the system grows, those specialized methods become non-functional, whereas SUDO-DRL maintains high performance.\n\n### 4. Summary of Numerical Improvements\nThe text accompanying the table (and the abstract) synthesizes these results, noting that SUDO-DRL:\n*   Improves system performance by **up to 45%**.\n*   Reduces convergence time by **40%** (as supported by the training curves in Fig. 4).\n*   Successfully manages systems with up to **40 devices and 20 channels**, a scale where previous structure-guided off-policy DRL methods were limited to roughly 20 sensors.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "How does the proposed method compare to baseline methods? Show the comparison.",
        "answer": "Based on the provided document, the proposed **SUDO-DRL (Structure-guided Unified Dual On-off policy DRL)** method significantly outperforms baseline methods in terms of performance, scalability, and training efficiency.\n\n### 1. Performance and Scalability Comparison\nThe paper compares SUDO-DRL against several state-of-the-art algorithms: **PPO** (on-policy benchmark), **DDPG** (off-policy benchmark), **SE-DDPG**, and **MRI-DDPG** (structure-guided off-policy benchmarks).\n\n*   **System Performance:** SUDO-DRL improves system performance (measured by Mean Square Error/MSE) by **25% to 45%** compared to the PPO benchmark.\n*   **Scalability:** SUDO-DRL demonstrates superior scalability. It effectively handles large-scale systems with up to **40 devices and 20 channels**. \n    *   **Off-policy baselines (DDPG, SE-DDPG, MRI-DDPG)** fail to converge entirely in these large-scale setups.\n    *   **On-policy baseline (PPO)** converges but exhibits significant performance loss (higher MSE) compared to SUDO-DRL.\n*   **Small-Scale Systems:** In smaller systems (e.g., 10 devices, 5 channels), SUDO-DRL achieves performance comparable to advanced off-policy methods like SE-DDPG, though it is noted to be slightly worse because off-policy methods are highly specialized for small-scale optimality.\n\n### 2. Training Efficiency\n*   **Convergence Time:** SUDO-DRL reduces convergence time by approximately **40%** compared to versions without its specific structure-guided enhancements.\n*   **Stability:** By combining on-policy stability with off-policy sample efficiency, it avoids the training instability and bias often found in pure off-policy methods (like DDPG) while overcoming the data inefficiency of pure on-policy methods (like PPO).\n\n### 3. Structural Property Alignment\nThe paper uses \"Structure Scores\" (CM, CC, and AM) to show how well the NNs adhere to theoretical optimal properties (monotonicity and convexity). The comparison in **Figures 5, 6, and 7** shows:\n*   **Critic Monotonicity (CM):** SUDO-DRL reaches a perfect score (100) almost immediately, whereas PPO takes 1,000 episodes.\n*   **Critic Convexity (CC):** SUDO-DRL reaches a perfect score within 200 episodes, while PPO never exceeds a score of 80.\n*   **Actor Monotonicity (AM):** SUDO-DRL reaches a perfect score by 2,000 episodes, while PPO plateaus below 75.\n\n### Summary Table (Based on Table II)\n| System Scale (N, M) | DDPG | SE-DDPG / MRI-DDPG | PPO | SUDO-DRL |\n| :--- | :--- | :--- | :--- | :--- |\n| **Small (10, 5)** | Works | **Best Performance** | Poor Performance | High Performance |\n| **Medium (20, 10)** | Fails | Works | Poor Performance | **Best Performance** |\n| **Large (40, 20)** | Fails | **Fails** | Works (High Cost) | **Best Performance (Low Cost)** |\n\n### Key Advantages of the Proposed Method\n1.  **Hybrid Approach:** It uses a unified loss function that integrates the stability of PPO with the sample efficiency of SAC (Soft Actor-Critic).\n2.  **Structure-Guided Storage:** It selectively stores \"high-quality\" data in the replay buffer only if the trajectory aligns with proven structural properties (monotonicity/convexity).\n3.  **Pre-training:** It uses a greedy structure (Theorem 5) to initialize the policy, leading to much faster initial learning.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What performance does the method achieve? Report the exact numbers from experiments.",
        "answer": "Based on the numerical experiments detailed in Section VI and the corresponding figures, the proposed **SUDO-DRL** method achieves the following performance results:\n\n### **1. Overall System Performance (MSE Cost)**\n*   **Reduction in Cost:** SUDO-DRL improves system performance by **25% to 45%** compared to state-of-the-art methods (specifically the benchmark on-policy PPO algorithm).\n*   **Large-Scale Scenarios:** In a system with **40 devices and 20 channels** (the largest scale tested), SUDO-DRL reduced the average sum MSE cost by approximately **35%** compared to PPO (as shown in Fig. 4).\n*   **Empirical Average Cost (Table II):** Across 16 different parameter settings, SUDO-DRL consistently outperformed PPO. For example:\n    *   **Scale (30, 15):** Achieved costs as low as **518.03** compared to PPO's **900.71**.\n    *   **Scale (40, 20):** Achieved costs as low as **689.81** compared to PPO's **971.35**.\n\n### **2. Training Efficiency and Convergence**\n*   **Convergence Speed:** The method reduces convergence time by approximately **40%** compared to state-of-the-art methods.\n*   **Impact of Pre-training:** The use of the structure-guided pre-training stage allowed the algorithm to converge in significantly fewer episodes—achieving over **40% faster convergence** than SUDO-DRL without pre-training (Fig. 4).\n\n### **3. Scalability**\n*   **Robustness:** SUDO-DRL successfully handles large-scale systems (up to **40 devices and 20 channels**). \n*   **Benchmark Failure:** In these large-scale setups, off-policy benchmarks like **DDPG, SE-DDPG, and MRI-DDPG failed to converge**, while the standard on-policy PPO exhibited significant performance loss.\n\n### **4. Structural Property Alignment (Scores)**\nThe method ensures that the neural networks adhere to theoretical optimal properties, measured on a scale of 0–100:\n*   **Critic Monotonicity (CM):** Achieved a full score of **100** very quickly (Fig. 5).\n*   **Critic Convexity (CC):** Guaranteed a full score of **100** after approximately **200 episodes**, whereas the PPO benchmark remained below 80 (Fig. 6).\n*   **Actor Monotonicity (AM):** Reached a score of **100** after **2000 episodes**, while PPO failed to exceed 75 (Fig. 7).",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What ablation study or sensitivity analysis is conducted? What are the findings?",
        "answer": "Based on the provided document, the authors conduct a comprehensive evaluation of the **SUDO-DRL** algorithm, which serves as a form of structural ablation and sensitivity analysis by comparing it against standard on-policy and off-policy benchmarks.\n\nThe analysis focuses on three primary areas: the contribution of the **pre-training stage**, the impact of **system scale (scalability)**, and the effectiveness of the **structural property scores**.\n\n### 1. Ablation of the Pre-training Stage\nThe authors specifically analyze the impact of the \"Structure-Guided Action Selection\" used during pre-training (based on the greedy structure derived in Theorem 5).\n*   **The Study:** They compare SUDO-DRL with a version of itself that excludes the pre-training phase (labeled \"SUDO-DRL without pre-training\" in Fig. 4).\n*   **Findings:** \n    *   **Convergence Speed:** The pre-training stage reduces convergence time by approximately **40%**.\n    *   **Final Performance:** The version with pre-training achieves a lower average cost (MSE), suggesting that the structural guidance helps the agent avoid local minima and find a better initialization point for formal training.\n\n### 2. Sensitivity to System Scale (Scalability Analysis)\nThe study tests the algorithms across 16 different parameter settings and four system scales: (10 devices, 5 channels), (20, 10), (30, 15), and (40, 20).\n*   **The Study:** Comparing SUDO-DRL against off-policy (DDPG, SE-DDPG, MRI-DDPG) and on-policy (PPO) benchmarks as the state/action space dimensions increase.\n*   **Findings:**\n    *   **Off-policy Failure:** Standard off-policy methods (DDPG) only work at the smallest scale. State-of-the-art variants (SE-DDPG, MRI-DDPG) fail to converge once the system reaches 30 devices.\n    *   **On-policy Performance Loss:** While the on-policy benchmark (PPO) converges in large systems, it exhibits significant performance loss.\n    *   **SUDO-DRL Superiority:** SUDO-DRL is the only algorithm that remains both stable and efficient at the largest scale (40 devices/20 channels), improving system performance by **25% to 45%** over PPO (Table II).\n\n### 3. Structural Property Score Evaluation\nThe authors evaluate how well the NNs adhere to the derived theoretical properties (Monotonicity and Convexity) during the training process.\n*   **The Study:** They track three metrics: **Critic-Monotonicity (CM)**, **Critic-Convexity (CC)**, and **Actor-Monotonicity (AM)** scores (Figs. 5, 6, and 7).\n*   **Findings:**\n    *   **Critic Monotonicity:** SUDO-DRL reaches a perfect score (100) almost immediately, whereas PPO takes 1,000 episodes.\n    *   **Critic Convexity:** SUDO-DRL guarantees a full score after 200 episodes, while PPO never rises above 80.\n    *   **Actor Monotonicity:** SUDO-DRL reaches a full score after 2,000 episodes, while PPO plateaus below 75.\n*   **Conclusion:** This analysis proves that standard DRL (PPO) struggles to learn the underlying mathematical structure of the optimal policy (especially convexity), which explains the performance gap.\n\n### Summary of Findings\nThe analysis demonstrates that **SUDO-DRL** successfully bridges the gap between on-policy stability and off-policy efficiency. By enforcing structural properties (monotonicity and convexity) through its unified loss function and using a structure-guided replay buffer, it achieves **faster convergence, better scalability, and higher accuracy** in goal-oriented communication tasks.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What analysis, case study, or discussion of the results is provided?",
        "answer": "Based on the provided document, the analysis and discussion of results are centered on the theoretical derivation of structural properties and the empirical validation of the proposed **SUDO-DRL** algorithm.\n\n### 1. Theoretical Analysis of Structural Properties\nThe authors provide a rigorous mathematical analysis of the optimal scheduling policy, which serves as the foundation for their DRL algorithm. Key findings include:\n*   **Monotonicity of the Value Function:** The authors prove that the optimal state-value function ($v^*$) is monotonically increasing with respect to both **Age of Information (AoI) states** (Lemma 1) and **channel states** (Theorem 1). This implies that system performance degrades as data becomes older or channel conditions worsen.\n*   **Asymptotic Convexity:** For the first time in transmission scheduling literature, the authors prove the **asymptotic convexity** of the value function with respect to AoI states (Theorem 3). This suggests that the \"cost\" of outdated information increases at an accelerating rate as AoI grows.\n*   **Optimal Policy Structure:** Theorem 4 establishes the monotonicity of the optimal policy regarding channel states, while Theorem 5 identifies an **asymptotic greedy structure** for co-located devices. This means that in certain conditions, the optimal policy aligns with a \"greedy\" approach that prioritizes devices with the highest immediate cost.\n\n### 2. Case Study: Remote State Estimation System\nThe paper uses a **Remote State Estimation System** (Example 1) as its primary case study to evaluate the goal-oriented framework.\n*   **Setup:** The system involves $N$ sensors measuring linear time-invariant (LTI) processes and sending estimates to a remote estimator.\n*   **Metric:** The performance goal is to minimize the **Mean Square Error (MSE)** of the remote estimates.\n*   **Linkage:** The authors prove in Lemma 2 that the MSE cost function in this system is asymptotically convex, justifying the use of convexity-based guidance in their DRL model.\n\n### 3. Discussion of Numerical Results\nThe results are discussed through comparisons with state-of-the-art algorithms (PPO, DDPG, SE-DDPG, and MRI-DDPG) across 16 different parameter settings:\n\n*   **Performance and Scalability:**\n    *   SUDO-DRL outperforms the benchmark PPO by **25% to 45%** in terms of reducing the average sum MSE.\n    *   **Scalability:** SUDO-DRL successfully handles large-scale systems (up to **40 devices and 20 channels**). In contrast, off-policy benchmarks like DDPG fail to converge at this scale, and PPO exhibits significant performance loss (Fig. 4 and Table II).\n*   **Convergence Efficiency:**\n    *   The inclusion of a **pre-training stage** (guided by the greedy structure in Theorem 5) reduces convergence time by approximately **40%** (Fig. 4).\n*   **Structural Property Scores (Figures 5, 6, and 7):**\n    *   The authors use a \"score\" system (CM, CC, and AM) to track how well the NNs adhere to the proven theoretical properties during training.\n    *   **Critic Monotonicity (CM):** SUDO-DRL reaches a perfect score almost immediately, whereas PPO takes 1,000 episodes.\n    *   **Critic Convexity (CC) and Actor Monotonicity (AM):** SUDO-DRL maintains near-perfect scores, while PPO struggles significantly (staying below 80 for CC and 75 for AM). This confirms that SUDO-DRL’s superior performance is directly linked to its ability to enforce these theoretical structures.\n\n### 4. Summary of SUDO-DRL Innovation\nThe discussion highlights that the algorithm's success stems from its **Unified Dual Approach**:\n1.  **On-policy:** Provides training stability via a structure-guided loss function.\n2.  **Off-policy:** Enhances sample efficiency by using a **Structure-Guided Data Storage Scheme**, which only keeps high-quality transitions in the replay buffer based on their structural property scores.",
        "mode": "fast_direct_with_vision",
        "success": true
      }
    ],
    "contributions": [
      {
        "query": "What are the main contributions listed in the introduction or conclusion?",
        "answer": "Based on the provided document, the main contributions of this work are detailed in the introduction and summarized in the conclusion. They can be categorized into three primary areas:\n\n### 1. Theoretical Derivation of Structural Properties\nThe authors establish several key structural properties of the optimal solution for goal-oriented transmission scheduling (which considers both Age of Information (AoI) and channel states):\n*   **Monotonicity of the Optimal V Function:** They prove that the optimal state-value function is monotonically increasing with respect to **channel states** (Theorem 1), complementing previous work that showed monotonicity regarding AoI states.\n*   **Asymptotic Convexity:** They provide the first result in the literature to prove the **asymptotic convexity** of the state-value function with respect to AoI states in transmission scheduling problems (Theorem 3 and Proposition 1).\n*   **Monotonicity of the Optimal Policy:** They derive the monotonicity of the optimal scheduling policy with respect to channel states (Theorem 4).\n*   **Greedy Structure:** They establish an **asymptotic greedy structure** for the optimal policy in systems with co-located devices (Theorem 5), suggesting that devices with the highest AoI (above a certain threshold) must be scheduled.\n\n### 2. Development of the SUDO-DRL Algorithm\nThe paper proposes the **Structure-guided Unified Dual On-off policy DRL (SUDO-DRL)**, a hybrid algorithm designed to solve large-scale MDPs:\n*   **Hybrid Approach:** It combines the **stability of on-policy training** (based on PPO) with the **sample efficiency of off-policy methods** (based on SAC) through a unified loss function.\n*   **Structural Property Evaluation Framework:** As shown in **Fig. 2**, the algorithm calculates \"scores\" for critic-monotonicity (CM), critic-convexity (CC), and actor-monotonicity (AM).\n*   **Structure-Guided Loss and Buffer Management:** \n    *   The **on-policy loss function** incorporates penalties for violating the derived structural properties.\n    *   The **off-policy replay buffer** uses a selective storage scheme (storing only high-quality data that aligns with structural properties) and a priority-based sampling method.\n*   **Pre-training Stage:** It utilizes the \"greedy structure\" derived in the theory to guide action selection during an initial pre-training phase to find a better starting policy.\n\n### 3. Numerical Validation and Scalability\nThe authors demonstrate the efficacy of SUDO-DRL through extensive experiments (as seen in **Table II** and **Figs. 4-7**):\n*   **Performance Gains:** SUDO-DRL improves system performance (reduces MSE cost) by **25% to 45%** compared to state-of-the-art methods like PPO.\n*   **Efficiency:** It reduces convergence time by approximately **40%**.\n*   **Scalability:** The algorithm successfully handles large-scale systems (up to **40 devices and 20 channels**). This is a significant contribution because benchmark off-policy algorithms (like DDPG) fail to converge at this scale, and standard on-policy benchmarks (like PPO) exhibit significant performance loss.",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What is novel or new about this work compared to existing methods?",
        "answer": "Based on the provided document, this work introduces several novel theoretical and algorithmic contributions to the field of goal-oriented transmission scheduling. The key innovations are detailed below:\n\n### 1. New Theoretical Structural Properties\nWhile previous works (including the authors' own earlier research) identified basic monotonicity, this paper significantly advances the theoretical framework by proving three new properties of the optimal solution:\n*   **Asymptotic Convexity of the Value Function:** This is the first result in the literature to explore and prove the convexity of the optimal state-value function ($v^*$) with respect to Age of Information (AoI) states. The authors prove this for two-device systems (Theorem 2) and establish asymptotic convexity for multi-device-multi-channel systems (Theorem 3).\n*   **Monotonicity w.r.t. Channel States:** The authors prove that the optimal $v^*$ function is monotonically decreasing with respect to channel states (Theorem 1), complementing existing knowledge of monotonicity regarding AoI.\n*   **Asymptotic Greedy Structure:** The paper proves that for co-located devices (identical channel conditions), the optimal policy follows a \"greedy\" structure where devices in a \"mandatory scheduling set\" (those with the highest AoI) must be scheduled (Theorem 5).\n\n### 2. SUDO-DRL: A Hybrid \"Dual\" Architecture\nMost existing methods use either **on-policy** DRL (stable but sample-inefficient, like PPO) or **off-policy** DRL (efficient but unstable in large scales, like DQN/DDPG). This work proposes the **Structure-guided Unified Dual On-off policy DRL (SUDO-DRL)**, which is novel in its hybrid approach:\n*   **Unified Loss Function:** It combines the stability of on-policy training with the sample efficiency of off-policy methods through a single unified loss function ($L_{SUDO} = L_{On} + \\beta L_{Off}$).\n*   **Structural Property Evaluation Framework:** As shown in **Fig. 2**, the algorithm introduces a framework that calculates \"scores\" for Critic-Monotonicity (CM), Critic-Convexity (CC), and Actor-Monotonicity (AM). These scores are used to penalize the NNs when they violate the proven theoretical properties.\n\n### 3. Novel Replay Buffer Management\nUnlike standard off-policy methods that store all experiences, SUDO-DRL uses a **Structure-Guided Data Storage Scheme**:\n*   **Selective Storage:** It only stores transitions in the replay buffer if the trajectory's structural scores (CM, CC, AM) meet specific constraints (Equation 34).\n*   **Priority-Based Sampling:** It uses a priority indicator ($p$) based on these structural scores to sample \"high-quality\" data that aligns with theoretical optimality, rather than just sampling based on temporal recency or TD-error.\n\n### 4. Structure-Guided Pre-training\nThe algorithm introduces a specific pre-training stage (Algorithm 1, Step 6) that uses the **Greedy Structure** derived in Theorem 5. This allows the agent to find a \"good\" initial policy quickly, rather than starting from a random state. This was shown to reduce convergence time by 40% (**Fig. 4**).\n\n### 5. Superior Scalability\nA major practical novelty is the demonstrated scalability. Existing off-policy methods (DDPG, SE-DDPG) failed to converge in large-scale scenarios (e.g., 40 devices, 20 channels). SUDO-DRL not only converges in these environments but outperforms the standard on-policy benchmark (PPO) by **25% to 45%** in system performance (**Table II**).",
        "mode": "fast_direct_with_vision",
        "success": true
      },
      {
        "query": "What limitations does the paper acknowledge? What future directions are suggested?",
        "answer": "Based on the provided document, the paper acknowledges several limitations and suggests specific future research directions to build upon the SUDO-DRL framework.\n\n### **Acknowledged Limitations**\n\nThe paper identifies limitations in both existing methodologies and the specific scope of the current study:\n\n*   **Computational Infeasibility of Conventional Methods:** The authors note that traditional dynamic programming methods (like value and policy iteration) are computationally infeasible for large-scale systems due to high-dimensional state and action spaces.\n*   **Suboptimality of Heuristics:** Heuristic methods, while efficient, cannot guarantee optimality in goal-oriented scheduling.\n*   **DRL Algorithm Trade-offs:**\n    *   **Off-policy DRL (e.g., DQN, DDPG):** While sample-efficient, these methods suffer from training instability and bias. The paper specifically notes that state-of-the-art off-policy algorithms (like SE-DDPG) fail to converge in large-scale setups (e.g., 40 devices and 20 channels).\n    *   **On-policy DRL (e.g., PPO):** These methods are stable but exhibit poor data efficiency and insufficient exploration, often getting stuck in local minima.\n*   **Theoretical Constraints:**\n    *   **Convexity Proofs:** While the authors prove asymptotic convexity for multi-device systems, they acknowledge that proving absolute convexity for general multi-device-multi-channel systems is extremely challenging due to the increased dimensionality of state and action spaces.\n    *   **Channel State Convexity:** The authors explicitly state they did not derive convexity for channel states because it is \"neither meaningful nor necessary,\" as the cost function fundamentally depends on AoI rather than channel quality.\n    *   **Policy Monotonicity:** The paper mentions that general results for policy monotonicity in terms of AoI have not yet been derived for multi-device systems (only for special cases like two-device-single-channel).\n*   **Scenario Specificity:** Some theoretical results (like the greedy structure of the optimal policy) were only derived for the special case of \"co-located devices\" with identical channel states.\n\n### **Suggested Future Directions**\n\nThe paper outlines several avenues for future research to enhance goal-oriented communication systems:\n\n*   **Expansion of Theoretical Framework:** The authors suggest exploring additional structural properties of the optimal solution to further refine the guidance provided to DRL algorithms.\n*   **Comprehensive Resource Allocation:** Future work aims to extend the SUDO-DRL algorithm beyond simple transmission scheduling to include:\n    *   **Power Allocation:** Optimizing the energy used for transmissions alongside scheduling.\n    *   **Advanced Multiple Access Schemes:** Integrating techniques like Non-Orthogonal Multiple Access (NOMA) into the scheduling framework.\n*   **Broader Application Goals:** While this paper focuses heavily on Age of Information (AoI) and remote state estimation, future directions involve applying these structure-guided methods to other goal-oriented metrics and application-specific objectives in 6G networks.\n*   **Scalability Enhancements:** Continuing to improve the scalability of these algorithms to handle even larger and more complex network environments where current benchmarks exhibit performance loss.",
        "mode": "fast_direct_with_vision",
        "success": true
      }
    ]
  },
  "markdown_paths": [
    "Z:\\PythonWorkSpace\\sophyverse-platform-backend\\src\\slide-svc\\data\\temp\\1b9a609e-fce7-49de-a3fc-cb828a2de68b\\2501.11921.md"
  ],
  "input_path": "Z:\\PythonWorkSpace\\sophyverse-platform-backend\\src\\slide-svc\\data\\temp\\1b9a609e-fce7-49de-a3fc-cb828a2de68b\\2501.11921.md",
  "content_type": "paper",
  "mode": "fast"
}