# A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models  

YINPENG CAI†, LEXIN LI‡¹, AND LINJUN ZHANG*¹  

†Peking University, ‡University of California at Berkeley, *Rutgers University  

# Abstract  

Large Language Models (LLMs) are rapidly gaining enormous popularity in recent years. However, the training of LLMs has raised significant privacy and legal concerns, particularly regarding the inclusion of copyrighted materials in their training data without proper attribution or licensing, an issue that falls under the broader concern of data misappropriation. In this article, we focus on a specific problem of data misappropriation detection, namely, to determine whether a given LLM has incorporated the data generated by another LLM. We propose embedding watermarks into the copyrighted training data and formulating the detection of data misappropriation as a hypothesis testing problem. We develop a general statistical testing framework, construct test statistics, determine optimal rejection thresholds, and explicitly control type I and type II errors. Furthermore, we establish the asymptotic optimality properties of the proposed tests, and demonstrate the empirical effectiveness through intensive numerical experiments.  

# 1 Introduction  

Large Language Models (LLMs) are rapidly gaining enormous popularity in recent years, thanks to their transformative ability to process and generate human-like text. The versatility and scalability of LLMs are driving widespread adoption, and are revolutionizing fields such as healthcare, education, and finance, with applications in content generation, data analysis, customer support, and research automation [19]. However, alongside their vast potential, the training of LLMs has raised significant privacy and legal concerns, particularly regarding the distillation or inclusion of copyrighted materials in their training data without proper attribution or licensing [18, 30]. These concerns fall under the broader issue of data misappropriation, namely, the unauthorized use, access, or exploitation of data by individuals or entities for unintended or unpermitted purposes, often in violation of governing regulations. Data misappropriation has been central to several high-profile debates, for instance, the lawsuit between the New York Times and OpenAI [27], and OpenAI's Terms of Service that explicitly prohibits distilling ChatGPT's output to develop competing models. Detection of such data misappropriation is challenging though, especially when the probabilistic nature of LLMs generates content that may resemble, but does not directly copy, original works [24, 10]. Such challenges underscore  

the importance of developing methods to identify and trace machine-generated text, and have recently generated considerable research interest in this area [23, 14, 17, 22, 13, 36, 34]. In this article, we aim to address a specific and crucial question in data misappropriation detection in LLMs, i.e., How can one determine whether a given LLM has used the data generated by another LLM as part of its training corpus?  

Watermarking is a technique used to embed identifiable patterns into the generated text, enabling traceability to distinguish AI-generated content from human-authored text [3]. This is typically achieved by subtly modifying token generation probabilities to favor specific words or patterns without compromising text coherence. There have been numerous watermarking techniques developed for LLMs, which can be broadly categorized as biased techniques [2, 32, 15], and unbiased techniques [1, 9, 7, 12, 35, 31]. The former explicitly alters token probabilities, favoring a predefined set of tokens, and often introducing detectable statistical biases in the text. The latter ensures the watermark signal is embedded without introducing noticeable deviations from the model's natural token distribution, preserving the text's statistical properties.  

In this article, we focus on a specific problem of data misappropriation detection, i.e., to determine whether a given LLM has incorporated the data generated by another LLM. To answer this question, we propose embedding watermarks into the copyrighted training data and formulating the detection of data misappropriation as a hypothesis testing problem. We develop a general statistical testing framework, construct test statistics, determine optimal rejection thresholds, and explicitly control type I and type II errors. Furthermore, we establish the asymptotic optimality properties of the proposed tests, and demonstrate the empirical effectiveness through intensive numerical experiments. Our proposal introduces a number of novel components and makes several unique contributions to the field.  

First, we develop a formal statistical hypothesis testing framework for data misappropriation detection. The core of our method lies in assessing the relationship between the tokens and secret keys. Under the null hypothesis $\mathcal{H}_0$, an LLM is not trained on the watermarked data, and the secret keys derived from its outputs are independent of the tokens. Conversely, under the alternative hypothesis $\mathcal{H}_1$, the tokens and secrete keys exhibit dependency. For each generation step $t$, given the next-token prediction (NTP) distribution $\mathbf{P}_t$, the null hypothesis assumes that the token generation adheres to the multinomial distribution $\mathbf{P}_t$. However, under $\mathcal{H}_1$, the distribution of token $\omega_t$ also depends on the secret key $\zeta_t$, and deviates from $\mathbf{P}_t$. To capture this dependency, we use a test statistic comprising $\omega_t$ and $\zeta_t$ satisfying that: under $\mathcal{H}_0$, the statistic follows a fixed distribution independent of the NTP mechanism, whereas under $\mathcal{H}_1$, the statistic follows a distribution depending on the NTP. To find the rejection threshold for the proposed test statistic, we employ the large deviation theory to evaluate the asymptotic rates of type I and type II errors, and employ the class-dependent efficiency to account for variations in the NTP distributions. We then transform the detection problem into a solvable minimax optimization problem, and derive the optimal rejection threshold accordingly.  

Second, we consider two types of dependency between the LLMs. Specifically, for an underwatermarked LLM, the tokens are sampled from a multinomial distribution conditional on preceding tokens. In contrast, for a watermarked LLM, a secret key is introduced that perturbs this multinomial distribution, generating tokens from a modified distribution. For any text, these  

secret keys can be computed at each generation step. In models unrelated to the watermarking process, these keys remain independent of the token sequences. However, when an LLM has been trained on a watermarked data, its token generation process depends, at least partially, on these keys. We consider two settings for such dependency: the complete inheritance where the new LLM trained on the data from a watermarked LLM fully inherits the same watermarking distribution, and the partial inheritance where the newly trained LLM generates outputs with distributions close to, but not identical to, the watermarked distribution. We quantify the difference between the two inheritance settings using the total variation distance, which serves as a bounded and computationally convenient metric for comparing multinomial distributions. We derive a separate rejection threshold for each inheritance setting. We also remark that we are the first to establish the optimality guarantees for data misappropriation detection under partial inheritance, while the existing literature only studies complete inheritance.  

Third, we also consider two types of rejection rule designs, and establish the optimality guarantees of the corresponding tests. Specifically, we consider the traditional setting where we fix the type I error at a predefined level and seek to maximize the asymptotic efficiency in the minimax sense. We also consider the setting where we aim to asymptotically minimize the sum of the type I and type II errors in the minimax sense. For the first setting, we show that our proposed test is asymptotically optimal in the sense that it achieves the largest power compared to all other testing methods. For the second setting, we show that our test is minimax optimal in that it achieves the smallest sum of type I and type II errors among all possible testing methods. Achieving such an optimality requires two key steps: deriving the optimal testing scheme based on the selected test statistic, and demonstrating that its asymptotic efficiency serves as an upper bound for the asymptotic efficiency of any alternative testing scheme. In cases where the minimax optimization problem derived from the chosen test statistic is convex, the solution is relatively straightforward. However, convexity is not always guaranteed in the partial inheritance setting, where the bounded total variation distance introduces additional uncertainty. Addressing this involves fully characterizing the equality conditions of the underlying inequalities to ensure their validity throughout the estimation process. In the context of minimax-type inequalities, relying on a single equality condition is often insufficient. Instead, it is necessary to consider combinations of multiple cases. Evaluating the efficiency of these combined cases adds another layer of complexity, as it requires careful analysis of interdependencies and potential deviations. Our approach rigorously handles such challenging scenarios, ensuring robust detection performance even under non-convexity.  

Finally, we develop the tests for two commonly used, highly representative watermarking techniques. Specifically, we consider the Gumbel-max watermark [1], which adds structured noise from Gumbel distributions into the sampling process. We then consider the red-green-list watermark [15], which randomly divides the tokens into the red list and the green list, and biases token selection toward the green list. The former represents an unbiased watermarking technique, whereas the latter a biased one. For each watermarking, we derive the corresponding optimal detection rule and demonstrate its efficacy through numerical experiments.  

Our proposal is related to but also clearly distinctive of the existing literature. The line of research most closely related to ours is watermarking detection, where the goal is to dis  

tinguish between human-authored text and machine-generated text [17, 29, 16], and [16] was the first to develop a formal statistical framework and rigorously study the statistical efficiency of the testing methods. Nevertheless, our work, while related, differs from [16] in numerous ways. First, [16] focused on detecting if a text is written by a human or an LLM, whereas we study an utterly different problem of whether one LLM has used the data generated by another LLM for training. Second, [16] only considered the complete inheritance setting, while we also study the partial inheritance setting, which is closer to real-world situations. Theoretically, complete inheritance corresponds to a simple-vs-simple test [28], where both null and alternative hypotheses only involve one distribution. In contrast, partial inheritance leads to a simple-vs-composite test, where the alternative hypothesis involves a class of distributions. Relatedly, the technical analysis in partial inheritance becomes much more challenging than that in complete inheritance. In particular, the objective function in partial inheritance lacks global convexity, and the convexity arguments used in complete inheritance no longer apply directly. To address this, we adopt a two-step strategy: we first fix the largest coordinate and analyze convexity in the remaining $n-1$ variables, then reduce the problem to a univariate optimization to complete the analysis. This careful decomposition is necessary to obtain tight minimax lower bounds. Third, [16] studied the optimality within a restricted class of tests, namely, those based on thresholding sums of test statistics, whereas we establish the minimax optimality over the space of all possible tests. That is, we prove our test achieves the lowest type II error among all tests with type I error at most $\alpha$, and also achieves the minimum total error when both types of error are penalized equally. Another line of relevant research is membership inference attacks, where the goal is to determine whether a specific data point was part of the training dataset used to train a machine learning model [25, 5, 8]. These attacks exploit overfitting or unintended information leakage, where the model exhibits different behaviors for training versus non-training data. Nevertheless, our work is different in that, instead of detecting the presence of specific data points in the training set, we aim to determine whether an LLM has incorporated the data generated by another LLM into its training corpus. This leads to a distinct analytical framework, as we focus on detecting subtle traces of patterns in data generation, rather than individual-level overfitting or leakage.  

In summary, we are among the first to conceptualize data misappropriation detection in LLMs as a statistical hypothesis testing problem. We develop a general testing framework incorporating different inheritance settings, different rejection rule designs, and different watermarking techniques. We establish the minimax optimality guarantees for the proposed tests. Our proposal offers a valuable integration of statistical principles and large language models, and provides meaningful statistical insights into the rapidly evolving field of generative AI.  

The rest of the article is organized as follows. Section 2 introduces the problem setup. Section 3 develops the statistical hypothesis testing framework. Sections 4 and 5 study the Gumbel-max watermarking and the red-green-list watermarking. Sections 6 and 7 present the numerical results. The supplementary appendix collects all proofs and additional results.  

# 2 Problem Setup  

Throughout this article, we adopt the following notation. For an event $A$, let $\mathbf{1}\{A\}$ denote the indicator function on $A$. Let $g(n)=O(f(n)),g(n)=\Omega(f(n))$ denote that there exists numerical constant $c_1,c_2$, such that $\lim_{n\to\infty}g(n)/f(n)<c_1$ and $\lim_{n\to\infty}g(n)/f(n)=\infty$, respectively. Let $TV(\mu,\nu)$ denote the total variation (TV) distance between two probability measures $\mu,\nu$. In particular, for the case that $\mu=(p_1,p_2,\cdots,p_m),\nu=(q_1,q_2,\cdots,q_m)$, we can calculate their TV distance by $TV(\mu,\nu)=1/2\sum_{i=1}^m|p_i-q_i|$. Let $TV_{.\zeta}(\mu,\nu)$ denote the TV distance between two probability measures $\mu,\nu$ conditional on a random variable $\zeta$.  

Language models are equipped with a vocabulary $\mathcal{W}$, which consists of words or word fragments called tokens. Typically, a vocabulary contains $|\mathcal{W}| = 50,000$ tokens or more. In this article, we denote the number of tokens in the vocabulary by $|\mathcal{W}| = m$. To generate text, a language model requires a sequence of tokens that constitutes a prompt. In the generated text, the entries with nonpositive indices, $\omega_{-L_p}, \omega_{-L_p+1}, \ldots, \omega_0$, represent the prompt, with a length of $L_p + 1$. The entries with positive indices, $\omega_1, \ldots, \omega_T$, are tokens generated by the language model in response to the prompt. A language model for next-token prediction (NTP) is a function $f$, often parameterized by a neural network, which takes as input a sequence of known tokens, $\omega_{-N_p}, \omega_{-N_p+1}, \ldots, \omega_{t-1}$, comprising the prompt and the first $t-1$ tokens generated by the model. The output is an $m$-dimensional multinomial distribution, which represents the probability distribution for the next token. Formally, let $\mathbf{P}_t := (P_{t,1}, P_{t,2}, \ldots, P_{t,m})$ denote this multinomial distribution at step $t$, conditional on the prompt and previously generated tokens. As a valid probability distribution, $\mathbf{P}_t$ satisfies that $\sum_{i=1}^{m} P_{t,i} = 1$ and $P_{t,i} \geq 0$.  

Watermarking is a technique for adjusting the NTP distribution so that the generated text exhibits specific statistical properties. The key insight is that the text generated by a watermarked LLM adheres to the watermarking generation rules. In this way, watermarks function as markers of an LLM's generation rules, and enable differentiating a watermarked LLM from an unwatermarked one based on a given text. This principle forms the foundation of our research. In Appendix A, we further show theoretically that it is impossible to detect data misappropriation without watermarking.  

More specifically, a watermarked LLM generates the next token through a process jointly determined by a secret key and the NTP distribution. Let $\zeta_t$ denote the secret key at step $t$, which is known only to the owner of the LLM. Formally, the generation rule of the watermarked LLM is $\omega_t \sim S(\mathbf{P}_t, \zeta_t)$, where $S$ is a decoding function that outputs an $m$-dimensional multinomial distribution. In watermarking, a fundamental requirement is that averaging over the randomness in $\zeta_t$, the difference between the watermarked and unwatermarked generation rules should not be too large. In other words, the NTP distribution conditional on previous tokens should not vary significantly between the watermarked and unwatermarked LLMs. For instance, for an unbiased watermarking technique, the watermarked NTP distribution averaged over the randomness in $\zeta_t$ matches the unwatermarked NTP distribution conditional on previous tokens. Consequently, from a user's perspective, there is no noticeable difference between the text generated by a watermarked LLM and that produced by an unwatermarked LLM.  

Nevertheless, the secret keys possess statistical properties that enable effective detection.  

Typically, the secret keys are generated using hash functions. Hash functions map data of arbitrary size to fixed-sized values, with specific forms varying across different watermarking techniques. In many hashing-based watermarks, the distribution of watermarked text is biased towards certain $k$-grams by hashing a sliding window of the previous $k-1$ tokens to determine the next token pseudorandomly. In practice, $\zeta_t$ can be computed using a hash function $\mathcal{A}$ that depends only on, for instance, the last five tokens $\omega_{t-5}, \ldots, \omega_{t-1}$ [1].  

Formally, we consider the following setup for our data misappropriation detection problem. Suppose there are three roles. A victim is the owner of an LLM who has trained an LLM with plentiful resources, has added some type of watermark in their LLM, and has the knowledge of the secret keys that correspond to the hash functions. A suspect is the owner of another LLM, and is suspected to have used data generated by the victim's LLM as part of the training data for their own LLM. The suspect has no knowledge of the secret keys. A detector is a trusted third party, such as law enforcement, who cooperates with the victim. The detector has the knowledge of both the secret keys and the NTP distribution of the victim's LLM, but does not know the NTP distribution of the suspect's LLM. The detector is to determine whether the suspect has used the data generated by the victim's LLM to train their own LLM.  

# 3 A Statistical Hypothesis Testing Framework  

# 3.1 Hypotheses and test statistics  

Let $n \in \mathbb{N}$ denote the text length. Let $\omega_{1:n} := \omega_1 \cdots \omega_n$ and $\zeta_{1:n} := \zeta_1 \cdots \zeta_n$ denote the generated text and the secret keys, respectively. Given a prompt and a text $\omega_{1:n}$ generated by the LLM under investigation, we target the problem of whether this LLM utilizes the data generated by another watermarked LLM, in the form of hypotheses:  

$\mathcal{H}_{0}:\omega_{1:n}$ is generated by the LLM without data misappropriation;

$\mathcal{H}_{1}:\omega_{1:n}$ is generated by the LLM with data misappropriation.  

In addition, the secret keys are generated using the hash function $\mathcal{A}$ that depends on the previous tokens. The knowledge of $\mathcal{A}$ is only available to the victim and the detector, but not the suspect. As such, $\zeta_{1:n}$ are regarded as random variables by the suspect, but are viewed deterministic by the victim and the detector, since they can be calculated by previous tokens plus the knowledge of the hash function. Meanwhile, the watermarked LLM generates the next token according to the rule, $\omega_t \sim \mathcal{S}(\mathbf{P}_t, \zeta_t)$, for some decoding function $\mathcal{S}$, and $\mathbf{P}_t$ represents the NTP distribution of the unwatermarked LLM given the prompt and previous tokens.  

Next, we mathematically formulate complete inheritance and partial inheritance.  

Definition 1 (Complete inheritance). The suspect's LLM follows the same generation rule as the victim's LLM, in that the hash function $A$ and the decoding function $S$ of both LLMs are the same if data misappropriation occurs.  

Under the complete inheritance setting, the hypotheses in (1) become  

$$
\begin{array} { r } { \mathcal { H } _ { 0 } : \omega _ { t } | \zeta _ { t } \sim \mathbf { P } _ { t } , \qquad \mathcal { H } _ { 1 } : \omega _ { t } | \zeta _ { t } \sim \mathcal { S } ( \mathbf { P } _ { t } , \zeta _ { t } ) , \quad \mathrm { f o r } ~ t = 1 , 2 , \ldots , n . } \end{array}
$$  

In practice, a newly trained LLM that uses data generated by a watermarked LLM produces outputs with a distribution that preserves certain patterns of the watermarked distribution, but not identical. This scenario is captured by the following partial inheritance setting.  

Definition 2 (Partial inheritance). There exists an upper bound for the total variation (TV) distance between the probability distribution of data generated by the suspect's LLM under data misappropriation, and the probability distribution of data generated by the victim's watermarked LLM, in that $TV_{|\zeta_t}(\omega_t, \mathcal{S}(\mathbf{P}_t, \zeta_t)) \leq 1 - \theta$, where $1 - \theta$ quantifies the upper bound on the allowable difference between the two distributions.  

We make a few remarks. First, we note that partial inheritance may not fully capture the nuanced behaviors of real-world LLMs, and we do not intend to present it as a definitive or exclusive model of inheritance. Rather, we introduce this setting to capture the intuition that when a suspect model is trained on outputs from a victim model, it inevitably learns both the semantic signals and any embedded watermark patterns, as there is no principled way for the suspect model to disentangle them. Under this assumption, we model the next-token distribution of the suspect model as being close to that of the victim model, and formalize this proximity via a TV distance bound. Importantly, we do not fix the TV distance to be small; instead, our theoretical framework allows the bound to vary over the full range of (0, 1), offering flexibility in characterizing different degrees of inheritance. Second, while partial inheritance serves a theoretical role in justifying our test construction, the resulting score function is directly used as a practical detection tool. Empirically, as we show in Section 7, our method remains effective and robust even when the true inheritance behavior deviates from the assumed model.  

Under the partial inheritance setting, the hypotheses in (1) become  

$$
\begin{array} { r } { \mathcal { H } _ { 0 } : \omega _ { t } | \zeta _ { t } \sim \mathbf { P } _ { t } ; \qquad \mathcal { H } _ { 1 } : T V _ { \lvert \zeta _ { t } \rvert } ( \omega _ { t } , \mathcal { S } ( \mathbf { P } _ { t } , \zeta _ { t } ) ) \leq 1 - \theta , \quad \mathrm { f o r ~ } t = 1 , 2 , \ldots , n . } \end{array}
$$  

We also briefly remark that, for the detection task to be feasible, the constant $\theta$ cannot be too small, as an overly small $\theta$ would imply that the two distributions are too similar to differentiate.  

Next, we describe the distribution of $\omega|\zeta$ under $\mathcal{H}_1$. In commonly used watermarking techniques, $\mathcal{S}(\mathbf{P},\zeta)$ can only take finitely many values. We provide more details in Sections 4 and 5. Let $k$ denote the total possible values of $\mathcal{S}(\mathbf{P},\zeta)$, and partition the space of $(\mathbf{P},\zeta)$ into multiple regions, $A_1, A_2, \ldots, A_k$, such that in each region $A_i$, the corresponding multinomial distribution $\mathcal{S}(\mathbf{P},\zeta)$ is identical, and is represented by a multinomial probability vector $(s_{i1}, s_{i2}, \ldots, s_{im})$, for $i \in [k]$. We further assume that, for any $t \in [n]$, and all $(\mathbf{P},\zeta) \in A_i$, the distribution of $\omega$ in the partial inheritance setting is the same, which is represented by $(q_{t,i1}, q_{t,i2}, \ldots, q_{t,i_m})$, regardless of the exact values of $(\mathbf{P},\zeta)$. This assumption simplifies the theoretical discussion and allows us to represent the extreme points using a feature matrix. It can be relaxed without compromising the validity of our theoretical results; see the remarks after Theorem 2. Now, we formally define the feature matrix, whose rows describe different possible distributions of $\omega|\zeta$.  

Definition 3. At the t-th token, define the feature matrix under the alternative $\mathcal{H}_1$ of the complete inheritance setting as $\mathbf{S}_t = (s_{t,ij})_{k\times m}$, and the feature matrix under the alternative $\mathcal{H}_1$ of the partial inheritance setting as $\mathbf{Q}_t = (q_{t,ij})_{k\times m}$.  

The feature matrix allows us to directly derive the distribution of test statistic under the null and alternative hypotheses. Additionally, the use of the feature matrix helps the representation of extreme points, and provides a more efficient framework for analysis. Importantly, we note that the order of rows in the feature matrix is arbitrary and does not affect the results.  

Let $Y_t := Y(\omega_t, \zeta_t)$ denote the test statistic, we have the following rejection rule,  

$$
T _ { h } ( Y _ { 1 : n } ) : = \left\{ \begin{array} { l l } { 1 } & { \mathrm { i f ~ } \sum _ { t = 1 } ^ { n } h ( Y _ { t } ) \geq \gamma _ { n } , } \\ { 0 } & { \mathrm { i f ~ } \sum _ { t = 1 } ^ { n } h ( Y _ { t } ) < \gamma _ { n } . } \end{array} \right.
$$  

where $h(\cdot)$ is the score function. We derive the specific forms of $Y_t$ and $h(\cdot)$ for specific watermarking techniques later in Sections 4 and 5. We reject $\mathcal{H}_0$ if $T_h(Y_{1:n}) = 1$.  

We also remark that, for complete inheritance, our testing problem reduces to the problem of determining whether a text is generated by an unwatermarked LLM or by a watermaked LLM, whose NTP distributions are the same, which has been studied in [16]. However, our problem is more general, in that we also consider partial inheritance, different rejection rule designs, and obtain tighter optimality guarantees.  

# 3.2 Rejection threshold  

For our testing method, it is crucial to choose an appropriate score function $h(\cdot)$ and the threshold $\gamma_n$ to ensure the optimality of the test. Next, we consider two rejection rule designs separately. We first fix the type I error at a predefined level and seek to maximize the asymptotic efficiency. We then aim to asymptotically minimize the sum of the type I and type II errors.  

We first consider the setting of the fixed type I error, where we choose the threshold $\gamma_{n}$, such that $\mathbb{P}_{\mathcal{H}_{0}}(T_{h}(Y_{1:n})=1)=\alpha$ for a pre-specified significance level $\alpha$, while we choose the optimal score function $h(\cdot)$ later to maximize the asymptotic power of the test. Our key idea is to turn hypothesis testing to a minimax optimization problem. Let $\mathbb{E}_{0}$ be the expectation under $\mathcal{H}_{0}, \mathbb{E}_{1,\mathbf{P}}, \mathbb{E}_{1,\mathbf{P},\mathbf{Q}}$ be the expectation under $\mathcal{H}_{1}$ that depends on $\mathbf{P}$ or on $\mathbf{P}$, $\mathbf{Q}$, and $\mathcal{P}$, $\mathcal{Q}$ be some distribution classes whose forms are specified later for specific watermarking techniques.  

Theorem 1. (Hypothesis testing as minimax optimization, fixed type I error). (a) (Complete inheritance). Suppose $\mathbf{P}_t \in \mathcal{P}$ are i.i.d. random vectors for all $t$. For any $h$ satisfying $\mathbb{E}_0|h| < \infty$, the type II error of the rejection rule $T_h$ satisfies that  

$$
\operatorname* { l i m } _ { n \to \infty } \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( T _ { h } ( Y _ { 1 : n } ) = 0 \right) ^ { 1 / n } \leq e ^ { - R _ { \mathcal { P } } ( h ) } ,
$$  

where $R_{\mathcal{P}}(h) = -\inf_{\theta \geq 0} \sup_{\mathbf{P} \in \mathcal{P}} \{\theta \mathbb{E}_0 h(Y) + \log \phi_{\mathbf{P},h}(\theta)\}$, $\phi_{\mathbf{P},h}(\theta) = \mathbb{E}_{1,\mathbf{P}} e^{-\theta h(Y)}$. In addition, the inequality in (4) is tight, in that there exist $\mathbf{P}^*$, such that, if $\mathbf{P}_t = \mathbf{P}^*$ for all $t$, then for any positive $\epsilon$ and a sufficiently large $n$,  

$$
\begin{array} { r } { \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( T _ { h } ( Y _ { 1 : n } ) = 0 \right) ^ { 1 / n } \geq e ^ { - \{ R _ { \mathcal { P } } ( h ) + \epsilon \} } . } \end{array}
$$  

(b) (Partial inheritance). Suppose $\mathbf{P}_t \in \mathcal{P}$, $\mathbf{Q}_t \in \mathcal{Q}$ are i.i.d. random vectors and i.i.d. random matrices for all $t$. For any $h$ satisfying $\mathbb{E}_0 |h| < \infty$, the type II error of the rejection rule $T_h$ satisfies that  

$$
\operatorname* { l i m } _ { n \to \infty } \operatorname* { s u p } _ { h \to \infty } \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( T _ { h } ( Y _ { 1 : n } ) = 0 \right) ^ { 1 / n } \leq e ^ { - R _ { \mathcal { P } , \mathcal { Q } } ( h ) } ,
$$  

where $R_{\mathcal{P},\mathcal{Q}}(h)=-\inf_{\theta\geq0}\sup_{\mathbf{Q}\in\mathcal{Q}}\sup_{\mathbf{P}\in\mathcal{P}}\{\theta\mathbb{E}_0h(Y)+\log\phi_{\mathbf{P},\mathbf{Q},h}(\theta)\}$, $\phi_{\mathbf{P},\mathbf{Q},h}(\theta)=\mathbb{E}_{1,\mathbf{P},\mathbf{Q}}e^{-\theta h(Y)}$. In addition, the inequality in (5) is tight, in that there exist $\mathbf{P}^*$ and $\mathbf{Q}^*$, such that, if $\mathbf{P}_t=\mathbf{P}^*$, $\mathbf{Q}_t=\mathbf{Q}^*$ for all $t$, then for any positive $\epsilon$ and a sufficiently large $n$,  

$$
\mathbb { P } _ { \mathcal { H } _ { 1 } } \left( T _ { h } ( Y _ { 1 : n } ) = 0 \right) ^ { 1 / n } \geq e ^ { - ( R _ { \mathcal { P } , \mathcal { Q } } ( h ) + \epsilon ) } .
$$  

Theorem 1 essentially transforms the hypothesis testing problem into a minimax optimization problem, by showing that the two problems are equivalent under the worst case of NTP distribution $\mathbf{P}_1, \mathbf{P}_2, \cdots, \mathbf{P}_n$. Consequently, we turn to the following optimization problems for complete and partial inheritance, respectively,  

$$
\begin{array} { r l } { \operatorname* { s u p } _ { h } R _ { \mathcal { P } } ( h ) } & { = \mathrm { ~ - i n f ~ i n f ~ } \operatorname* { s u p } _ { h \, \, \theta \geq 0 } \{ \theta \mathbb { E } _ { 0 } h ( Y ) + \log \phi _ { \mathbf { P } , h } ( \theta ) \} } \\ & { = \mathrm { ~ - ~ i n f ~ } \operatorname* { s u p } _ { h , \theta \geq 0 } \{ \mathbb { E } _ { 0 } \theta h ( Y ) + \log \mathbb { E } _ { 1 , \mathbf { P } } e ^ { - \theta h ( Y ) } \} \, ; } \\ { \operatorname* { s u p } _ { h } R _ { \mathcal { P } , \mathcal { Q } } ( h ) } & { = \mathrm { ~ - ~ i n f ~ i n f ~ } \operatorname* { s u p } _ { h \, \, \theta \geq 0 } \operatorname* { s u p } _ { \mathbf { Q } \in \mathcal { Q } \, \mathbf { P } \in \mathcal { P } } \{ \theta \mathbb { E } _ { 0 } h ( Y ) + \log \phi _ { \mathbf { P } , \mathbf { Q } , h } ( \theta ) \} } \\ & { = \mathrm { ~ - ~ i n f ~ } \operatorname* { s u p } _ { h , \theta \geq 0 } \operatorname* { s u p } _ { \mathbf { Q } \in \mathcal { Q } \, \mathbf { P } \in \mathcal { P } } \{ \mathbb { E } _ { 0 } \theta h ( Y ) + \log \mathbb { E } _ { 1 , \mathbf { P } , \mathbf { Q } } e ^ { - \theta h ( Y ) } \} \, . } \end{array}
$$  

We provide the detailed solutions to these minimax optimization problems later in Sections 4 and 5 for specific watermarking techniques.  

We next consider the setting of minimizing the sum of type I and type II errors,  

$$
\operatorname* { i n f } _ { h } \operatorname* { l i m s u p } _ { n \to \infty } \left[ \operatorname* { i n f } _ { \gamma } \left\{ \mathbb { P } _ { \mathcal { H } _ { 0 } } ( T _ { h } ( Y _ { 1 : n } ) = 1 ) + \mathbb { P } _ { \mathcal { H } _ { 1 } } ( T _ { h } ( Y _ { 1 : n } ) = 0 ) \right\} \right] ^ { \frac { 1 } { n } } .
$$  

Theorem 2. (Hypothesis testing as minimax optimization, sum of type I and type II errors).  

(a) (Complete inheritance). Suppose $\mathbf{P}_t \in \mathcal{P}$ are i.i.d. random vectors for all $t$. For any $h$ satisfying $\mathbb{E}_0|h| < \infty$, the type II error of the rejection rule $T_h$ satisfies that  

$$
\operatorname* { l i m } _ { n \to \infty } \{ \mathbb { P } _ { \mathcal { H } _ { 0 } } ( T _ { h } ( Y _ { 1 : n } ) = 1 ) + \mathbb { P } _ { \mathcal { H } _ { 1 } } ( T _ { h } ( Y _ { 1 : n } ) = 0 ) \} ^ { 1 / n } \leq e ^ { - S _ { \mathcal { P } } ( h ) } ,
$$  

where $S_{\mathcal{P}}(h) = -\inf_{\theta_1, \theta_2 > 0} \left\{ \theta_2(\theta_1 + \theta_2)^{-1} \log \mathbb{E}_0 \left[ \exp \{ \theta_1 h(Y) \} \right] + \theta_1(\theta_1 + \theta_2)^{-1} \log \sup_{\mathbf{P} \in \mathcal{P}} \mathbb{E}_{1,\mathbf{P}} \left[ \exp \{ (-\theta_2 h(Y)) \} \right] \right\}$. In addition, this inequality is tight, in that there exists $\mathbf{P}^*$, such that if $\mathbf{P}_t = \mathbf{P}^*$ for all $t$, then for any $0 < b < 1$ and a sufficiently large $n$,  

$$
\operatorname* { i n f } _ { \gamma } \left\{ \mathbb { P } _ { \mathcal { H } _ { 0 } } ( T _ { h } ( Y _ { 1 : n } ) = 1 ) + \mathbb { P } _ { \mathcal { H } _ { 1 } } ( T _ { h } ( Y _ { 1 : n } ) = 0 ) \right\} ^ { 1 / n } \geq b \cdot e ^ { - S _ { \mathcal { P } } ( h ) } .
$$  

(b) (Partial inheritance). Suppose $\mathbf{P}_t \in \mathcal{P}$, $\mathbf{Q}_t \in \mathcal{Q}$ are i.i.d. random vectors and i.i.d. random matrices for all $t$. For any $h$ satisfying $\mathbb{E}_0|h| < \infty$, the type II error of the rejection rule $T_h$ satisfies that  

$$
\operatorname* { l i m } _ { n \to \infty } \{ \mathbb { P } _ { \mathcal { H } _ { 0 } } ( T _ { h } ( Y _ { 1 : n } ) = 1 ) + \mathbb { P } _ { \mathcal { H } _ { 1 } } ( T _ { h } ( Y _ { 1 : n } ) = 0 ) \} ^ { 1 / n } \leq e ^ { - S _ { \mathcal { P } , Q } ( h ) } ,
$$  

where $S_{\mathcal{P},\mathcal{Q}}(h)=-\inf_{\theta_1,\theta_2>0}\left\{\theta_2(\theta_1+\theta_2)^{-1}\log\mathbb{E}_0\left[\exp\left\{\theta_1h(Y)\right\}\right]+\theta_1(\theta_1+\theta_2)^{-1}\log\sup_{\mathbf{Q}\in\mathcal{Q}}$ $\sup_{\mathbf{P}\in\mathcal{P}}\mathbb{E}_{1,\mathbf{P},\mathbf{Q}}\left[\exp(-\theta_2h(Y))\right]\right\}$. In addition, this inequality is tight, in that there exists $\mathbf{P}^*$ and $\mathbf{Q}^*$, such that if $\mathbf{P}_t=\mathbf{P}^*$, $\mathbf{Q}_t=\mathbf{Q}^*$ for all $t$, then for any $0<b<1$ and $a$ sufficiently large $n$,  

$$
\operatorname* { i n f } _ { \gamma } \left\{ \mathbb { P } _ { \mathcal { H } _ { 0 } } ( T _ { h } ( Y _ { 1 : n } ) = 1 ) + \mathbb { P } _ { \mathcal { H } _ { 1 } } ( T _ { h } ( Y _ { 1 : n } ) = 0 ) \right\} ^ { 1 / n } \geq b \cdot e ^ { - S _ { \mathcal { P } , Q } ( h ) } .
$$  

Theorem 2 again transforms hypothesis testing to minimax optimization, and we turn to the following optimization problems for complete and partial inheritance, respectively,  

$$
\begin{array} { r l } { \operatorname* { s u p } _ { h } S _ { \mathcal { P } } ( h ) } & { = \, \, - \operatorname* { i n f } _ { h } \operatorname* { i n f } _ { \theta _ { 1 } , \theta _ { 2 } > 0 } \frac { \theta _ { 2 } } { \theta _ { 1 } + \theta _ { 2 } } \log \mathbb { E } _ { 0 } \exp \left\{ \theta _ { 1 } h ( Y ) \right\} } \\ & { \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad
$$  

We make a few remarks regarding Theorems 1 and 2.  

First, the proofs of both theorems rely on that $\{Y_t\}$, or $\{h(Y_t)\}$, are i.i.d. under both $\mathcal{H}_0$ and $\mathcal{H}_1$. This is justified under two perspectives: the cryptographic property of the hash function guarantees that $\zeta_t$ behaves like a fresh independent key at each time step from the analyst's point of view, and the next-token distributions $\mathbf{P}_t$ are assumed to be i.i.d. More specifically, our framework depends on a cryptographic-style hash function $\mathcal{A}$, which deterministically maps sequences, typically composed of the prompt and prior tokens, to secret keys. That is, we assume a secret key generation rule of the form $\zeta_{t+1} = \mathcal{A}(\text{prompt}, \omega_{1:t})$. Importantly, while the function is deterministic, it is designed to be indistinguishable from a truly random mapping to any observer who does not know the internal structure or seed of $\mathcal{A}$. This property aligns with standard constructions of pseudorandom functions that are widely used in cryptography [11]. Under this design, and from the perspective of a statistical analyst or adversary without access to $\mathcal{A}$, the secret keys $\zeta_t$ can be treated as statistically independent of the token sequence $\omega_{1:t}$. Therefore, even though $\zeta_t$ is a function of the previous tokens, its randomness remains cryptographically opaque. Given a fixed next-token distribution $\mathbf{P}_t$ and a randomized secret key $\zeta_t$, the test statistic $Y_t$ depends only on the current token $\omega_t$ and $\zeta_t$, and is thus modeled as a draw from some distribution $\mu_{1,\mathbf{P}_t}$ under $\mathcal{H}_1$. Furthermore, we assume that the distributions $\mathbf{P}_t$ are i.i.d. across $t$. While this assumption is a simplification, it is standard in theoretical analysis  

[13], and provides a tractable setting for establishing error bounds and optimality guarantees. Under this assumption, the sequence $\{Y_t\}$ can be treated as i.i.d. under both $\mathcal{H}_0$ and $\mathcal{H}_1$. In the case of $\mathcal{H}_0$, since there is no watermarking, the secret keys $\zeta_t$ are not present, and the $Y_t$ values depend solely on unwatermarked generation. In the case of $\mathcal{H}_1$, the randomized behavior induced by $\mathcal{A}$ ensures that $\{Y_t\}$ are conditionally independent and identically distributed when marginalizing over the hash randomness. We also note that, this i.i.d. assumption may not hold for any watermark technique, but instead for the two that we study in this article.  

Next, we clarify why it is reasonable to assume the distribution of $\omega|\zeta$ is identical whenever $\mathcal{S}(\mathbf{P},\zeta)$ is the same in the partial inheritance setting. In fact, in (9), and also similarly in (7), the only term that depends on the distribution of $Y$ under $\mathcal{H}_1$ is $\mathbb{E}_{1,\mathbf{P},\mathbf{Q}}e^{-\theta_2h(Y)}$. We focus on optimizing $\sup_{\mathbf{Q}\in\mathcal{Q}}\sup_{\mathbf{P}\in\mathcal{P}}\mathbb{E}_{1,\mathbf{P},\mathbf{Q}}e^{-\theta_2h(Y)}$ given $\theta_2$ and $h$. For this optimization, it is sufficient to consider the optimal $\mathbf{Q}$ given $\mathbf{P}$. By definition, $\mathbf{Q}$ imposes an upper bound on the total variation distance between $\omega|\zeta$ and $\mathcal{S}(\mathbf{P},\zeta)$. Consequently, the worst-case scenario can be reduced to the case where the distribution of $\omega|\zeta$ is identical whenever $\mathcal{S}(\mathbf{P},\zeta)$ is the same. This reduction also justifies the introduction of the feature matrix, which provides a well-defined and structured representation of the relation between $\omega|\zeta$ and $\mathcal{S}(\mathbf{P},\zeta)$, and simplifies the analysis by capturing all the relevant information in a systematic manner. Furthermore, we clarify that we treat $m$ and $k$ fixed, and we do not impose any additional regularity conditions on $\gamma_n$ or $\mathcal{P}$.  

Next, the choice of the distribution class $\mathcal{P}$ and $\mathcal{Q}$ determines the form of the optimization problems derived from the hypothesis testing problems. Specifically, the distribution class $\mathcal{P}$ needs to exclude the singleton distributions, e.g., $(1,0,0,\cdots,0)$, because in such cases, the joint distribution of $(\omega_t,\zeta_t)$ would be identical under $\mathcal{H}_0$ and $\mathcal{H}_1$, making the two hypotheses indistinguishable. In the partial inheritance setting, the distribution class $\mathcal{Q}$ must satisfy the condition $TV_{:\zeta_t}(\omega_t,\mathcal{S}(\mathbf{P}_t,\zeta_t))\leq 1-\theta$ for all $t\in[n]$, ensuring the consistency with the bounded total variation condition. Beyond these requirements, our framework is flexible and can, in principle, analyze any distribution classes. Nevertheless, we follow the literature [16, 4, 13], and primarily focus on the following distribution class for $\Delta,\theta\in(0,1)$:  

$$
\mathcal { P } _ { \Delta } : = \left\{ \mathbf { P } : \operatorname* { m a x } _ { i \in [ m ] } P _ { i } \leq 1 - \Delta \right\} , \quad \mathcal { Q } _ { \theta } : = \left\{ \omega | \zeta : T V _ { | \zeta } \left( \omega , \mathcal { S } ( \mathbf { P } , \zeta ) \right) \leq 1 - \theta \right\} .
$$  

Finally, we comment on the practical usefulness of our method. In our theoretical analysis for partial inheritance, we construct the optimal score function $h(\cdot)$ assuming knowing the true $\Delta$ that controls the entropy of the probability distribution, and $\theta$ that controls the closeness between the next-token distributions for the original and watermarked data. While this is idealized, it helps reveal the structure of the optimal testing procedures. In practice, the true values of $\Delta$ and $\theta$ are generally unknown. Nevertheless, we argue they can be reasonably estimated or approximated in numerous cases. For instance, if one has knowledge of the NTP distribution of the underlying LLM, e.g., its temperature setting or sampling method, such knowledge can be used to estimate $\Delta$. A higher sampling temperature typically induces greater randomness, corresponding to a larger value of $\Delta$. Similarly, the value of $\theta$ can be informed by the robustness of the watermarking method. A more robust watermarking scheme typically yields a  

smaller value of $\theta$. Furthermore, our numerical experiments in Sections 6 and 7 show that our proposed detection method remains effective and robust even when the estimates of $\Delta$ and $\theta$ are imperfect. We show that, with reasonably chosen but approximate values of $\Delta$ and $\theta$, our method outperforms the baseline detection solutions. Besides, precise tuning is not required, and an approximate estimate of the parameters at the right scale is sufficient.  

# 4 Analysis of Gumbel-max Watermark  

We now apply our general hypothesis testing framework to a specific watermarking scheme: the Gumbel-max watermark [1]. We first derive the feature matrix. We then establish three key results: the distribution of the test statistic, the optimal score function, and the optimality guarantee of our proposed test. We first report the results for complete inheritance in Theorems 3 to 5, then parallelly for partial inheritance in Theorems 6 to 8.  

# 4.1 Feature matrix  

We first quickly review the Gumbel-max watermark. The secret key $\zeta_t$ consists of $m = |\mathcal{W}|$ i.i.d. random variables $(U_{t,\omega})_{\omega \in \mathcal{W}}$ following a uniform $(0,1)$ distribution. Given the NTP distribution $\mathbf{P} = (P_1, P_2, \ldots, P_m)$, the next token is generated following the rule $\omega_t = \arg \max_{\omega \in \mathcal{W}} \log U_{t,\omega}/P_\omega$. The corresponding score vector $\mathcal{S}(\mathbf{P}_t, \zeta_t)$ is defined as the one-hot vector indicating which token $\omega$ is selected under this rule; i.e., it takes the form $(0, \ldots, 1, \ldots, 0)$ where 1 appears at the position of the selected token. As the detector has the knowledge of the hash function, the generating process becomes a deterministic process. In other words, if the detector knows $\zeta_t$ as well as $\mathbf{P}_t$, one can directly determine what the next token is. We note that, although $\mathbf{P}_t$ can vary continuously over the probability simplex and thus takes infinitely many values, the randomness in $\zeta_t$ and the argmax rule ensure that $\mathcal{S}(\mathbf{P}_t, \zeta_t)$ is always one of $m$ possible one-hot vectors. Therefore, regardless of how $\mathbf{P}_t$ varies, the function $\mathcal{S}(\mathbf{P}_t, \zeta_t)$ maps into a finite set of size $m$, corresponding to the $m$ possible token choices. Consequently, the number of possible values $k$ that $\mathcal{S}(\mathbf{P}_t, \zeta_t)$ can take is exactly $m$; in other words, $k = m$.  

We next derive the feature matrix for the Gumbel-max watermark. Recall that, for both complete inheritance and partial inheritance, the feature matrix represents the conditional probabilities of token generation under $\mathcal{H}_0$ and $\mathcal{H}_1$. Under $\mathcal{H}_0$, the generation of $\omega_t$ is independent of $\zeta_t$, and $\omega_t$ follows the unwatermarked NTP distribution $\mathbf{P}_t$. Under $\mathcal{H}_1$, the generation rule differs depending on the inheritance setting. Let $\omega^{(i)}$ denote the $i$th token in the vocabulary $\mathcal{W}$. In the complete inheritance setting, we have $\omega_t = \arg\max_{i \in [m]} \log U_{t,i}/P_{t,i}$. The probability that $\omega_t = \omega^{(i)}$ is 1, given that $\log U_{t,i}/P_{t,i}$ is the largest among $\{\log U_{t,i'}/P_{t,i'}\}_{i' \in [m]}$. In the partial inheritance setting, the generated tokens satisfy that $TV(\omega_t|\zeta_t, \mathcal{S}(\mathbf{P}_t, \zeta_t)) \leq 1 - \theta$, for all $t$. Since $\mathcal{S}(\mathbf{P}_t, \zeta_t)$ is deterministic, the corresponding probability distribution must be $(1, 0, \ldots, 0)$ or one of its permutations. Therefore, the probability that $\omega_t = \omega^{(i)}$ is no smaller than $\theta$. Let $\mathbf{S}_{\mathbf{P}_t}$ be an $m \times m$ matrix with each row being $\mathbf{P}_t$. The next proposition formally summarizes the feature matrix for the complete and partial inheritance settings.  

Proposition 1. (Feature matrix). Following Definition 3, for $t \in [n]$, (a) (Complete inheritance). The feature matrix is $\mathbf{S}_{\mathbf{P}_t}$ under $\mathcal{H}_0$, and is the identity matrix $I_{m \times m}$ under $\mathcal{H}_1$.

(b) (Partial inheritance). The feature matrix is $\mathbf{S}_{\mathbf{P}_t}$ under $\mathcal{H}_0$, and is any matrix in the class $\left\{\mathbf{Q}_t \in \mathbb{R}^{m \times m}: q_{t,ii} \geq \theta, q_{t,ij} \geq 0, \sum_{j \in [m]} q_{t,ij} = 1\right\}$ under $\mathcal{H}_1$.  

# 4.2 Complete inheritance  

We choose the test statistic $Y_t$ as the random number $U_{t,\omega_t}$, which corresponds to the selected token $\omega_t$ at step $t$. In this case, it is a statistic whose distribution does not rely on $\mathbf{P}_t$ under the null hypothesis. First, we obtain the distribution of $Y_t$ under the null $\mathcal{H}_0$ and the alternative $\mathcal{H}_1$, a result already established in the literature [20].  

Theorem 3. (Distribution of $Y_t$, adapted from [20]). Under $\mathcal{H}_0$, $Y_t \sim U(0,1)$, and $\mathbb{P}_{\mathcal{H}_0}(Y_t \leq r|\mathbf{P}_t) = r$, for $r \in [0,1]$. Under $\mathcal{H}_1$, $\mathbb{P}_{\mathcal{H}_1}(Y_t \leq r|\mathbf{P}_t) = \sum_{\omega \in \mathcal{W}} P_{t,\omega} r^{1/P_{t,\omega}}$, for $r \in [0,1]$.  

Next, we derive the optimal score function and threshold value. We choose the distribution class as $\mathcal{P}_{\Delta} := \{\mathbf{P} : \max_{i \in [m]} P_i \leq 1 - \Delta\}$ for $\Delta \in [0, 1 - m^{-1}]$.  

Theorem 4. (Optimal score function and threshold value).  

(a) For both rejection rule design settings, the optimal score function is  

$$
h _ { \Delta } ^ { * } ( r ) = \log \left( \left[ \frac { 1 } { 1 - \Delta } \right] r ^ { \frac { \Delta } { 1 - \Delta } } + r ^ { \frac { \tilde { \Delta } } { 1 - \tilde { \Delta } } } \right) ,
$$  

where $\tilde{\Delta} = (1 - \Delta) \cdot \left\lfloor \frac{1}{1-\Delta} \right\rfloor$. Moreover, the worst case in the minimax problem is attained at the following least-favorable NTP distribution in $\mathcal{P}_{\Delta}$,  

$$
\mathbf { P } ^ { * } = \left( 1 - \Delta , \ldots , 1 - \Delta , 1 - ( 1 - \Delta ) \cdot \left\lfloor \frac { 1 } { 1 - \Delta } \right\rfloor , 0 , \ldots \right) .
$$  

(b) For the setting of fixed type I error with $\alpha$, the optimal threshold is $\gamma_n = n \cdot \mathbb{E}_0 h^*_\Delta(Y) + \Phi^{-1}(1 - \alpha)\sqrt{n \cdot \text{Var}_0(h^*_\Delta(Y))}$. For the setting of minimizing the sum of type I and type II errors, the optimal threshold is $\gamma_n = \log \alpha^* / (1 - \alpha^*)$, where $\alpha^*$ is the solution to $\inf_{\alpha^* \in (0,1)} \int_0^1 \left( \lfloor \frac{1}{1-\Delta} \rfloor r^{\frac{\Delta}{1-\Delta}} + r^{\frac{\Delta}{1-\Delta}} \right)^\alpha^* \, dr.$  

Finally, we establish the optimality guarantee for our proposed test, in that the asymptotic sum of type I and type II errors for our test matches the lower bound across all possible tests.  

Theorem 5. (Optimality). For a given test $\psi$, define the asymptotic type II error as $f(\psi) = \limsup_{n\to\infty} e^{\frac{1}{\psi,2}}$, and the asymptotic sum of type I and type II errors as $g(\psi) = \limsup_{n\to\infty}(e_{\psi,1} + e_{\psi,2})^{\frac{1}{n}}$. Denote the test in Theorem 4(a),(b) by $\psi_1$ and $\psi_2$, respectively.  

(a) (Fixed Type I error). Denote the class of all tests whose type I error are lower than $\alpha$ by $\Psi_{\alpha}$. When $1/(1-\Delta)$ is an integer, $m=\Omega(n)$, and $m=o(\exp\sqrt{n})$, or when $\Delta$ is a function of $n$, and $1/\Delta(n)=\Omega(m)$, then $\inf_{\psi\in\Psi_{\alpha}}f(\psi)=f(\psi_{1})-O(1/m)$.

(b) (Sum of Type I and Type II errors). When $1/(1-\Delta)$ is an integer, or when $\Delta$ is a function of $n$, and $1/\Delta(n)=\Omega(m)$, then $\inf_{\psi}g(\psi)=g(\psi_{2})-O(1/m)$.  

Theorem 5(a) shows that, among all tests with type I error not exceeding $\alpha$, our proposed test $\psi_1$ is asymptotically optimal in terms of the worst-case efficiency. The gap between the asymptotic type II error of $\psi_1$ and the optimal test is $O\left(1/m\right)$. Given that $m$ is typically very large, our test is effectively optimal in practice. Theorem 5(b) shows that, among all tests, our proposed test $\psi_2$ minimizes the asymptotic sum of type I and type II errors. The gap between $g(\psi_2)$ and the theoretical minimum $\inf_\psi g(\psi)$ is also $O\left(1/m\right)$.  

In the proof of Theorem 5, a key challenge lies in characterizing $\inf_{\psi \in \Psi_{\alpha}} f(\psi)$ and $\inf_{\psi} g(\psi)$ Take bounding $\inf_{\psi \in \Psi_{\alpha}} f(\psi)$ as an example. By definition, $\inf_{\psi \in \Psi_{\alpha}} f(\psi)$ can be expressed in a minimax form, $\inf_{\psi} \lim_{n \to \infty} \sup_{\mathbf{P}_{1:n}} e_{\psi,2}^{1/n}$, where $e_{\psi,2}$ is the type II error. A naive lower bound, obtained by setting all $\mathbf{P}_i = \mathbf{P}^*$, turns out to be too loose to match the upper bound. To obtain a sharp lower bound, we construct a specific distribution over $\mathbf{P}_{1:n}$, by sampling each $\mathbf{P}_i$ independently from a carefully chosen mixture over a finite set of least-favorable NTP distributions $\mathbf{R}_i$. This randomized construction enables us to evaluate the type II error analytically. However, this approach also introduces significant technical challenges. Even under randomized $\mathbf{P}_i$, computing the corresponding type II error is non-trivial. To address this, we leverage symmetry among the sampled distributions to simplify combinatorial counting and identify the optimal rejection region. We then compute the limiting type II error, establish a tight lower bound matching the upper bound up to $O(1/m)$, and finally the minimax optimality.  

Moreover, in Theorem 5, we treat $1/(1-\Delta) \in \mathbb{N}$ and $\Delta = \Delta_n$ separately. This is because the least favorable NTP distribution we construct is the equal-probability mixture of $(1-\Delta, \ldots, 1-\Delta, 0, \ldots, 0)$ and its permutations when $1/(1-\Delta)$ is an integer, and $(\Delta, 1-\Delta)$ times  

$\Delta, 0, \ldots, 0)$ and its permutations when $\Delta$ is a function of $n$. For the latter, the structure of the least-favorable NTP distribution P changes, and its support reduces to two atoms, leading to a different form of the optimal rejection region.  

# 4.3 Partial inheritance  

We continue to use the test statistic $Y_t$ as the random number $U_{t,\omega_t}$, and obtain the distribution of $Y_t$ under the null and alternative hypotheses.  

Theorem 6. (Distribution of $Y_t$). Under $\mathcal{H}_0$, $\mathbb{P}_{\mathcal{H}_0}(Y_t \leq r|\mathbf{P}_t, \mathbf{Q}_t) = r$, for $r \in [0,1]$. Under $\mathcal{H}_1$, $\mathbb{P}_{\mathcal{H}_1}(Y_t \leq r|\mathbf{P}_t, \mathbf{Q}_t) = \sum_{i=1}^{n} \sum_{j \neq i} \{p_i/(1-p_j)\} (r - r^{1/p_j} p_j) q_{ij} + \sum_{i=1}^{n} p_i r^{1/p_i} q_{ii}$, for $r \in [0,1]$. Moreover, $\mathbb{P}_{\mathcal{H}_1}(Y_t \leq r|\mathbf{P}_t, \mathbf{Q}_t) \leq r$.  

Next, we derive the optimal score function and threshold value. We choose the distribution class as $\mathcal{P}_{\Delta} := \{\mathbf{P} : \max_{\omega \in \mathcal{W}} P_{\omega} \leq 1 - \Delta\}, \Delta \in (0,1)$, and $\mathcal{Q}_{\theta} := \{\mathbf{Q} : q_{ii} \geq \theta\}$, for $\theta \in (1/2,1)$.  

Theorem 7. (Optimal score function and threshold value).

(a) For both rejection rule design settings, the optimal score function is  

$$
\begin{array} { r } { h _ { \Delta } ^ { * } ( r ) = \left\{ \begin{array} { l l } { \log \left( \frac { 1 - \theta } { \Delta } + \left( \lfloor \frac { 1 } { 1 - \Delta } \rfloor \, \theta + \frac { 1 } { \Delta } \theta - \frac { 1 } { \Delta } \right) r ^ { \frac { \Delta } { 1 - \Delta } } + \theta r ^ { \frac { \bar { \Delta } } { 1 - \bar { \Delta } } } \right) } & { i f \, \Delta \geq \frac { 1 } { 2 } , } \\ { \log \left( 2 ( 1 - \theta ) + ( 2 \theta - 1 ) \left( r ^ { \frac { 1 - \Delta } { \Delta } } + r ^ { \frac { \bar { \Delta } } { 1 - \bar { \Delta } } } \right) \right) } & { i f \, \Delta < \frac { 1 } { 2 } . } \end{array} \right. } \end{array}
$$  

Moreover, the worst case in the minimax problem is attained at the following least-favorable NTP distribution in $\mathcal{P}_{\Delta}$ and the feature matrix,  

$$
\mathbf { P } ^ { * } = \left( 1 - \Delta , \ldots , 1 - \Delta , 1 - ( 1 - \Delta ) \left[ \frac { 1 } { 1 - \Delta } \right] , 0 , \ldots \right) , \, \mathbf { Q } ^ { * } = \left( \begin{array} { l l l l l } { \theta } & { 1 - \theta } & { 0 } & { \cdots } & { 0 } \\ { 1 - \theta } & { \theta } & { 0 } & { \cdots } & { 0 } \\ { 1 - \theta } & { 0 } & { \theta } & { \cdots } & { 0 } \\ { \vdots } & { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { 1 - \theta } & { 0 } & { 0 } & { \cdots } & { \theta } \end{array} \right)
$$  

(b) For the setting of fixed type I error with $\alpha$, the optimal threshold is $\gamma_n = n \cdot \mathbb{E}_0 h^*_\Delta(Y) + \Phi^{-1}(1 - \alpha)\sqrt{n \cdot \text{Var}_0(h^*_\Delta(Y))}$. For the setting of minimizing the sum of type I and type II errors, the optimal threshold is $\gamma_n = \log\{\alpha^*/(1 - \alpha^*)\} if \Delta \geq 1/2$, and $\log\{\beta^*/(1 - \beta^*)\}$ otherwise, where $\alpha^*$, $\beta^*$ are the solutions to:  

$$
\begin{array} { r l r } & { } & { \operatorname* { i n f } _ { \alpha ^ { * } \in ( 0 , 1 ) } \int _ { 0 } ^ { 1 } \left\{ \frac { 1 - \theta } { \Delta } + \left( \left[ \frac { 1 } { 1 - \Delta } \right] \theta + \frac { 1 } { \Delta } \theta - \frac { 1 } { \Delta } \right) r ^ { \frac { \Delta } { 1 - \Delta } } + \theta r ^ { \frac { \bar { \Delta } } { 1 - \bar { \Delta } } } \right\} ^ { \alpha ^ { * } } d r } \\ & { } & { \operatorname* { i n f } _ { \beta ^ { * } \in ( 0 , 1 ) } \int _ { 0 } ^ { 1 } \left\{ 2 ( 1 - \theta ) + ( 2 \theta - 1 ) \left( r ^ { \frac { 1 - \Delta } { \bar { \Delta } } } + r ^ { \frac { \bar { \Delta } } { 1 - \bar { \Delta } } } \right) \right\} ^ { \beta ^ { * } } d r . } \end{array}
$$  

We note that the least favorable $\mathbf{Q}^*$ in Theorem 7 is not unique. In fact, all feature matrices $\mathbf{Q}$ that achieve the worst-case efficiency satisfy the following conditions: $q_{ii} = \theta$; for $i \leq \left\lfloor \frac{1}{1-\Delta} \right\rfloor$, $\sum_{j=1}^{\left\lfloor \frac{1}{1-\Delta} \right\rfloor} q_{ij} = 1$; for $i > \left\lfloor \frac{1}{1-\Delta} \right\rfloor$, $\sum_{j=1}^{\left\lfloor \frac{1}{1-\Delta} \right\rfloor} q_{ij} = 1 - \theta$; and for $j > \left\lfloor \frac{1}{1-\Delta} \right\rfloor$, $q_{ij} = 0$. We also note that, for complete inheritance, the problem can be reduced to considering only the extreme points thanks to the convexity of the objective function. However, for partial inheritance, convexity is no longer guaranteed. To address this challenge, we adopt a two-step approach. That is, we first fix the largest element of the multivariable function and analyze the convexity of the function with respect to the remaining $n - 1$ variables. We then remove the restriction on the largest element, and reduce the problem to a univariate function.  

Finally, we establish the optimality guarantee for our proposed test, mirroring the optimality result for the complete inheritance setting.  

Theorem 8. (Optimality). Denote the test in Theorem 7(a),(b) by $\tilde{\psi}_1$ and $\tilde{\psi}_2$, respectively.  

(a) (Fixed Type I error). When $1/(1-\Delta)$ is an integer, $m=\Omega(n)$, and $m=o(\exp\sqrt{n})$, or when $\Delta$ is a function of $n$, and $1/\Delta(n)=\Omega(m)$, then $\inf_{\psi\in\Psi_{\alpha}}f(\psi)=f(\tilde{\psi}_{1})-O(1/m)$.

(b) (Sum of Type I and Type II errors). When $1/(1-\Delta)$ is an integer, or when $\Delta$ is a function of $n$, and $1/\Delta(n)=\Omega(m)$, then $\inf_{\psi}g(\psi)=g(\tilde{\psi}_{2})-O(1/m)$.  

We emphasize that our tests can also be used for the problem of determining if a given text is written by a human or an LLM [3, 1]. While the existing methods perform well when the text strictly adheres to the watermarking scheme, they may fail when modifications such as text editing and paraphrasing are applied to the generated text [34]. In contrast, the detection methods we propose are inherently robust to such modifications. Even when the generated text is altered, our methods can still reliably and optimally detect the presence of the watermark. Such robustness enhances the practical utility of our detection methods in real-world scenarios.  

# 5 Analysis of Red-green-list Watermark  

We next apply our general hypothesis testing framework to another watermarking scheme: the red-green-list watermark [15]. Similarly as in Section 4, we first derive the feature matrix. We then establish the distribution of the test statistic, the optimal score function, and the optimality guarantee of our proposed test. Again, we first report the results for complete inheritance in Theorems 9 to 11, then parallelly for partial inheritance in Theorems 12 to 14.  

# 5.1 Feature matrix  

We first quickly review the red-green-list watermark. This technique, at each step, randomly divides the vocabulary into two parts, designated as the red list and the green list, respectively. It then selects the next token from the green list. The resulting watermarking generation process is nondeterministic, even when conditional on $\mathbf{P}_t$ and $\zeta_t$. This is different from the Gumbel-max watermarking technique. The secret key $\zeta_t$ is a subset $D \subset \{1,2,\ldots,m\}$ satisfying that $|D| = \gamma m$ for some $\gamma \in (0,1)$. The distribution of $\omega_t$, given $\mathbf{P}_t$ and $\zeta_t$, can be written as $\mathcal{S}(\mathbf{P}_t,\zeta_t) = (p_{t,1}\mathbf{1}\{1 \in D_t\},\ldots,p_{t,m}\mathbf{1}\{m \in D_t\})/\sum_{i \in D_t} p_{t,i}$. That is, given $\mathbf{P}_t$ and $\zeta_t$, $\omega_t$ follows the multinomial distribution, $\mathbb{P}(\omega_t = k|\mathbf{P}_t,\zeta_t) = p_{t,k}\mathbf{1}\{k \in D_t\}/\sum_{i \in D_t} p_{t,i}$.  

We next derive the feature matrix for the red-green-list watermark. Let $k=\binom{m}{\gamma m}$, and $A_1,A_2,\cdots,A_k$ denote all subsets of $\{1,2,\cdots,m\}$ containing $\gamma m$ elements. Let $\mathsf{S}_{\mathsf{P}_t}$ be a $k\times m$ matrix with each row being $\mathbf{P}_t$, $\mathbf{Q}_t$ be a $k\times m$ matrix with the $i$th row being $\mathbf{Q}_{t,i}=(q_{t,i1},\ldots q_{t,m})$, and $\mathbf{M}_t$ be a $k\times m$ matrix with the $i$th row being $\mathbf{M}_{t,i}=(p_{t,1}\mathbf{1}\{1\in A_i\},p_{t,2}\mathbf{1}\{2\in A_i\},\ldots,p_{t,m}\mathbf{1}\{m\in A_i\})/\sum_{i\in A_i}p_{t,i}$, for $i\in[k]$.  

Proposition 2. (Feature matrix). Following Definition 3, for $t \in [n]$, (a) (Complete inheritance). The feature matrix is $\mathbf{S}_{\mathbf{P}_t}$ under $\mathcal{H}_0$, and is $\mathbf{M}_t$ under $\mathcal{H}_1$.

(b) (Partial inheritance). The feature matrix is $\mathbf{S}_{\mathbf{P}_t}$ under $\mathcal{H}_0$, and is any matrix in the $\mathsf{cl}$ $\left\{\mathbf{Q}_t \in \mathbb{R}^{k \times m}: \mathsf{TV}(\mathbf{Q}_{t,i}, \mathbf{M}_{t,i}) \leq 1 - \theta, q_{t,ij} \geq 0, \sum_{j \in [m]} q_{t,ij} = 1\right\}$ under $\mathcal{H}_1$.  

# 5.2 Complete inheritance  

We choose the test statistic $Y_t = \mathbf{1}\{\omega_t \in \zeta_t\}$, and we obtain the distribution of $Y_t$, which does not depend on $P_t$ under $\mathcal{H}_0$.  

Theorem 9. (Distribution of $Y_t$). Under $\mathcal{H}_0$, $\mathbb{P}(Y_t = 0) = 1 - \gamma$, and $\mathbb{P}(Y_t = 1) = \gamma$. Under $\mathcal{H}_1$, $\mathbb{P}(Y_t = 1) = 1$.  

Theorem 9 highlights the distinct behavior of $Y_t$ under $\mathcal{H}_0$ versus $\mathcal{H}_1$. Under $\mathcal{H}_0$, $Y_t$ is a Bernoulli random variable, reflecting the random selection process for subsets in the red-green-list watermarking scheme. Under $\mathcal{H}_1$, $Y_t$ always equals 1, as complete inheritance ensures that every token $\omega_t$ is fully aligned with the watermarking process.  

Next, we derive the optimal threshold value. We also note that, since $Y_t$ can only take the value 0 or 1, any score function $h(\cdot)$ yields a test statistic equivalent to counting the occurrences of $\{\omega_t \in \zeta_t\}$, and as such the specific form of $h(\cdot)$ becomes irrelevant.  

Theorem 10. (Optimal threshold value). For the setting of fixed type I error with $\alpha$, the optimal threshold is $\gamma_n = n\gamma + \sqrt{n\gamma(1-\gamma)}\Phi^{-1}(1-\alpha)$. For the setting of minimizing the sum of type I and type II errors, the optimal threshold is $\gamma_n = n$.  

Finally, we establish the optimality guarantee for our proposed test.  

Theorem 11. (Optimality). Denote the test in Theorem 10(a),(b) by $\psi'_1$ and $\psi'_2$, respectively. (a) (Fixed Type I error). $\inf_{\psi \in \Psi_\alpha} f(\psi) = f(\psi'_1)$.

(b) (Sum of Type I and Type II errors). $\inf_{\psi} g(\psi) = g(\psi'_2)$.  

Theorem 11 establishes that our proposed test achieves the infimum of the asymptotic type II error and the asymptotic sum of type I and type II errors, respectively, confirming its optimality. Its proof is considerably simpler than that of the Gumbel-max watermark. Specifically, it suffices to focus on the case where each $\mathbf{P}_i$ is $(1/m, 1/m, \ldots, 1/m)$. Once the NTP distributions are fixed, deriving the optimal testing scheme reduces to analyzing a Bernoulli process under $\mathcal{H}_0$ and a deterministic sequence under $\mathcal{H}_1$. This structure ensures that the test is determined entirely by the number of occurrences of $\{\omega_t \in \zeta_t\}$, namely, $\sum_{t=1}^{n} Y_t$. It also eliminates the $O(1/m)$ term in the Gumbel-max watermark case, resulting the value of $f(\psi'_1)$ and $g(\psi'_2)$ precisely matching the theoretical infimum $\inf_\psi f(\psi)$ and $\inf_\psi g(\psi)$, respectively.  

# 5.3 Partial inheritance  

We continue to use the test statistic $Y_t = \mathbf{1}\{\omega_t \in \zeta_t\}$, where the token $\omega_t$ conditional on $\zeta_i$ follows a multinomial distribution with parameter $\mathbf{Q}_{t,i} = (q_{t,i}1, \ldots, q_{t,im})$, and we obtain the distribution of $Y_t$.  

Theorem 12. (Distribution of $Y_t$). Under $\mathbb{P}(Y_t=1)=\gamma$ and $\mathbb{P}(Y_t=0)=1-\gamma$. Under $\mathcal{H}_1$, $\mathbb{P}(Y_t=1)=\sum_{i=1}^{C_{m,\gamma}}C_{m,\gamma}^{-1}\sum_{j\in\xi_i}q_{ij}\geq\theta$, where $C_{m,\gamma}=(\frac{m}{\gamma m})$, and $\mathbb{P}(Y_t=0)\leq1-\theta$.  

Theorem 12 reflects the additional complexity of partial inheritance. Under $\mathcal{H}_0$, $Y_t$ remains a Bernoulli random variable. However, under $\mathcal{H}_1$, the probability $\mathbb{P}(Y_t = 1)$ is influenced by the distributions $\mathbf{Q}_{t,i}$ and the constraint $TV(\omega_t|\zeta_t, \mathcal{S}(\mathbf{P}_t, \zeta_t)) \leq 1 - \theta$.  

Next, we derive the optimal threshold value.  

Theorem 13. (Optimal threshold value). For the setting of fixed type I error with $\alpha$, the optimal threshold is $\gamma_n = n\gamma + \sqrt{n\gamma(1-\gamma)}\Phi^{-1}(1-\alpha)$. For the setting of minimizing the sum of type I and type II errors, the optimal threshold is $\gamma_n = \lceil n\{\log(1-\gamma)-\log(1-\theta)\}/\{\log\theta+\log(1-\gamma)-\log\gamma-\log(1-\theta)\}\rceil$.  

Finally, we establish the optimality guarantee for our proposed test.  

Theorem 14. (Optimality). Denote the test in Theorem 13(a),(b) by $\tilde{\psi}'_1$ and $\tilde{\psi}'_2$, respectively.

(a) (Fixed Type I error). $\inf_{\psi \in \Psi_\alpha} f(\psi) = f(\tilde{\psi}'_1)$.

(b) (Sum of Type I and Type II errors). $\inf_{\psi} g(\psi) = g(\tilde{\psi}'_2)$.  

The key to prove Theorem 14 lies in analyzing the worst-case scenario for $\mathcal{H}_1$. After fixing $\mathbf{P}_i$ and $\mathbf{Q}_i$, the optimal test can be derived by considering the constraint on the TV distance $TV(\omega_t|\zeta_t, \mathcal{S}(\mathbf{P}_t, \zeta_t)) \leq 1-\theta$. This leads to a feasible determination of the test statistic $\sum_{t=1}^{n} Y_t$ and its corresponding rejection region.  

# 6 Simulation Studies  

# 6.1 Gumbel-max watermark  

We first investigate the empirical performance of our proposed tests through simulations, and begin with the Gumbel-max watermark. We consider a vocabulary $\mathcal{W}$ of size $m=1000$, generate token sequences of length $n$ with $n_{\max}=3000$, and repeat the generation for 5000 times. For text generation without watermark, we sample each token from $\mathcal{W}$ uniformly. For text generation with watermark, we first randomly select a prompt, then generate the subsequent tokens based on the random seed that depends on the values of the previous five tokens. The resulting NTP distribution $\mathbf{P}_t$ satisfies multinomial distribution with the largest entry of the probability vector being $1-\Delta^*$, and the remaining terms uniformly distributed with their sum equal to $\Delta^*$. For the complete inheritance setting, at each step $t$, we generate 1000 i.i.d. random variables $U_{t,\omega}$ from the uniform $(0,1)$ distribution, then generate the watermarked text following the rule $\omega_t=\arg\max_{\omega\in\mathcal{W}}\log U_{t,\omega}/P_{t,\omega}$. For the partial inheritance setting, we need to further ensure that the total variation distance between the adjusted probability distribution and the original one remains within a specified bound. Towards that end, we calculate $\mathcal{S}(\mathbf{P}_t,\zeta_t)$ at each step $t$. Since it takes the form $(0,\ldots,1,\ldots,0)$, where 1 appears at the position of the selected token, we only need to control the value at this selected position in $\mathcal{S}(\mathbf{P}_t,\zeta_t)$. We set the value as $1-\theta^*$, corresponding to the upper bound of the TV distance. Then, at each step, we randomly select $\theta'$ from the interval $[\theta^*,1]$, and the corresponding distribution of $\omega|\zeta$ satisfies that the  

![](http://61.160.97.222:59003/kb-paper-images/arxiv-2025-2501.02441/4ed88a4984499612211f12501483cb64fc084e276efe3b080d6ed07e9c9229c8.png)  
Figure 1: Average type I and type II errors versus text length for the Gumbel-max watermark under the setting of fixed type I error, where $\Delta = 0.005$ and $\theta = \{0.7, 0.8, 0.9, 0.95\}$.  

term corresponding to 1 is $\theta'$, and the sum of the remaining terms equals $1 - \theta'$. We randomly sample those remaining terms from the interval $[0, 1]$ then normalize them to sum to $1 - \theta'$. We set the true value of $\Delta^*$ randomly sampled from $[0.001, 0.5]$ and $\theta^* = 0.8$, whereas we set the working value of $\Delta = \{0.005, 0.01\}$ and $\theta = \{0.7, 0.8, 0.9, 0.95\}$.  

We apply our proposed tests to the generated sequences, and consider both the test of fixed type I error with $\alpha = 0.05$ and the test of minimizing the sum of type I and type II errors. For each setting, we use the theoretically optimal score function $h_{\Delta}^{*}(r)$ and corresponding threshold $\gamma_{n}$, as derived in Theorems 4 and 7. We also compare with two baseline score functions, $h_{\text{ars}}(r) = -\log(1 - r)$ as proposed by [1], and $h_{\log}(r) = \log r$. To ensure a fair comparison, we compute their corresponding optimal threshold using the procedure detailed in Appendix E.1.  

Figures 1 reports the average type I and type II errors versus the text length, based on 5000 replications, under the setting of fixed type I error, where $\Delta = 0.005$ and $\theta = \{0.7, 0.8, 0.9, 0.95\}$. In this case, the rejection thresholds for the baseline score functions $h_{\log}$ and $h_{\text{ars}}$ depend solely on $\alpha$, but not on $\theta$. We see that, empirically, the type I errors generally align with the nominal level of 0.05 and become increasingly closer to 0.05 as the text length increases, agreeing with our theory regarding the asymptotic control of type I error. We also see that, in both the complete and partial inheritance settings, our proposed test achieves consistently lower type II error compared to the baseline methods, highlighting the effectiveness of our approach in minimizing type II errors while maintaining type I error control. We report the results for $\Delta = 0.01$ in Appendix E.2, and observe similar qualitative patterns.  

Figures 2 report the average sum of type I and type II errors versus text length, based on  

![](http://61.160.97.222:59003/kb-paper-images/arxiv-2025-2501.02441/e629ee3aad2a81ea4896da05421721655beac84dabbd2dd3179fabfbdaf5486c.png)  
Figure 2: Average sum of type I and type II errors versus text length for the Gumbel-max watermark under the setting of minimizing the sum of type I and type II errors, where $\Delta = 0.005$ and $\theta = 0.8$.  

5,000 repetitions, under the setting of minimizing the sum of two errors, where $\Delta = 0.005$ and $\theta = 0.8$. In this case, the rejection thresholds for the baseline score functions $h_{\log}$ and $h_{\text{ars}}$ are numerically solved and do depend on $\theta$. We see again that our proposed test consistently achieves a smaller sum of errors than the baseline methods. We report the results for $\Delta = 0.01$ and $\theta = \{0.7, 0.8, 0.9, 0.95\}$ in Appendix E.2, and again observe similar patterns. In general, our proposed tests remain effective and robust as long as $\Delta$ and $\theta$ are within a reasonable range.  

# 6.2 Red-green-list watermark  

We next study the red-green-list watermark. We adopt a similar simulation setup as before, and consider a vocabulary $\mathcal{W}$ of size $m=1,000$, token sequences of length $n$ with $n_{\max}=200$. For text generation with watermark, the NTP distribution is a 1000-dimensional multinomial distribution, and the parameter $\gamma$ is set to 0.5, meaning the red list and the green list are of equal length. For the complete inheritance setting, the secret key $\zeta_t$ is a subset of $\{1,2,\ldots,1000\}$ containing 500 elements. We randomly select the secret keys based on a random seed determined by the previous five tokens. For the partial inheritance setting, we set the TV distance between $\omega|\zeta$ and $\mathcal{S}(\mathbf{P}_t,\zeta_t)$ to be $1-\theta^*$. To satisfy this constraint, we adjust the probabilities for the green and red tokens, such that the sum of probabilities for the green list equals $1-\theta^*$, whereas the sum for the red list equals $\theta^*$. We set the true value of $\theta^*=0.8$, and set the working value of $\theta=\{0.7,0.8,0.9,0.95\}$. We report the results for $\theta^*=\{0.6,0.7\}$ in Appendix E.3, and observe similar qualitative patterns.  

We apply our proposed tests to the generated sequences. Since for the red-green-list watermark, the specific form of the score function $h(\cdot)$ does not affect the hypothesis testing outcome, and thus there is no need to compare different score functions. We choose the optimal threshold $\gamma_n$ following Theorems 10 and 13 for complete inheritance and partial inheritance, respectively.  

Figure 3 reports the average type I and type II errors versus the text length, based on 5000 replications, under the setting of fixed type I error. In this case, the rejection threshold of our test depends only on the nominal level $\alpha$, but not on the value of $\theta$. We again see that the  

![](http://61.160.97.222:59003/kb-paper-images/arxiv-2025-2501.02441/28b50cba2b1271dc1a114c536c8013abdd6deec5f9fd39902e41190ba12d4d00.png)  
Figure 3: Average type I and type II errors versus text length for the red-green-list watermark under the setting of fixed type I error, where $\theta^{*} = 0.8$ and $\theta = \{0.7, 0.8, 0.9, 0.95\}$.  

![](http://61.160.97.222:59003/kb-paper-images/arxiv-2025-2501.02441/adbf2f7eb9bd1797e4b0de9fb6adaa94b3b6121ca7def7d339299c38c8102de9.png)  
Figure 4: Average sum of type I and type II errors versus text length for the red-green-list watermark under the setting of minimizing the sum of type I and type II errors, where $\theta^{*} = 0.8$ and $\theta = \{0.7, 0.8, 0.9, 0.95\}$.  

type I errors empirically align closely with the nominal level of 0.05 and become increasingly closer to 0.05 as the text length increases, agreeing with our theory. Additionally, in both the complete and partial inheritance settings, the Type II errors decrease rapidly to 0. In fact, we can compute the type II error of our optimal test, which is given by  

$$
\mathbb { P } \left( Z < \frac { \sqrt { n } ( \gamma - \theta ) } { \sqrt { \theta ( 1 - \theta ) } } + \sqrt { \frac { \gamma ( 1 - \gamma ) } { \theta ( 1 - \theta ) } } \Phi ^ { - 1 } ( 1 - \alpha ) \right) ,
$$  

where $Z$ is a standard normal variable and $\Phi$ is its cdf. When $\theta$ is close to 1, the term on the right-hand-side of becomes large, leading to a rapid decay of the type II error. This highlights the efficiency of our test, particularly for the cases when the TV distance constraint is tight.  

Figures 4 reports the average sum of type I and type II errors versus the text length, based on 5,000 replications, under the setting of minimizing the sum of type I and type II errors. In this case, the rejection threshold of our test depends on $\theta$. We see that the sum of type I and type II errors drops quickly to 0, reflecting the effectiveness of our test.  

Finally, we clarify the difference in how $\theta$ appears in Figures 3 and 4. In the fixed type I error setting (Figure 3), the rejection threshold depends only on the nominal level $\alpha$, and is  

![](http://61.160.97.222:59003/kb-paper-images/arxiv-2025-2501.02441/edeee5f0456ae113fea0da7390dcd6fc75da1e51714b1215864a8d1707fd1837.png)  
Figure 5: Average type I error, type II error, and sum of type I and type II errors for the real LLM study.  

independent of $\theta$ (as shown in Theorem 13). Therefore, only one curve appears in each panel. In contrast, in the setting where the goal is to minimize the total error (Figure 4), the rejection threshold depends on the chosen value of $\theta$, and hence multiple curves are plotted. Additional results for other values of $\theta$ that are used for data generation are reported in Appendix E.2.  

# 7 A Study with Large Language Models  

We conduct a numerical study using real-world LLMs. More specifically, we use OPT-1.3B [33] to generate 1500 watermarked sentences with the Gumbel-max watermark. We use these sentences to fine-tune a publicly available GPT-2 model [21]. We then generate 1000 tokens, first from the original GPT-2 model, then separately from the fine-tuned GPT-2 model. The former corresponds to the scenario without data misappropriation, and the latter with data misappropriation. We repeat this experiment for 100 times.  

A technical challenge in this experiment arises from the fact that OPT-1.3B and GPT-2 use different tokenization schemes. The sentences are formatted in the Alpaca instruction-response style [26] and used to fine-tune the GPT-2 model [21], mimicking an unauthorized reuse of LLM-generated data. To align them, we tokenize the generated sentences with the encoder of OPT-1.3B, embed the watermarks at the token level, decode the watermarked tokens back into text using the decoder of OPT-1.3B, and use this text to fine-tune GPT-2. We apply our proposed test with the optimal score function, and continue to compare with the two baseline solutions, $h_{\text{ars}}(r) = -\log(1 - r)$ and $h_{\log} = \log r$. Moreover, we set the working value of $\Delta = \{0.005, 0.01\}$ and $\theta = 0.8$ for the partial inheritance setting.  

Figure 5(a),(b) report the average type I and type II errors, based on 100 repetitions, under the setting of fixed type I error, and Figure 5(c) reports the average sum of type I and type II errors under the setting of minimizing the sum of two errors. We see that all methods maintain the type I error around the nominal level of 0.05, but our proposed method achieves a substantially lower type II error as well as the sum of two errors, indicating a clearly superior performance over the baseline solutions by a large margin.  

This experiment thus provides empirical evidence that our detection method can reliably identify data misappropriation in practical scenarios involving real LLMs. Also importantly, our method remains effective even when the victim and suspect models differ in architecture and tokenization, and when only limited quantities of watermarked data are used for fine-tuning. These results underscore the practical relevance and robustness of our proposed method.  

# References  

[1] Scott Aaronson. Talk on watermarking of large language models, available at 10:10:4, 2023. https://www.youtube.com/watch?v=2Kx9jbSMZqA.

[2] Mikhail J Atallah, Victor Raskin, Michael Crogan, Christian Hempelmann, Florian Kerschbaum, and Dina et al. Mohamed. Natural language watermarking: Design, analysis, and a proof-of-concept implementation. In Information Hiding: 4th International Workshop, IH 2001 Pittsburgh, PA, USA, pages 185–200. Springer, 2001.

[3] Franziska Boenisch. A systematic review on model watermarking for neural networks. Frontiers in Big Data, 4, November 2021.

[4] Zhongze Cai, Shang Liu, Hanzhao Wang, Huaiyang Zhong, and Xiaocheng Li. Towards better statistical understanding of watermarking Ilms, 2024.

[5] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles, 2022.

[6] Herman Chernoff. A Measure of Asymptotic Efficiency for Tests of a Hypothesis Based on the sum of Observations. The Annals of Mathematical Statistics, 23(4):493 – 507, 1952.

[7] Miranda Christ, Sam Gunn, and Or Zamir. Undetectable watermarks for language models, 2023.

[8] Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch. Flocks of stochastic parrots: Differentially private prompt learning for large language models, 2023.

[9] Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, and Teddy Furon. Three bricks to consolidate watermarks for large language models, 2023.

[10] Lee Gesmer. Copyright and the challenge of large language models. https://wwwmasslawblog.com/correct/correct-and-the-challenge-of-large-language-models-part-2-2/, 10 2024. America.  

[11] Oded Goldreich. Foundations of cryptography: volume 2, basic applications, volume 2. Cambridge university press, 2001.  

[12] Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, and Heng Huang. Unbiased watermark for large language models, 2023.

[13] Baihe Huang, Hanlin Zhu, Banghua Zhu, Kannan Ramchandran, Michael I. Jordan, and Jason D. Lee et al. Towards optimal statistical watermarking, 2024.

[14] Zhengyuan Jiang, Jinghuai Zhang, and Neil Zhenqiang Gong. Evading watermark based detection of ai-generated content. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, pages 1168–1181, 2023.

[15] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models, 2024.

[16] Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, and Weijie J. Su. A statistical framework of watermarks for large language models: Pivot, detection efficiency and optimal rules, 2024.

[17] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature, 2023.

[18] Eduardo Santacana Noorjahan Rahman. Beyond fair use: Legal risk evaluation for training Ilms on copyrighted text. https://blog.genlaw.org/CameraReady/57.pdf, 7 2023. America.

[19] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, and Ilge Akkaya et al. Gpt-4 technical report, 2024.

[20] Julien Piet, Chawin Sitawarin, Vivian Fang, Norman Mu, and David Wagner. Mark my words: Analyzing and evaluating language model watermarks, 2024.

[21] Alex Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Technical Report, 2019.

[22] Jie Ren, Han Xu, Pengfei He, Yingqian Cui, Shenglai Zeng, and Jiankun et al. Zhang. Copyright protection in generative ai: A technical perspective. arXiv preprint arXiv:2402.02333, 2024.

[23] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can ai-generated text be reliably detected? arXiv preprint arXiv:2303.11156, 2023.

[24] Matthew Sag. Copyright safety for generative AI. https://Houstonlawreview.org/article/92126, 2023.

[25] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models, 2017.  

[26] Rohan Iaori, Ishaan Gulrajani, Ianyi Zhang, Yann Dubois, Xuechen L1, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

[27] The New York Times. The times sues openai and microsoft over a.i. use of copyrighted work. https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html, 12 2023. America.

[28] Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.

[29] Debora Weber-Wulff, Ala Anohina-Naumeca, Sonja Bjelobaba, Tomáš Foltýnek, Jean Guerrero-Dib, and Olumide et al. Popoola. Testing of detection tools for ai-generated text. International Journal for Educational Integrity, 19(1), December 2023.

[30] Xiaodong Wu, Ran Duan, and Jianbing Ni. Unveiling security, privacy, and ethical concerns of chatgpt, 2023.

[31] Yihan Wu, Zhengmian Hu, Hongyang Zhang, and Heng Huang. Dipmark: A stealthy, efficient and resilient watermark for large language models, 2024.

[32] Xi Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, and Jie Zhang et al. Watermarking text generated by black-box language models, 2023.

[33] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.

[34] Xuandong Zhao, Prabhanjan Ananth, Lei Li, and Yu-Xiang Wang. Provable robust watermarking for ai-generated text, 2023.

[35] Xuandong Zhao, Lei Li, and Yu-Xiang Wang. Permute-and-flip: An optimally robust and watermarkable decoder for lllms, 2024.

[36] Xuandong Zhao, Yu-Xiang Wang, and Lei Li. Protecting language generation models via invisible watermarking. In International Conference on Machine Learning, pages 42187-42199. PMLR, 2023.  

# A An Impossibility Result  

We show that, based solely on the generated text and without watermarking, detecting data misappropriation is impossible. Consider $N$ sequences $x_1, \ldots, x_N \stackrel{\text{i.i.d.}}{\sim} \mathbf{P}$ that are used for training, where $N$ denotes the number of training sequences. We first introduce an assumption.  

Assumption 1. If the training data $x_1, \ldots, x_N$ follow an identical probability distribution $\mathbf{P}$, then as $N \to \infty$, the distribution of the output of the LLM converges to $\mathbf{P}$.  

It is easy to see that this assumption is natural and reasonable. Suppose we have two LLMs, $f_1$ and $f_2$, and the data used to train $f_1$ is i.i.d. and follows the distribution $\mathbf{P}$. We aim to detect whether the LLM $f_2$ has used the data generated by the first LLM $f_1$ for training. That is, the null hypothesis states that $f_2$ is not trained using the data generated from $f_1$, and the alternative hypothesis states the otherwise. We have the following result.  

Proposition 3. The summation of type-I and type-II errors converge to $1$ as $N \to \infty$.  

Proof. By Assumption 1, the distribution of the output generated by $f_1$ converges to P as $N \to \infty$. Therefore, under both hypotheses, the distribution of the training data converges to P. Then the TV distance between the output distributions under the two hypotheses converge to 0 as $N \to \infty$, making the summation of type-I and type-II errors converge to 1. $\square$  

This result highlights the inherent limitations of detecting data misappropriation solely based on the generated text. Consequently, watermarking is essential to assist in reliable detection. In practice, one may infer that a model has distilled ChatGPT if it responds with "ChatGPT" when asked "What language model are you?". However, we argue that this alone is insufficient evidence to support such a claim. Since "ChatGPT" frequently co-occurs with "language model" in many publicly available texts, models trained on these datasets may naturally learn to generate "ChatGPT" as a likely response, even without direct exposure to the ChatGPT-generated data.  

# B Proofs for General Framework in Section 3  

# B.1 Proof of Theorem 1  

Since $(a)$ is a special case of $(b)$, in the following, we directly prove $(b)$. Consider the moment-generating function $\phi_{\mathbf{P},\mathbf{Q},h}(\theta):=\mathbb{E}_{1,\mathbf{P},\mathbf{Q}}e^{-\theta h(Y)}$. By Markov's inequality, for any $\theta>0$,  

$$
\begin{array} { r l } & { 1 - \mathbb { E } _ { 1 } T _ { h } ( Y _ { 1 : n } ) = \mathbb { P } _ { 1 } ( \sum _ { t = 1 } ^ { n } - h ( Y _ { t } ) \geq - \gamma _ { n , \alpha } ) } \\ & { \leq \exp \{ \gamma _ { n , \alpha } \theta \} \mathbb { E } _ { 1 } \exp \{ \sum _ { t = 1 } ^ { n } - \theta h ( Y _ { t } ) \} \leq \exp \{ \gamma _ { n , \alpha } \theta \} \cdot \exp \{ n \log \phi _ { \mathcal { P } , \mathcal { Q } , h } ( \theta ) \} , } \end{array}
$$  

where $\phi_{\mathcal{P},\mathcal{Q},h}(\theta) = \max_{\mathbf{Q}\in\mathcal{Q}} \max_{\mathbf{P}\in\mathcal{P}} \mathbb{E}_{1,\mathbf{P},\mathbf{Q}} e^{-\theta h(Y)}$.  

Recall the definition of $\gamma_{n,\alpha}$ that it is the threshold chosen to ensure the significance level at $\alpha$, i.e., $\mathbb{P}_{\mathcal{H}_0}(\sum_{t=1}^n h(Y_t) \geq \gamma_{n,\alpha}) = \alpha$. Following the strong law of large numbers, $\lim_{n \to \infty} \gamma_{n,\alpha}/n \leq \mathbb{E}_0 h(Y)$. Therefore,  

$$
\begin{array} { r l } & { \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \
$$  

Next, we prove the tightness. By standard results from the large deviation theory, if we set all the next-token-prediction (NTP) distribution as $\mathbf{P}_k$, all the feature matrix as $\mathbf{Q}_k$, then  

$$
\operatorname* { l i m } _ { n \to \infty } \mathbb { P } ( T _ { h } ( Y _ { 1 : n } ) = 0 | \mathcal { H } _ { 1 } ) ^ { \frac { 1 } { n } } = e ^ { - R _ { \mathbf { P } _ { k } , \mathbf { Q } _ { k } } ( h ) } \geq e ^ { - \left( R _ { \mathcal { P } , \mathcal { Q } } ( h ) + \epsilon _ { k } ^ { ( 1 ) } \right) } ,
$$  

where $\epsilon_{k}^{(1)} \rightarrow 0$ as $k \rightarrow \infty$. It then follows that  

$$
\begin{array} { r } { \mathbb { P } ( T _ { h } ( Y _ { 1 : n } ) = 0 | \mathcal { H } _ { 1 } ) \geq e ^ { - n \left( R _ { \mathcal { P } , \mathcal { Q } } ( h ) + \epsilon _ { k } ^ { ( 1 ) } + \epsilon _ { n } ^ { ( 2 ) } \right) } , } \end{array}
$$  

where $\epsilon_{n}^{(2)}$ denotes a sequence of positive numbers approaching to zero as $n \rightarrow \infty$ to ensure (10) to hold. Then the lower bound holds by choosing a sufficiently large $k$.  

This completes the proof of Theorem 1.  

# B.2 Proof of Theorem 2  

We first present two supporting lemmas.  

Lemma 1. ([6]) Let $S_n = X_1 + \cdots + X_n$, where $X_1, \cdots, X_n$ are i.i.d. Define $m(a) = \inf_t e^{-at} \mathbb{E}(e^{tX})$. If $\mathbb{E}(X) > -\infty$ and $a \leq \mathbb{E}(X)$, then $\mathbb{P}(S_n \leq na) \leq [m(a)]^n$. For $0 < \epsilon < m(a)$, we have $\lim_{n \to \infty} \{m(a) - \epsilon\}^n / \mathbb{P}(S_n \leq na) = 0$. If $\mathbb{E}(X) < +\infty$ and $a \geq \mathbb{E}(X)$, then $\mathbb{P}(S_n \geq na) \leq [m(a)]^n$. For $0 < \epsilon < m(a)$, we have $\lim_{n \to \infty} \{m(a) - \epsilon\}^n / \mathbb{P}(S_n \geq na) = 0$.  

Lemma 2. (Exchangeability of infimum) For any function $f(\theta,t)$, we have $\inf_{\theta} \inf_{t} f(\theta,t) = \inf_{t} \inf_{\theta} f(\theta,t)$.  

Proof. We have  

$$
f ( \theta , t ) \geq \operatorname* { i n f } _ { \theta } f ( \theta , t ) \Rightarrow \operatorname* { i n f } _ { t } f ( \theta , t ) \geq \operatorname* { i n f } _ { t } \operatorname* { i n f } _ { \theta } f ( \theta , t ) \Rightarrow \operatorname* { i n f } _ { t } \operatorname* { i n f } _ { t } f ( \theta , t ) \geq \operatorname* { i n f } _ { t } \operatorname* { i n f } _ { \theta } f ( \theta , t ) .
$$  

Similarly, we have $\inf_{\theta} \inf_{t} f(\theta, t) \leq \inf_{t} \inf_{\theta} f(\theta, t)$.  

This completes the proof of Lemma 2.  

We now proceed with the proof of Theorem 2. Again, we prove (b), as (a) is a special case. We first obtain the asymptotic result of the sum of two types of errors. By the Markov inequality, there exist $\theta_1 > 0$ and $\theta_2 > 0$, such that  

$$
\begin{array} { r l } & { \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \sum _ { t = 1 } ^ { n } h ( Y _ { t } ) \geq \gamma \right) + \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( \sum _ { t = 1 } ^ { n } h ( Y _ { t } ) < \gamma \right) } \\ & { = \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \sum _ { t = 1 } ^ { n } \theta _ { 1 } h ( Y _ { t } ) \geq \theta _ { 1 } \gamma \right) + \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( - \theta _ { 2 } \sum _ { t = 1 } ^ { n } h ( Y _ { t } ) > \theta _ { 2 } \gamma \right) } \\ & { \leq \frac { \mathbb { E } _ { \mathcal { H } _ { 0 } } \exp \{ \sum _ { t = 1 } ^ { n } \theta _ { 1 } h ( Y _ { t } ) \} } { \exp \{ \theta _ { 1 } \gamma \} } + \exp \{ \theta _ { 2 } \gamma \} \cdot \mathbb { E } _ { \mathcal { H } _ { 1 } } \exp \{ - \sum _ { t = 1 } ^ { n } \theta _ { 2 } h ( Y _ { t } ) \} } \\ & { \leq \frac { \exp \{ n \cdot \log \mathbb { E } _ { 0 } \exp \left( \theta _ { 1 } h ( Y ) \right) \} } { \exp \{ \theta _ { 1 } \gamma \} } + \exp \{ \theta _ { 2 } \gamma \} \cdot \exp \{ n \log \mathbb { E } _ { 1 , \mathbf { P } , \mathbf { Q } } \exp \left( - \theta _ { 2 } h ( Y ) \right) \} . } \end{array}
$$  

Therefore,  

$$
\begin{array} { r l } & { \operatorname* { i n f } _ { \gamma } \Bigg \{ \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \sum _ { t = 1 } ^ { n } h ( Y _ { t } ) \geq \gamma \right) + \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( \sum _ { t = 1 } ^ { n } h ( Y _ { t } ) < \gamma \right) \Bigg \} } \\ & { \leq \operatorname* { i n f } _ { \gamma } \operatorname* { i n f } _ { \theta _ { 1 } , \theta _ { 2 } > 0 } \Bigg [ \frac { \exp \{ n \cdot \log \mathbb { E } _ { 0 } \exp \left( \theta _ { 1 } h ( Y ) \right) \} } { \exp \{ \theta _ { 1 } \gamma \} } + \exp \{ \theta _ { 2 } \gamma \} \cdot \exp \{ n \log \mathbb { E } _ { 1 , \mathbf { P } , \mathbf { Q } } \exp \left( - \theta _ { 2 } h ( Y ) \right) \} \Bigg ] } \\ & { = \operatorname* { i n f } _ { \theta _ { 1 } , \theta _ { 2 } > 0 } \operatorname* { i n f } _ { \gamma } \Bigg [ \frac { \exp \{ n \cdot \log \mathbb { E } _ { 0 } \exp \left( \theta _ { 1 } h ( Y ) \right) \} } { \exp \{ \theta _ { 1 } \gamma \} } + \exp \{ \theta _ { 2 } \gamma \} \cdot \exp \{ n \log \mathbb { E } _ { 1 , \mathbf { P } , \mathbf { Q } } \exp \left( - \theta _ { 2 } h ( Y ) \right) \} \Bigg ] \, . } \end{array}
$$  

By choosing  

$$
\gamma = \frac { 1 } { \theta _ { 1 } + \theta _ { 2 } } \ln \frac { \theta _ { 1 } \exp \{ n \cdot \log \mathbb { E } _ { 0 } \exp \left( \theta _ { 1 } h ( Y ) \right) \} } { \theta _ { 2 } c \exp \{ n \cdot \log \mathbb { E } _ { 1 , \mathbf { P } , \mathbf { Q } } \exp \left( - \theta _ { 2 } h ( Y ) \right) \} } ,
$$  

the above sum attains its minimum.  

We then prove the tightness. By Lemma 1, we have that, for any $b \in (0,1)$, there exists $N > 0$, when $n > N$,  

$$
\mathbb { P } ( S _ { n } \geq n a ) \leq [ m ( a ) ] ^ { n } , \quad \mathbb { P } ( S _ { n } \geq n a ) \geq [ b m ( a ) ] ^ { n }
$$  

Then we have,  

$$
\begin{array} { r l } & { \operatorname* { i n f } _ { \gamma } \Bigg \{ \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \sum _ { t = 1 } ^ { n } h ( Y _ { t } ) \geq \gamma \right) + \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( \sum _ { t = 1 } ^ { n } h ( Y _ { t } ) < \gamma \right) \Bigg \} } \\ & { \geq \operatorname* { i n f } _ { \gamma } \operatorname* { i n f } _ { \theta _ { 1 } , \theta _ { 2 } > 0 } \left( \left[ b \cdot \frac { \exp n \cdot \log \mathbb { E } _ { 0 } \exp \left\{ \theta _ { 1 } h ( Y ) \right\} } { \exp \theta _ { 1 } \gamma } \right] ^ { n } + [ b \cdot \exp \theta _ { 2 } \gamma \cdot \exp n \log \mathbb { E } _ { 1 , \mathbf { P } , \mathbf { Q } } \exp \left\{ - \theta _ { 2 } h ( Y ) \right\} ] ^ { n } \right) } \\ & { = \operatorname* { i n f } _ { \theta _ { 1 } , \theta _ { 2 } > 0 } \operatorname* { i n f } _ { \gamma } \left( \left[ b \cdot \frac { \exp n \cdot \log \mathbb { E } _ { 0 } \exp \left\{ \theta _ { 1 } h ( Y ) \right\} } { \exp \theta _ { 1 } \gamma } \right] ^ { n } + [ b \cdot \exp \theta _ { 2 } \gamma \cdot \exp n \log \mathbb { E } _ { 1 , \mathbf { P } , \mathbf { Q } } \exp \left\{ - \theta _ { 2 } h ( Y ) \right\} ] ^ { n } \right) } \end{array}
$$  

$$
= \left[ b \cdot \operatorname* { i n f } _ { \theta _ { 1 } , \theta _ { 2 } > 0 } \exp { \frac { \theta _ { 2 } } { \theta _ { 1 } + \theta _ { 2 } } } \cdot \log \mathbb { E } _ { 0 } \exp \left\{ \theta _ { 1 } h ( Y ) \right\} \cdot \exp { \frac { \theta _ { 1 } } { \theta _ { 1 } + \theta _ { 2 } } } \log \mathbb { E } _ { 1 , \mathbf { p } , \mathbf { Q } } \exp \left\{ - \theta _ { 2 } h ( Y ) \right\} \right] ^ { n } .
$$  

When we set $\mathbf{P}_{t}$, $\mathbf{Q}_{t}$ at $\mathbf{P}^{*}$, $\mathbf{Q}^{*}$, such that  

$$
\operatorname* { s u p } _ { h } S _ { \mathcal { P } , \mathcal { Q } } ( h ) = - \operatorname* { i n f } _ { h } \operatorname* { i n f } _ { \theta _ { 1 } , \theta _ { 2 } > 0 } \frac { \theta _ { 2 } } { \theta _ { 1 } + \theta _ { 2 } } \log \mathbb { E } _ { 0 } \exp \left\{ \theta _ { 1 } h ( Y ) \right\} + \frac { \theta _ { 1 } } { \theta _ { 1 } + \theta _ { 2 } } \log \operatorname* { s u p } _ { \mathbf { P } , \mathbf { Q } \in \mathcal { P } , \mathcal { Q } } \mathbb { E } _ { 1 , \mathbf { P } , \mathbf { Q } } \exp \left\{ - \theta _ { 2 } h ( Y ) \right\} ,
$$  

we get the desired result, which completes the proof of Theorem 2.  

# C Proofs for Gumbel-max Watermark in Section 4  

# C.1 Proof of Proposition 1  

For the complete inheritance setting, under $\mathcal{H}_1$, $S(\mathbf{P},\zeta)$ can only take values $e_1, e_2, \ldots, e_m$, where $e_k$ is the $m$-dimensional multinomial distribution satisfying the $k$-th term being 1 and the remaining terms being 0.  

For the partial inheritance setting, under $\mathcal{H}_1$, the TV distance between $\mathbf{Q}_i$ and $\mathcal{S}(\mathbf{P},\zeta)$ has an upper bound $1-\theta$, so the term corresponding to 1 in $\mathcal{S}(\mathbf{P},\zeta)$ is larger than $\theta$.  

This completes the proof of Proposition 1.  

# C.2 Proof of Theorem 3  

This result is adapted from [20]. For completeness, we outline the proof here.  

Suppose $\zeta = (U_{\omega})_{\omega \in \mathcal{W}}$ with each $U_{\omega}$ is i.i.d. sampled from $U(0,1)$. We split the probability $\mathbb{P}(U_{\omega_t} \leq r)$ into the sum of probabilities of $|\mathcal{W}|$ disjoint events as follows:  

$$
\mathbb { P } ( U _ { \omega _ { t } } \leq r ) = \sum _ { \omega \in \mathcal { W } } \mathbb { P } ( U _ { \omega _ { t } } \leq r , \omega _ { t } = \omega ) = \sum _ { \omega \in \mathcal { W } } \mathbb { P } ( U _ { \omega } \leq r , \omega _ { t } = \omega ) .
$$  

By definition, we have that  

$$
\begin{array} { r l } & { \mathbb { P } \big ( U _ { \omega } \leq r , \omega _ { t } = \omega \big ) = \mathbb { P } \left( U _ { \omega } \leq r , \omega = \arg \operatorname* { m a x } _ { j \in \mathcal { W } } U _ { j } ^ { 1 / P _ { t , j } } \right) } \\ & { \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \
$$  

where the third equality follows from the independence of $U_1, \ldots, U_{|\mathcal{W}|}$.  

This completes the proof of Theorem 3.  

# C.3 Proof of Theorem 4  

The proof of (a) follows [16].

We next prove (b). It suffices to consider the following optimization problem,  

$$
\operatorname* { i n f } _ { h } \operatorname* { i n f } _ { \theta _ { 1 } , \theta _ { 2 } > 0 } \frac { \theta _ { 2 } } { \theta _ { 1 } + \theta _ { 2 } } \log \mathbb { E } _ { 0 } \exp { ( \theta _ { 1 } h ( Y ) ) } + \frac { \theta _ { 1 } } { \theta _ { 1 } + \theta _ { 2 } } \log \mathbb { E } _ { 1 , \mathbf { P } } \exp { ( - \theta _ { 2 } h ( Y ) ) } .
$$  

Suppose $(\alpha^*, p^*)$ is the solution to the minimax problem  

$$
\operatorname* { i n f } _ { \alpha > 0 } \operatorname* { s u p } _ { p \in \mathcal { P } } \int p _ { 0 } ^ { \frac { k } { 1 + k } } ( y ) p ^ { \frac { 1 } { 1 + k } } ( y ) d y .
$$  

By the Holder inequality, we have that, for a distribution class $\mathcal{P}$, the optimal score function of (11) is  

$$
h ( y ) = \frac { 1 } { 1 + \alpha ^ { * } } \log \frac { p ^ { * } ( y ) } { p _ { 0 } ( y ) } .
$$  

For this score function, the optimal threshold is  

$$
\gamma = \frac { 1 } { \theta _ { 1 } + \theta _ { 2 } } \ln \frac { \theta _ { 1 } \exp n \cdot \log \mathbb { E } _ { 0 } \exp \left( \theta _ { 1 } h ( Y ) \right) } { \theta _ { 2 } c \exp n \cdot \log \mathbb { E } _ { 1 , \mathbf { P } } \exp \left( - \theta _ { 2 } h ( Y ) \right) } .
$$  

Note that this score function and the threshold value are equivalent to the ones in (b), as they only differ by a constant $1/(1 + \alpha^*)$. This completes the proof of Theorem 4. $\square$  

# C.4 Proof of Theorem 5  

We first prove (a) under two scenarios, one is when $1/(1-\Delta)$ is an integer, and the other is when $\Delta$ is a function of $n$.  

Case 1.1: Fixed type I error, and $1/(1-\Delta)$ is an integer. 
Our focus is to derive  

$$
\operatorname* { s u p } _ { \mathbf { P } _ { 1 : n } } \operatorname* { i n f } _ { \psi } \operatorname* { l i m } _ { n \to \infty } ( e _ { 2 } ) ^ { \frac { 1 } { n } } ,
$$  

where $\psi$ is chosen from all tests satisfying that the type I error is smaller than or equal to the pre-specified significance level $\alpha$. We first note that  

$$
\operatorname* { s u p } _ { \mathbf { P } _ { 1 : n } } \operatorname* { i n f } _ { \psi } \operatorname* { l i m } _ { n \to \infty } ( e _ { 2 } ) ^ { \frac { 1 } { n } } \geq \operatorname* { i n f } _ { \psi } \operatorname* { s u p } _ { \mathbf { P } _ { 1 : n } } ( e _ { 2 } ) ^ { \frac { 1 } { n } } \geq \operatorname* { l i m } _ { n \to \infty } \operatorname* { i n f } _ { \psi } \operatorname* { s u p } ( e _ { 2 } ) ^ { \frac { 1 } { n } } .
$$  

As such, we turn to the problem,  

$$
A : = \operatorname* { l i m } _ { n \to \infty } \operatorname* { i n f } _ { \psi _ { \mathrm { ~ \bf ~ P _ { 1 : n } ~ } } } ( e _ { 2 } ) ^ { \frac { 1 } { n } } .
$$  

Recall that each $\mathbf{P}_{i}$ is a multinomial distribution. To make the detection method more powerful, we require that each term $p_{ij}$ of $\mathbf{P}_{i}$ is no larger than $1 - \Delta$.  

There are $(\frac{m}{1-\Delta})$ permutations of $(1-\Delta,1-\Delta,\cdots,1-\Delta,0,\ldots,0)$. Denote these multinomial distributions as $\mathbf{R}_i$, $i=1,2,\cdots,(\frac{m}{1-\Delta})$. Let each $\mathbf{P}_j^0$ be a mixture of $\mathbf{R}_i$, i.e., each $\mathbf{P}_j^0$ has the probability $1/(\frac{m}{1-\Delta})$ to be the distribution $\mathbf{R}_i$, for $i=1,\ldots,(\frac{m}{1-\Delta})$. Then we have that,  

$$
A \geq B : = \operatorname* { l i m } _ { n \to \infty } \operatorname* { i n f } _ { \psi } \mathbb { P } _ { \mathcal { H } _ { 1 } , \mathbf { P } _ { 1 : n } ^ { 0 } } \left( \psi \big ( \omega _ { 1 : n } , \zeta _ { 1 : n } \big ) = 0 \right) ^ { \frac { 1 } { n } } .
$$  

We next calculate type I and type II errors under the distribution $\mathbf{P}_{1:n}^{0}$, where each $\mathbf{P}_{j}^{0}$ has the equal probability $1/\left(\frac{\frac{m}{1}}{1-\Delta}\right)$ to be the distribution $\mathbf{R}_{i}, i=1, \ldots, \left(\frac{\frac{m}{1}}{1-\Delta}\right)$.  

We note that, under $\mathcal{H}_0$, $\omega_t$ is independent of $\zeta_t$ for all $t$. Meanwhile, under $\mathcal{H}_1$, the distribution of $\omega_t$ follows the Gumbel-max watermark rule. Formally, the hypotheses are:  

$$
\begin{array} { r l } { \mathcal { H } _ { 0 } : } & { { } \omega _ { t } \perp \zeta _ { t } , \qquad \mathcal { H } _ { 1 } : } & { \omega _ { t } = \mathcal { S } ( \mathbf { P } _ { t } ^ { 0 } , \zeta _ { t } ) , \qquad t = 1 , \ldots , n , } \end{array}
$$  

where the decoder function $\mathcal{S}(\mathbf{P}_{t}^{0},\zeta_{t})=\arg\max_{i}U_{ti}^{\frac{1}{2i}}$, $U_{ti}$'s are the $U(0,1)$ random variables generated in the step $t$ in the Gumbel-max watermarking scheme.  

We now derive the optimal rejection region. By the Neyman-Pearson lemma, we only need to calculate the likelihood ratio function. Under $\mathcal{H}_0$, $\omega_t \sim \mathbf{P}_t^0$, $\zeta_t = (U_{t1}, U_{t2}, \cdots, U_{tm})$, where $U_{ti} \sim U(0,1)$ and is independent from each other. We view $\omega_t$ as a random variable that takes values uniformly from $1, \ldots, m$. Therefore, the likelihood of $(\omega_t, \zeta_t)$ is $1/m \cdot 1 = 1/m$, and the likelihood of $(\omega_{1:n}, \zeta_{1:n})$ is $(1/m)^n$. Under $\mathcal{H}_1$, the relationship between $\omega_t$ and $\zeta_t$ is $\omega_t = \arg\max_i U_{t^i}^{1/0}$. We consider each case of $\mathbf{R}_i$ separately, then combine all cases to compute the likelihood.  

The likelihood of $(\omega_t, \zeta_t)$ is $\frac{1}{\left(\frac{m}{1-\Delta}\right)} \cdot \sum_{S \subset \{1,2,\cdots,m\}, |S|=\frac{1}{1-\Delta}} \mathbf{1} \{U_{t,\omega_i}$ is the largest one among $U_{t,i|i \in S}\}$.

Here, $U_{t,i|i \in S}$ denotes $U_{ti}$ when $i \in S$. As a result, the likelihood of $(\omega_{1:n}, \zeta_{1:n})$ is  

$$
\prod _ { t = 1 } ^ { n } \left( \frac { 1 } { \binom { \frac { 1 } { m } } { \frac { 1 } { 1 - \Delta } } } \cdot \sum _ { S \subset \{ 1 , 2 , \cdots , m \} , | S | = \frac { 1 } { 1 - \Delta } } \mathbf { 1 } \{ U _ { t , i | i \in S } \} \right) .
$$  

Therefore, by the Neyman-Pearson lemma, the optimal rejection region can be written as  

$$
\begin{array} { r l } & { R = \Bigg \{ ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) : \left( \eta ( 1 - \Delta ) \cdot \left( \frac { m - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) \right) ^ { n } } \\ & { \qquad \leq \prod _ { t = 1 } ^ { n } \left( \sum _ { S \subset \{ 1 , 2 , \cdots , m \} , | S | = \frac { 1 } { 1 - \Delta } } \mathbf { 1 } \{ U _ { t , i | i \in S } \} \right) \Bigg \} . \quad ( 1 3 ) } \end{array}
$$  

where $\eta$ is a constant determined by $\alpha$.  

Denote the right-hand-side of (13) as $\prod_{t=1}^{n} X_{t}$, i.e.,  

$X_{t}:=\sum_{S\subset\{1,2,\cdots,m\},|S|=\frac{1}{1-\Delta}}1\{U_{t,\omega_{t}}$ is the largest one among $U_{t,i|i\in S}\}.$

$\sum_{S\subset\{1,2,\cdots,m\},|S|=1}1\{U_{t,\omega_{t}}\}$  

We have $X_{1}, X_{2}, \cdots, X_{n}$ are i.i.d. We next derive the distribution of $X_{t}$ under $\mathcal{H}_{0}$ and $\mathcal{H}_{1}$. As $U_{ti}$'s are i.i.d., to simplify the notation, we omit $t$ in $U_{ti}$ when we compute the likelihood below.  

Under $\mathcal{H}_0$, the rank of $U_{\omega_t}$ distributes uniformly on $\{1,2,\cdots,m\}$. If its rank is smaller than $\frac{1}{1-\Delta}$, then $X_t=0$. If its rank is $k\geq\frac{1}{1-\Delta}$, then $X_t=(\frac{k-1}{1-\Delta}-1)$. So under $\mathcal{H}_0$, the distribution of $X_t$ is  

$$
\begin{array} { r } { \mathbb { P } ( X _ { t } = 0 ) = \frac { \frac { 1 } { 1 - \Delta } - 1 } { m } } \\ { \mathbb { P } ( X _ { t } = \left( \frac { k - 1 } { \frac { 1 } { 1 - \Delta } } - 1 \right) ) = \frac { 1 } { m } , k = \frac { 1 } { 1 - \Delta } , \cdots , m . } \end{array}
$$  

Under $\mathcal{H}_1$, the prior condition is equivalent to that there exists $\frac{1}{1-\Delta}-1$ times of $U_t$ that must be smaller than $U_{\omega_t}$. Therefore, the distribution of $X_t$ is related to the distribution of $r(U_1)|U_1 \geq U_2, \cdots, U_{\frac{1}{1-\Delta}}$, where $r(U_1)$ is the rank of $U_t$ among $U_1, \cdots, U_m$. For $k \geq \frac{1}{1-\Delta}$,  

$$
\mathbb { P } ( r ( U _ { 1 } ) = k | U _ { 1 } \geq U _ { 2 } , \cdots , U _ { \frac { 1 } { 1 - \Delta } } ) = \mathbb { P } ( r ( U _ { 1 } ) = k , U _ { 1 } \geq U _ { 2 } , \cdots , U _ { \frac { 1 } { 1 - \Delta } } ) / \mathbb { P } ( U _ { 1 } \geq U _ { 2 } , \cdots , U _ { \frac { 1 } { 1 - \Delta } } )
$$  

We have that,  

$$
\begin{array} { r l r } & { } & { \mathbb { P } ( r ( U _ { 1 } ) = k , U _ { 1 } \geq U _ { 2 } , \cdots , U _ { \frac { 1 } { 1 - \Delta } } ) = \left( \frac { k - 1 } { \frac { 1 } { 1 - \Delta } } - 1 \right) / \left( \frac { m } { \frac { 1 } { 1 - \Delta } } \right) ; } \\ & { } & { \mathbb { P } ( U _ { 1 } \geq U _ { 2 } , \cdots , U _ { \frac { 1 } { 1 - \Delta } } ) = \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \mathbb { P } ( r ( U _ { 1 } ) = k , U _ { 1 } \geq U _ { 2 } , \cdots , U _ { \frac { 1 } { 1 - \Delta } - 1 } ) } \\ & { } & { = \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \left( \frac { k - 1 } { \frac { 1 } { 1 - \Delta } } - 1 \right) / \left( \frac { m } { \frac { 1 } { 1 - \Delta } } \right) . } \end{array}
$$  

As a result,  

$$
\begin{array} { r } { \mathbb { P } ( X _ { t } = \left( \frac { k - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) ) = \mathbb { P } ( r ( U _ { 1 } ) = k | U _ { 1 } \geq U _ { 2 } , \cdots , U _ { \frac { 1 } { 1 - \Delta } } ) = \frac { \binom { k - 1 } { \frac { 1 } { 1 - \Delta } - 1 } } { \sum _ { i = \frac { 1 } { 1 - \Delta } } ^ { m } \binom { i - 1 } { \frac { 1 } { 1 - \Delta } - 1 } } , k = \frac { 1 } { 1 - \Delta } , \cdots , m . } \end{array}
$$  

So under $\mathcal{H}_1$, the distribution of $X_t$ is  

$$
\mathbb { P } \big ( X _ { t } = \Bigg ( \frac { k - 1 } { 1 - \Delta } - 1 \Bigg ) \bigg ) = \frac { \bigg ( \frac { k - 1 } { 1 - \Delta } - 1 \bigg ) } { \sum _ { i = \frac { 1 } { 1 - \Delta } } ^ { m } \bigg ( \frac { i - 1 } { 1 - \Delta } - 1 \bigg ) } , k = \frac { 1 } { 1 - \Delta } , \cdots , m .
$$  

Consequently, we can represent the two types of error as  

$$
\begin{array} { r l } & { \mathrm { t y p e ~ I ~ e r r o r } = \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \prod _ { t = 1 } ^ { n } X _ { t } \geq \left\{ \eta ( 1 - \Delta ) \cdot \left( \frac { m - 1 } { 1 - \Delta } - 1 \right) \right\} ^ { n } \right) ; } \\ & { \mathrm { t y p e ~ I I ~ e r r o r } = \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( \prod _ { t = 1 } ^ { n } X _ { t } < \left\{ \eta ( 1 - \Delta ) \cdot \left( \frac { m - 1 } { 1 - \Delta } - 1 \right) \right\} ^ { n } \right) . } \end{array}
$$  

Note that  

$$
\begin{array} { r l } & { \alpha \geq \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \sum _ { i = 1 } ^ { n } \ln X _ { t } \geq n \ln \{ \eta ( 1 - \Delta ) \} + n \ln \left( \frac { m - 1 } { 1 - \Delta } - 1 \right) \right) } \\ & { = \left( 1 - \frac { 1 } { 1 - \Delta } - 1 \right) ^ { n } \mathbb { P } \left( \sum _ { i = 1 } ^ { n } \ln Y _ { t } \geq n \ln \{ \eta ( 1 - \Delta ) \} + n \ln \left( \frac { m - 1 } { 1 - \Delta } - 1 \right) \right) , } \end{array}
$$  

where $Y_{t}$ obeys the following probability distribution,  

$$
\mathbb { P } \left( Y _ { t } = \left( \frac { k - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) \right) = \frac { 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } , k = \frac { 1 } { 1 - \Delta } , \ldots , m .
$$  

When $m = \Omega(n)$, $\left(1 - \frac{\frac{1}{1-\Delta}-1}{m}\right)^n \to 1$. Then according to the large number rule, we can choose $\hat{\eta}$ satisfying that  

$$
\ln \left( \hat { \eta } ( 1 - \Delta ) \Bigl ( \frac { m - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \Bigr ) \right) = \mathbb { E } \ln Y + \Phi ^ { - 1 } ( 1 - \alpha ) \sqrt { \mathrm { V a r } ( \ln Y ) / n } .
$$  

Next, we derive the approximations for the expectation and the variance of $\ln Y$.  

$$
\begin{array} { r l } & { \mathbb { E } \ln Y = \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \ln \left( \frac { k - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) \cdot \frac { 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } } \\ & { = \frac { 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \ln \frac { ( k - 1 ) ( k - 2 ) \cdots ( k - \frac { 1 } { 1 - \Delta } + 1 ) } { ( \frac { 1 } { 1 - \Delta } - 1 ) ! } } \\ & { = \frac { 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \left\{ \sum _ { i = k - \frac { 1 } { 1 - \Delta } + 1 } ^ { k - 1 } \ln i - \ln ( \frac { 1 } { 1 - \Delta } - 1 ) ! \right\} } \\ & { \leq \frac { 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } \cdot ( \frac { 1 } { 1 - \Delta } - 1 ) \cdot \sum _ { i = 1 } ^ { m - 1 } \ln i - \ln ( \frac { 1 } { 1 - \Delta } - 1 ) ! } \\ & { \leq \frac { 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } \cdot ( \frac { 1 } { 1 - \Delta } - 1 ) \cdot \int _ { 1 } ^ { m } \ln x d x - \ln ( \frac { 1 } { 1 - \Delta } - 1 ) } \end{array}
$$  

$$
{ \begin{array} { r l } & { = ( { \frac { 1 } { 1 - \Delta } } - 1 ) { \frac { m \ln m - m + 1 } { m - { \frac { 1 } { 1 - \Delta } } + 1 } } - \ln ( { \frac { 1 } { 1 - \Delta } } - 1 ) ! } \\ & { = ( { \frac { 1 } { 1 - \Delta } } - 1 ) \ln m + o ( \ln m ) } \\ & { = O ( \ln m ) } \end{array} }
$$  

$$
\begin{array} { r l } & { \mathrm { V a r } ( \ln Y ) = \mathbb { E } ( \ln Y ) ^ { 2 } - ( \mathbb { E } \ln Y ) ^ { 2 } } \\ & { \qquad = \displaystyle \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \left[ \ln \left( \frac { k - 1 } { 1 - \Delta } - 1 \right) \right] ^ { 2 } \cdot \frac { 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } - \left[ \displaystyle \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \ln \left( \frac { k - 1 } { 1 - \Delta } \right) \cdot \frac { 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } \right] ^ { 2 } } \end{array}
$$  

For the second term, using the same method to estimate the lower bound of $\mathbb{E}\ln Y$, we can prove that $\mathbb{E}\ln Y \geq O(\ln m)$. We next derive the approximation to the first term.  

$$
\begin{array} { r l } & { \mathbb { E } ( \ln Y ) ^ { 2 } = \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \left[ \ln \left( \frac { k - 1 } { 1 - \Delta } - 1 \right) \right] ^ { 2 } \cdot \frac { 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } } \\ & { \quad = \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \left[ \ln ( k - 1 ) + \ln ( k - 2 ) + \cdots + \ln ( k - \frac { 1 } { 1 - \Delta } + 1 ) - \ln ( \frac { 1 } { 1 - \Delta } - 1 ) ! \right] ^ { 2 } \cdot \frac { 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } } \\ & { \quad = \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \frac { [ \ln ( k - 1 ) + \ln ( k - 2 ) + \cdots + \ln ( k - \frac { 1 } { 1 - \Delta } + 1 ) ] ^ { 2 } } { m - \frac { 1 } { 1 - \Delta } + 1 } } \\ & { \quad - 2 \ln ( \frac { 1 } { 1 - \Delta } - 1 ) ! \cdot \frac { \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } [ \ln ( k - 1 ) + \ln ( k - 2 ) + \cdots + \ln ( k - \frac { 1 } { 1 - \Delta } + 1 ) ] } { m - \frac { 1 } { 1 - \Delta } + 1 } + \left[ \ln ( \frac { 1 } { 1 - \Delta } - 1 ) ! \right. } \\ & { \quad \leq \frac { \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } [ ( \frac { 1 } { 1 - \Delta } - 1 ) \ln ( k - 1 ) ] ^ { 2 } } { m - \frac { 1 } { 1 - \Delta } + 1 } } \\ & { \quad \leq \frac { ( \frac { 1 } { 1 - \Delta } - 1 ) ^ { 2 } \int _ { 0 } ^ { m } \ln ^ { 2 } x d x } { m - \frac { 1 } { 1 - \Delta } + 1 } } \\ & { = O ( \ln ^ { 2 } m ) } \end{array}
$$  

Henceforth, we have derived the result that  

$$
\mathbb { E } \ln Y = O ( \ln m ) , \mathrm { V a r } ( \ln Y ) = O ( \ln ^ { 2 } m ) .
$$  

When $m = o(\exp(\sqrt{n}))$, we have $\Phi^{-1}(1 - \alpha)\sqrt{n \cdot \text{Var}(\ln Y)} = o(n \cdot \mathbb{E} \ln Y)$. Combining this result and lemma 1, we have  

$$
B = \operatorname* { i n f } _ { s \geq 0 } \exp \{ - s \mathbb { E } \ln Y \} \mathbb { E } \exp \{ s \ln X _ { t } \}
$$  

$$
\begin{array} { r l } & { = \operatorname* { i n f } _ { s \geq 0 } \frac { \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \left( \frac { k - 1 } { 1 - \Delta } - 1 \right) ^ { s } \cdot \left( \frac { k - 1 } { 1 - \Delta } - 1 \right) / \sum _ { i = \frac { 1 } { 1 - \Delta } } ^ { m } \left( \frac { i - 1 } { 1 - \Delta } - 1 \right) } { e ^ { s \mathbb { E } \ln Y } } } \\ & { \geq \operatorname* { i n f } _ { s \geq 0 } \frac { \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \left( \frac { k - 1 } { 1 - \Delta } - 1 \right) ^ { s + 1 } } { \sum _ { i = \frac { 1 } { 1 - \Delta } } ^ { m } \left( \frac { i - 1 } { 1 - \Delta } - 1 \right) \cdot e ^ { s \cdot \left( \frac { ( \frac { 1 } { 1 - \Delta } - 1 ) \cdot ( m \ln m - m + 1 ) } { m - \frac { 1 } { 1 - \Delta } + 1 } - \ln ( \frac { 1 } { 1 - \Delta } - 1 ) ! \right) } } } \\ & { = \operatorname* { i n f } _ { s \geq 0 } \frac { \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \left( \frac { i - 1 } { 1 - \Delta } - 1 \right) \cdot e ^ { s \cdot \left( \frac { 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } - \ln ( \frac { 1 } { 1 - \Delta } - 1 ) ! \right) } } } { \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \left( k - 1 \right) \left( k - 2 \right) \cdots \left( k - \frac { 1 } { 1 - \Delta } + 1 \right) \cdot \exp \{ \frac { m \ln m - m + 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } \cdot \left( \frac { 1 } { 1 - \Delta } - 1 \right) \cdot s \} } } \\ & { \geq \operatorname* { i n f } _ { s \geq 0 } \frac { \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \left( \left( k - \frac { 1 } { 1 - \Delta } + 1 \right) / m \right) ^ { ( s + 1 ) \left( \frac { 1 } { 1 - \Delta } - 1 \right) } \cdot m ^ { - s \left( \frac { 1 } { 1 - \Delta } - 1 \right) } } { \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \left( \left( k - 1 \right) / m \right) ^ { \frac { 1 } { 1 - \Delta } - 1 } \cdot \exp \{ \frac { m \ln m - m + 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } \cdot \left( \frac { 1 } { 1 - \Delta } - 1 \right) \cdot s \} } } \\ & { \geq \operatorname* { i n f } _ { s \geq 0 } \frac { \int _ { 0 } ^ { \frac { m - \frac { 1 } { 1 - \Delta } } { m } } x ^ { ( s + 1 ) \left( \frac { 1 } { 1 - \Delta } - 1 \right) } d x \cdot m ^ { s - \frac { s } { 1 - \Delta } } } { x ^ { ( s + 1 ) \left( \frac { 1 } { 1 - \Delta } - 1 \right) } d x \cdot m ^ { s - \frac { s } { 1 - \Delta } } } } \\ & { = \operatorname* { i n f } _ { s \geq 0 } \frac { 1 } { 1 - \Delta s } \left( \frac { m - \frac { 1 } { 1 - \Delta } } { m } \right) ^ { s \cdot \left( \frac { 1 } { 1 - \Delta } - 1 \right) } \frac { m ^ { s \cdot \left( 1 - \frac { 1 } { 1 - \Delta } \right) } } { \exp \{ \frac { m \ln m - m + 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } \cdot \left( \frac { 1 } { 1 - \Delta } - 1 \right) \cdot s \} } } \\ & { = \operatorname* { i n f } _ { s \geq 0 } \frac { 1 } { 1 - \Delta s } \left( \frac { m - \frac { 1 } { 1 - \Delta } } { m } \right) ^ { s \cdot \left( \frac { 1 } { 1 - \Delta } - 1 \right) } \exp \left\{ \left( \frac { 1 } { 1 - \Delta } - 1 \right) \cdot s \cdot \left( \ln m - \frac { m \ln m - m + 1 } { m - \frac { 1 } { 1 - \Delta } + 1 } \right) \right\} } \end{array}
$$  

When $m \rightarrow \infty$, the above expression tends to $\inf_{s \geq 0} \frac{1}{e^{\frac{s\Delta}{1-\Delta} \cdot (1-\Delta s)}}$.  

Next we compute the asymptotic type II error of our test. By Theorem 1, we have  

$$
\begin{array} { r l } & { \operatorname* { l i m } _ { n \to \infty } \operatorname* { s u p } _ { \mathbf { P } _ { 1 : n } } ( \mathbb { P } _ { \mathcal { H } _ { 1 } } ( T _ { h } ( Y _ { 1 : n } ) = 0 ) ) ^ { \frac { 1 } { n } } = e ^ { - R \mathcal { P } ( h ) } } \\ & { = \exp \operatorname* { i n f } _ { s \geq 0 } \operatorname* { s u p } _ { \mathbf { P } \in \mathcal { P } } \{ t \mathbb { E } _ { 0 } h ( Y ) + \log \phi _ { \mathbf { P } , h } ( s ) \} } \\ & { = \exp \operatorname* { i n f } _ { s \geq 0 } \left\{ s \cdot ( \ln \frac { 1 } { 1 - \Delta } - \frac { \Delta } { 1 - \Delta } ) + \ln \left[ ( \frac { 1 } { 1 - \Delta } ) ^ { 1 - s } \cdot \frac { 1 } { \frac { \Delta } { 1 - \Delta } \cdot ( 1 + s ) + 1 } \right] \right\} } \\ & { = \operatorname* { i n f } _ { s \geq 0 } \exp \left\{ \ln \frac { 1 } { 1 - \Delta } - s \cdot \frac { \Delta } { 1 - \Delta } - \ln \left( \frac { 1 } { 1 - \Delta } + \frac { \Delta } { 1 - \Delta } \cdot s \right) \right\} } \\ & { = \operatorname* { i n f } _ { s \geq 0 } \frac { 1 } { e ^ { \frac { s \Delta } { 1 - \Delta } } \cdot ( 1 - \Delta s ) } } \end{array}
$$  

which is exactly the lower bound of $B$ as defined in (12).  

Case 1.2: Fixed type I error, and $\Delta$ is a function of $n$. As $n$ increases, $1 - \Delta$ tends to 1. We first calculate the asymptotic type II error.  

$$
\operatorname* { l i m } _ { n \to \infty } \mathbb { P } _ { \phi _ { 0 } , \mathcal { H } _ { 1 } } ( R ^ { c } ) = \exp \{ - R _ { \mathcal { P } } ( h ) \}
$$  

$$
R _ { \mathcal { P } } ( h ) = - \operatorname* { i n f } _ { h } \operatorname* { s u p } _ { \mathbf { P } \in \mathcal { P } } \mathbb { E } _ { 0 } h ( Y ) + \log \mathbb { E } _ { 1 , \mathbf { P } } e ^ { - h ( Y ) } = - \int _ { 0 } ^ { 1 } \log ( y ^ { a } + y ^ { \frac { 1 } { a } } ) d y ,
$$  

where $a=\frac{1-\Delta}{\Delta}$.  

As a direct consequence of Neyman-Pearson lemma, under the condition that each $\mathbf{P}_j$ has the equal probability $1/m(m-1)$ to be the distribution $(\Delta, 1 - \Delta, 0, 0, \cdots, 0)$ and its permutation, the best rejection region can be represented as  

$$
R = \left\{ ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) : ( \eta \cdot ( m - 1 ) ) ^ { n } \leq \prod _ { t = 1 } ^ { n } \left( \sum _ { j = 1 } ^ { m - 1 } ( 1 \{ U _ { i , m } \geq U _ { i , j } ^ { a } \} + 1 \{ U _ { i , m } \geq U _ { i , j } ^ { \frac { 1 } { a } } \} ) \right) \right\} ,
$$  

where $\eta$ is determined by the pre-specified constant $\alpha$, and $U_{i,j}$'s are i.i.d. random variables, $U_{i,j} \sim U(0,1), i=1,2,\cdots,n,j=1,2,\ldots,m$.  

Denote the right-hand-side of (14) as $\prod_{t=1}^{n} X_{t}$. As we can see, $X_{1}, X_{2}, \cdots, X_{n}$ are i.i.d. We compute the following probabilities,  

$$
\begin{array} { r l } & { \mathbb { P } \left( \displaystyle \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { a } \} = y , \displaystyle \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { \frac { 1 } { a } } \} = x | U _ { m } \geq U _ { 1 } ^ { \frac { 1 } { a } } \right) } \\ & { = ( a + 1 ) \int _ { 0 } ^ { 1 } \binom { m - 2 } { x - 1 } \cdot \binom { m - x - 1 } { y - x } ( r ^ { a } ) ^ { x } \cdot ( r ^ { \frac { 1 } { a } } - r ^ { a } ) ^ { y - x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - y - 1 } d r , y \geq x \geq 1 } \\ & { \mathbb { P } \left( \displaystyle \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { a } \} = y , \displaystyle \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { \frac { 1 } { a } } \} = x | U _ { m } \geq U _ { 1 } ^ { a } \right) } \\ & { = \frac { a + 1 } { a } \int _ { 0 } ^ { 1 } \binom { m - 2 } { x - 1 } ( r ^ { a } ) ^ { x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - y - 1 } d r , y = x \geq 1 } \\ & { \mathbb { P } \left( \displaystyle \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { a } \} = y , \displaystyle \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { \frac { 1 } { a } } \} = x | U _ { m } \geq U _ { 1 } ^ { a } \right) } \\ & { = \frac { a + 1 } { a } \int _ { 0 } ^ { 1 } \binom { m - 2 } { x } \cdot \binom { m - x - 2 } { y - x - 1 } ( r ^ { a } ) ^ { x } \cdot ( r ^ { \frac { 1 } { a } } - r ^ { a } ) ^ { y - x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - y - 1 } d r , y > x . } \end{array}
$$  

Henceforth, under $\mathcal{H}_0$, the distribution of $X_t$ is  

$$
\begin{array} { r l } & { \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { a } \} = y , \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { \frac { 1 } { a } } \} = x \right) } \\ & { = \int _ { 0 } ^ { 1 } \left( m - 1 \right) \left( m - 1 - x \right) ( r ^ { a } ) ^ { x } \cdot ( r ^ { \frac { 1 } { a } } - r ^ { a } ) ^ { y - x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - 1 - y } d r , y \geq x . } \end{array}
$$  

Under $\mathcal{H}_{1}$, the distribution of $X_{t}$ is  

$$
\mathbb { P } _ { \mathcal { H } _ { 1 } } \left( \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { a } \} = y , \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { \frac { 1 } { a } } \} = x \right)
$$  

$$
\begin{array} { r l } & { = \frac { 1 } { 2 } \mathbb { P } \big ( \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { a } \} = y , \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { \frac { 1 } { a } } \} = x | U _ { m } \geq U _ { 1 } ^ { a } ) } \\ & { \quad + \frac { 1 } { 2 } \mathbb { P } \big ( \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { a } \} = y , \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { \frac { 1 } { a } } \} = x | U _ { m } \geq U _ { 1 } ^ { \frac { 1 } { a } } \big ) . } \end{array}
$$  

We then calculate the type II error of this case as,  

$$
\begin{array} { r l } & { \operatorname* { l i m } _ { n \to \infty } \mathbb { P } _ { \mathcal { H } _ { 1 } } ^ { \frac { 1 } { n } } ( R ^ { c } ) = \operatorname* { l i m } _ { n \to \infty } \mathbb { P } _ { \mathcal { H } _ { 1 } } ^ { \frac { 1 } { n } } \left( \prod _ { t = 1 } ^ { n } X _ { t } < ( \eta \cdot ( m - 1 ) ) ^ { n } \right) } \\ & { \quad = \operatorname* { l i m } _ { n \to \infty } \mathbb { P } _ { \mathcal { H } _ { 1 } } ^ { \frac { 1 } { n } } \left( \sum _ { i = 1 } ^ { n } \ln X _ { t } < n \cdot \ln ( \eta ( m - 1 ) ) \right) } \\ & { \quad = \operatorname* { i n f } _ { t } \frac { \mathbb { E } _ { \mathcal { H } _ { 1 } } X _ { i } ^ { t } } { [ \eta ( m - 1 ) ] ^ { t } } . } \end{array}
$$  

Similar to Case 1.1, we have $\ln(\eta(m-1))=\mathbb{E}\ln Y+o(\mathbb{E}\ln Y)$, where $Y$ is the random variable that has the identical distribution as $X_t$ under $\mathcal{H}_0$. Since $a(n)=\frac{1}{\Delta(n)}-1$ and $\frac{1}{\Delta(n)}=\Omega(m)$, we have $a(n)\gg m\gg0$. Now we derive the asymptotic type $\Pi$ error under the condition that $a(n)\gg m\gg0$.  

$$
\begin{array} { r l } & { \mathbb { E } _ { \mathcal { H } _ { 1 } } X _ { t } ^ { s } = \Gamma _ { 1 } + \Gamma _ { 2 } + \Gamma _ { 3 } } \\ & { : = \frac { 1 } { 2 } \sum _ { x = 1 } ^ { m - 1 } \sum _ { y = x } ^ { m - 1 } ( a + 1 ) \int _ { 0 } ^ { 1 } \binom { m - 2 } { x - 1 } \binom { m - x - 1 } { y - x } \cdot ( r ^ { a } ) ^ { x } ( r ^ { \frac { 1 } { a } } - r ^ { a } ) ^ { y - x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - y - 1 } d r \cdot ( x + y ) ^ { s } } \\ & { \quad + \frac { 1 } { 2 } \sum _ { x = 1 } ^ { m - 1 } \frac { a + 1 } { a } \int _ { 0 } ^ { 1 } \binom { m - 2 } { x - 1 } ( r ^ { a } ) ^ { x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - x - 1 } d r \cdot ( 2 x ) ^ { s } } \\ & { \quad + \frac { 1 } { 2 } \sum _ { x = 0 } ^ { m - 1 } \sum _ { y = x + 1 } ^ { m - 1 } \frac { a + 1 } { a } \int _ { 0 } ^ { 1 } \binom { m - 2 } { x } \binom { m - x - 2 } { y - x - 1 } \cdot ( r ^ { a } ) ^ { x } ( r ^ { \frac { 1 } { a } } - r ^ { a } ) ^ { y - x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - y - 1 } d r \cdot ( x + y ) ^ { s } } \end{array}
$$  

$$
\begin{array} { r l } & { \mathbb { E } \ln Y = \frac { 1 } { 1 - \frac { 1 } { \binom { m + a - 1 } { a } } } ( \sum _ { y = 1 } ^ { m - 1 } \int _ { 0 } ^ { 1 } \binom { m - 1 } { y } ( r ^ { \frac { 1 } { a } } - r ^ { a } ) ^ { y } ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - 1 - y } d r \cdot \ln y } \\ & { \quad + \sum _ { x = 1 } ^ { m - 1 } \sum _ { y = x } ^ { m - 1 } \int _ { 0 } ^ { 1 } \binom { m - 1 } { x } \binom { m - 1 - x } { y - x } ( r ^ { a } ) ^ { x } ( r ^ { \frac { 1 } { a } } - r ^ { a } ) ^ { y - x } ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - 1 - y } d r \cdot \ln ( x + y ) ) } \end{array}
$$  

Under the condition that $a \gg m \gg 0$, we have $r^{a} \rightarrow 0, r \in (0,1)$ and $r^{\frac{1}{a}} \rightarrow 1, r \in (0,1)$, $\mathbb{E}\ln Y \rightarrow ln(m-1)$, $\Gamma_{1} \rightarrow \frac{1}{2}m^{s}$, $\Gamma_{2} \rightarrow 0$ and $\Gamma_{3} \rightarrow \frac{1}{2}(m-1)^{s}$. Henceforth, we have $\mathbb{E}_{\mathcal{H}_{1}}X_{t}^{s}/(\eta(m-1))^{s} \rightarrow 1$. We also have $R_{\mathcal{P}}(h) \rightarrow 0, e^{-R_{\mathcal{P}}(h)} \rightarrow 1$. This shows that the asymptotic type II error of our proposed test matches the lower bound of the asymptotic type II error of all tests.  

We next prove (b) under two scenarios.  

Case 2.1: Minimizing the sum of type I and type II errors, and $1/1 - \Delta$ is an integer. We first present a supporting lemma.  

Lemma 3. For any two distributions $\mathbf{P}_0$, $\mathbf{P}_1$, we have that,  

$$
\operatorname* { i n f } _ { \mathbf { \Psi } } \frac { 1 } { 2 } [ \mathbb { P } _ { 0 } ( \Psi ( X ) \neq 0 ) + \mathbb { P } _ { 1 } ( \Psi ( X ) \neq 1 ) ] = \frac { 1 } { 2 } ( 1 - T V ( \mathbf { P } _ { 0 } , \mathbf { P } _ { 1 } ) ) .
$$  

In particular, the likelihood ratio function $\Psi = \mathbf{1}_{\{f_0(X) \leq f_1(X)\}}$ can achieve the infimum.  

Proof. For any test $\Psi$, let $A$ denote its acceptance region, i.e., if $X \in A$ then $\Psi(X) = 0$. We then have that,  

$$
\begin{array} { r l r } & { } & { \frac { 1 } { 2 } [ \mathbb { P } _ { 0 } ( \Psi ( X ) \neq 0 ) + \mathbb { P } _ { 1 } ( \Psi ( X ) \neq 1 ) ] = \frac { 1 } { 2 } [ \mathbb { P } _ { 0 } ( X \notin A ) + \mathbb { P } _ { 1 } ( X \in A ) ] } \\ & { } & { = \frac { 1 } { 2 } [ 1 - ( \mathbb { P } _ { 0 } ( X \in A ) - \mathbb { P } _ { 1 } ( X \in A ) ) ] . } \end{array}
$$  

As a result, we have  

$$
\begin{array} { r l } & { \operatorname* { i n f } _ { \Psi } \mathbb { P } ( \Psi ( X ) \geq T ) = \frac { 1 } { 2 } [ 1 - \operatorname* { s u p } _ { A } ( \mathbb { P } _ { 0 } ( X \in A ) - \mathbb { P } _ { 1 } ( X \in A ) ) ] } \\ & { \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad
$$  

We can verify that the infimum can be achieved when choosing $\Psi = \mathbf{1}_{f_0(X) \leq f_1(X)}$. This completes the proof of Lemma 3. $\square$  

We next proceed with the proof. Our focus is to derive  

$$
\operatorname* { i n f } _ { \psi } \operatorname* { l i m } _ { n \to \infty } \operatorname* { s u p } ( e _ { 1 } + e _ { 2 } ) ^ { \frac { 1 } { n } } .
$$  

We first note that  

$$
\operatorname* { i n f } _ { \psi } \operatorname* { l i m } _ { n \to \infty } \operatorname* { s u p } _ { \mathbf { P } _ { 1 : n } } ( e _ { 1 } + e _ { 2 } ) ^ { \frac { 1 } { n } } \geq \operatorname* { l i m } _ { n \to \infty } \operatorname* { i n f } _ { \psi } \operatorname* { s u p } _ { \mathbf { P } _ { 1 : n } } ( e _ { 1 } + e _ { 2 } ) ^ { \frac { 1 } { n } } .
$$  

We can then turn to the following problem,  

$$
A : = \operatorname* { l i m } _ { n \to \infty } [ \operatorname* { i n f } _ { \psi } \operatorname* { s u p } ( \mathrm { t y p e ~ I ~ e r r o r } + \mathrm { t y p e ~ I I ~ e r r o r } ) ] ^ { \frac { 1 } { n } }
$$  

Here we also add some restrictions on the distributions $\mathbf{P}_{1:n}$. Recall that each distribution $\mathbf{P}_i$ is a multinomial distribution. To make the detection method more powerful, we impose that each term $p_{ij}$ of $\mathbf{P}_i$ cannot be larger than $1 - \Delta$.  

There are $(\frac{m}{1-\Delta})$ permutations of distribution $(1-\Delta,1-\Delta,\cdots,1-\Delta,0,0,\cdots,0)$. Denote these multinomial distributions as $\mathbf{R}_i,i=1,2,\cdots,\left(\frac{m}{1-\Delta}\right)$. Let each $\mathbf{P}_j^0$ be a combination of $\mathbf{R}_i,i=1,2,\ldots,\left(\frac{m}{1-\Delta}\right)$, i.e., each $\mathbf{P}_j^0$ has the probability $1/\left(\frac{m}{1-\Delta}\right)$ to be the distribution $\mathbf{R}_i,i=1,2,\cdots,\left(\frac{m}{1-\Delta}\right)$. Then we have,  

$$
A \geq B : = \operatorname* { l i m } _ { n \to \infty } [ \operatorname* { i n f } _ { \psi } ( \mathbb { P } _ { \mathcal { H } _ { 0 } , \mathbf { P } _ { 1 : n } ^ { 0 } } \left( \psi ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) = 1 \right) + \mathbb { P } _ { \mathcal { H } _ { 1 } , \mathbf { P } _ { 1 : n } ^ { 0 } } \left( \psi ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) = 0 \right) ] ^ { \frac { 1 } { n } }
$$  

Here, both of the two types of error are calculated under the distribution $\mathbf{P}_{1:n}^{0}$, each $\mathbf{P}_{j}^{0}$ has the equal probability $1/\left(\frac{m}{1-\Delta}\right)$ to be the distribution $\mathbf{R}_{i}, i=1,2, \cdots,\left(\frac{m}{1-\Delta}\right)$. We then use the distribution to calculate the two types of error.  

Similar to Case 1.1, the likelihood of $(\omega_{1:n},\zeta_{1:n})$ under $\mathcal{H}_0$ is $(\frac{1}{m})^n$, and the likelihood of $(\omega_{1:n},\zeta_{1:n})$ under $\mathcal{H}_1$ is  

$$
\prod _ { t = 1 } ^ { n } \left( 1 / \left( \frac { m } { \frac { 1 } { 1 - \Delta } } \right) \cdot \sum _ { S \subset \{ 1 , 2 , \cdots , m \} , | S | = \frac { 1 } { 1 - \Delta } } \mathbf { 1 } \{ U _ { t , i | i \in S } \} \right) .
$$  

By Lemma 3, the best rejection region can be represented as  

$$
\begin{array} { r l } & { R = \Bigg \{ ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) : \bigg ( ( 1 - \Delta ) \cdot \bigg ( \frac { m - 1 } { \frac { 1 } { 1 - \Delta } } - 1 \bigg ) \bigg ) ^ { n } } \\ & { \quad \leq \prod _ { t = 1 } ^ { n } \Bigg ( \sum _ { S \subset \{ 1 , 2 , \cdots , m \} , | S | = \frac { 1 } { 1 - \Delta } } \mathbf { 1 } \{ U _ { t , \omega _ { t } } \mathrm { ~ i s ~ t h e ~ l a r g e s t ~ o n e ~ a m o n g ~ } U _ { t , i | i \in S } \} \Bigg ) \Bigg \} . } \end{array}
$$  

Denote the right-hand-side of 17 as $\prod_{t=1}^{n} X_{t}$. We have $X_{1}, X_{2}, \cdots, X_{n}$ are i.i.d. We next derive the distribution of $X_{t}$ under $\mathcal{H}_{0}$ and $\mathcal{H}_{1}$. Following Case 1.1, the distribution of $X_{t}$ under $\mathcal{H}_{0}$ and $\mathcal{H}_{1}$ can be represented as,  

$$
\begin{array} { r l } & { \mathbb { P } _ { \mathcal { H } _ { 0 } } ( X _ { t } = 0 ) = \frac { \frac { 1 } { 1 - \Delta } - 1 } { m } ; } \\ & { \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( X _ { t } = \left( \begin{array} { l } { k - 1 } \\ { \frac { 1 } { 1 - \Delta } - 1 } \end{array} \right) \right) = \frac { 1 } { m } , \; k = \frac { 1 } { 1 - \Delta } , \ldots , m . } \\ & { \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( X _ { t } = \left( \begin{array} { l } { k - 1 } \\ { \frac { 1 } { 1 - \Delta } - 1 } \end{array} \right) \right) = \frac { \binom { k - 1 } { 1 - \Delta } - 1 } { \sum _ { i = \frac { 1 } { 1 - \Delta } } ^ { m } \binom { i - 1 } { 1 - \Delta } } , \; k = \frac { 1 } { 1 - \Delta } , \ldots , m . } \end{array}
$$  

Consequently, we can represent the two types of error as,  

$$
\mathrm { t y p e ~ I ~ e r r o r } = \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \prod _ { t = 1 } ^ { n } X _ { t } \geq \left\{ ( 1 - \Delta ) \cdot \left( \frac { m - 1 } { 1 - \Delta } - 1 \right) \right\} ^ { n } \right) ;
$$  

$$
\mathrm { t y p e ~ I I ~ e r r o r } = \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( \prod _ { t = 1 } ^ { n } X _ { t } < \left\{ \left( 1 - \Delta \right) \cdot \left( \frac { m - 1 } { 1 - \Delta } - 1 \right) \right\} ^ { n } \right) .
$$  

By Lemma 1, we have that,  

$$
B \geq \operatorname* { l i m } _ { n \to \infty } ( \mathrm { t y p e ~ I ~ e r r o r } ) ^ { \frac { 1 } { n } } = \operatorname* { i n f } _ { s } \exp \{ - s \ln ( ( 1 - \Delta ) \cdot \left( \frac { m - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) ) \} \cdot \mathbb { E } \exp \{ s \ln X _ { t } \} .
$$  

Furthermore, we have that,  

$$
\begin{array} { r l } & { \frac { \mathbb { E } \big ( X _ { t } ^ { s } \big ) } { \Big ( ( 1 - \Delta ) \cdot \Big ( \frac { m - 1 } { 1 - \Delta } - 1 \Big ) \Big ) ^ { s } } = \frac { \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \frac { 1 } { m } \cdot \big ( \big ( \frac { k - 1 } { 1 - \Delta } - 1 \big ) \big ) ^ { s } } { \Big ( \big ( ( 1 - \Delta ) \cdot \Big ( \frac { m - 1 } { 1 - \Delta } - 1 \big ) \Big ) ^ { s } } } \\ & { \quad = \frac { \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \frac { 1 } { m } \cdot \big [ \frac { ( k - 1 ) ( k - 2 ) \cdots ( k - \frac { 1 } { 1 - \Delta } + 1 ) } { ( \frac { 1 } { 1 - \Delta } - 1 ) ! } \big ] ^ { s } } { \Big ( ( 1 - \Delta ) \cdot \Big ( \frac { m - 1 } { 1 - \Delta } - 1 \Big ) \Big ) ^ { s } } } \\ & { \quad = \frac { \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \frac { 1 } { m } \cdot m ^ { s \cdot ( \frac { 1 } { 1 - \Delta } - 1 ) } \cdot \frac { 1 } { ( \frac { 1 } { 1 - \Delta } - 1 ) ! ^ { s } } \cdot \big ( \frac { k - 1 } { m } \cdot \frac { k - 2 } { m } \cdot \ldots \frac { k - \frac { 1 } { 1 - \Delta } + 1 } { m } \big ) ^ { t } } { \Big ( ( 1 - \Delta ) \cdot \Big ( \frac { m - 1 } { 1 - \Delta } - 1 \Big ) \Big ) ^ { s } } } \\ & { \geq \frac { \frac { 1 } { m } \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \big ( \frac { k - 1 } { m } \cdot \frac { k - 2 } { m } \ldots \frac { k - \frac { 1 } { 1 - \Delta } + 1 } { m } \big ) ^ { s } } { \big ( 1 - \Delta \big ) ^ { s } } } \\ & { \geq \frac { \frac { 1 } { m } \sum _ { k = 1 } ^ { m } \frac { 1 } { 1 - \Delta } + 1 } { \big ( 1 - \Delta \big ) ^ { s } } } \\ & { = \frac { \frac { 1 } { m } \sum _ { k = 1 } ^ { m } \big ( \frac { k } { m } \big ) ^ { t } } { \big ( 1 - \Delta \big ) ^ { s } } - \frac { \frac { 1 } { m } \sum _ { k = m - \frac { 1 } { 1 - \Delta } + 2 } ^ { m } \big ( \frac { k } { m } \big ) ^ { s } } { \big ( 1 - \Delta \big ) ^ { s } } } } \\ & { \geq \frac { \frac { 1 } { m } \sum _ { k = 1 } ^ { m } \big ( \frac { k } { m } \big ) ^ { s } } { \big ( 1 - \Delta \big ) ^ { s } } - \frac { \frac { 1 } { m } \big ( \frac { 1 } { 1 - \Delta } - 1 \big ) } { \big ( 1 - \Delta \big ) ^ { s } } } \\ & { \geq \frac { \int _ { 0 } ^ { 1 } ( r ^ { \frac { 1 } { 1 - \Delta } - 1 } ) ^ { s } d r - \frac { 1 } { m } \big ( \frac { 1 } { 1 - \Delta } - 1 \big ) } { \big ( 1 - \Delta \big ) ^ { s } } } \\ & { = \frac { \int _ { 0 } ^ { 1 } ( r ^ { \frac { 1 } { 1 - \Delta } - 1 } ) ^ { s } d r } { \big ( 1 - \Delta \big ) ^ { s } } - O ( \frac { 1 } { m } ) , } \end{array}
$$  

which is exactly the result of our testing method. By Theorem 4, the asymptotic sum of type I and II errors of our proposed test is $\inf_{s\in(0,1)}\frac{f_0^1(r^{\frac{1}{1-\Delta}-1})^sdr}{(1-\Delta)^s}$. This shows that our test attains the minimal asymptotic sum of the two types of errors over all tests.  

Case 2.2: Minimizing the sum of type I and type II errors, and $\Delta$ is a function of $n$. Our focus is to derive  

$$
\operatorname* { i n f } _ { \psi } \operatorname* { l i m } _ { n \to \infty } \operatorname* { s u p } _ { \mathbf { P } _ { 1 : n } } ( e _ { 1 } + e _ { 2 } ) ^ { \frac { 1 } { n } } .
$$  

We first note that  

$$
\operatorname* { i n f } _ { \psi } \operatorname* { l i m } _ { n \to \infty } \operatorname* { s u p } _ { \mathbf { P } _ { 1 : n } } ( e _ { 1 } + e _ { 2 } ) ^ { \frac { 1 } { n } } \geq \operatorname* { l i m } _ { n \to \infty } \operatorname* { i n f } _ { \psi } \operatorname* { s u p } _ { \mathbf { P } _ { 1 : n } } ( e _ { 1 } + e _ { 2 } ) ^ { \frac { 1 } { n } } .
$$  

We can then turn to the following problem:  

$$
A : = \operatorname* { l i m } _ { n \to \infty } [ \operatorname* { i n f } _ { \psi } \operatorname* { s u p } ( \mathrm { t y p e ~ I ~ e r r o r } + \mathrm { t y p e ~ I I ~ e r r o r } ) ] ^ { \frac { 1 } { n } }
$$  

We first calculate the asymptotic sum of two types of errors of our test.  

$\limsup_{n\to\infty}\mathbb{P}_{\phi_0,\mathcal{H}_1}(R^c)=\exp\{-S_{\mathcal{P}}(h)\}$  

$$
\begin{array} { r l } & { S _ { \mathcal { P } } ( h ) = - \operatorname* { i n f } _ { h } \operatorname* { s u p } _ { \mathbf { \mathbb { E } } _ { 0 } h ( Y ) } - \log \mathbb { E } _ { 1 , \mathbf { P } } e ^ { - h ( Y ) } } \\ & { \quad = - \operatorname* { i n f } _ { h } \operatorname* { i n f } _ { \theta _ { 1 } , \theta _ { 2 } > 0 } \frac { \theta _ { 2 } } { \theta _ { 1 } + \theta _ { 2 } } \log \mathbb { E } _ { 0 } \exp \left\{ \theta _ { 1 } h ( Y ) \right\} - \frac { \theta _ { 1 } } { \theta _ { 1 } + \theta _ { 2 } } \log \operatorname* { s u p } _ { \mathbf { \mathbb { P } } \in \mathcal { P } } \mathbb { E } _ { 1 , \mathbf { P } } \exp \left\{ - \theta _ { 2 } h ( Y ) \right\} } \\ & { \quad = - \operatorname* { i n f } _ { k \in ( 0 , 1 ) } \log \int _ { 0 } ^ { 1 } p _ { 0 } ^ { k } ( y ) p _ { 1 } ^ { 1 - k } ( y ) d y } \\ & { \quad = - \operatorname* { i n f } _ { k \in ( 0 , 1 ) } \log \int _ { 0 } ^ { 1 } ( y ^ { \frac { \Delta } { 1 - \Delta } } + y ^ { \frac { 1 - \Delta } { \Delta } } ) ^ { k } d y } \end{array}
$$  

By Lemma 3, the best rejection region can be represented as,  

$$
R = \left\{ ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) : ( ( m - 1 ) ) ^ { n } \leq \prod _ { t = 1 } ^ { n } \left( \sum _ { j = 1 } ^ { m - 1 } \left( \mathbf { 1 } \{ U _ { i , m } \geq U _ { i , j } ^ { a } \} + \mathbf { 1 } \{ U _ { i , m } \geq U _ { i , j } ^ { \frac { 1 } { a } } \} \right) \right) \right\} .
$$  

Denote the right-hand-side of 19 as $\prod_{t=1}^{n} X_{t}$. We see that $X_{1}, X_{2}, \cdots, X_{n}$ are i.i.d. Similar as in Case 1.2, we first obtain that the distribution of $X_{t}$ under $H_{0}$ is,  

$$
\begin{array} { r l } & { \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { a } \} = y , \, \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { \frac { 1 } { a } } \} = x \right) } \\ & { = \int _ { 0 } ^ { 1 } \left( \frac { m - 1 } { x } \right) \left( \frac { m - 1 - x } { y - x } \right) ( r ^ { a } ) ^ { x } \cdot ( r ^ { \frac { 1 } { a } } - r ^ { a } ) ^ { y - x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - 1 - y } d r , \, \, y \geq x . } \end{array}
$$  

The distribution of $X_{t}$ under $\mathcal{H}_{1}$ is,  

$$
\begin{array} { r l } & { \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( \displaystyle \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { a } \} = y , \displaystyle \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { \frac { 1 } { a } } \} = x \right) } \\ & { = \frac { 1 } { 2 } \mathbb { P } \left( \displaystyle \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { a } \} = y , \displaystyle \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { \frac { 1 } { a } } \} = x | U _ { m } \geq U _ { 1 } ^ { a } \right) } \\ & { \quad + \frac { 1 } { 2 } \mathbb { P } \left( \displaystyle \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { a } \} = y , \displaystyle \sum _ { i = 1 } ^ { m - 1 } \mathbf { 1 } \{ U _ { m } \geq U _ { i } ^ { \frac { 1 } { a } } \} = x | U _ { m } \geq U _ { 1 } ^ { \frac { 1 } { a } } \right) , } \end{array}
$$  

where the first probability equals  

$$
\frac { a + 1 } { a } \int _ { 0 } ^ { 1 } \binom { m - 2 } { x - 1 } ( r ^ { a } ) ^ { x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - y - 1 } d r , \; \; \mathrm { w h e n } \; y = x \geq 1 ,
$$  

and equals  

$$
\frac { a + 1 } { a } \int _ { 0 } ^ { 1 } \left( \frac { m - 2 } { x } \right) \cdot \binom { m - x - 2 } { y - x - 1 } ( r ^ { a } ) ^ { x } \cdot ( r ^ { \frac { 1 } { a } } - r ^ { a } ) ^ { y - x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - y - 1 } d r , \; \; \mathrm { w h e n } \; y > x ,
$$  

and the second probability equals  

$$
( a + 1 ) \int _ { 0 } ^ { 1 } \binom { m - 2 } { x - 1 } \cdot \binom { m - x - 1 } { y - x } ( r ^ { a } ) ^ { x } \cdot ( r ^ { \frac { 1 } { a } } - r ^ { a } ) ^ { y - x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - y - 1 } d r , \; \; \mathrm { w h e n } \; y \geq x \geq 1 .
$$  

Next, we derive the lower bound of the optimal asymptotic sum of two types of errors,  

$$
A \geq \operatorname* { l i m } _ { n \to \infty } \operatorname* { i n f } ( \mathrm { t y p e ~ I I ~ e r r o r ~ u n d e r ~ } \mathbf { P } _ { 1 : n } ^ { * } ) ^ { \frac { 1 } { n } }
$$  

We obtain the type II error as  

$$
\begin{array} { r l } & { \operatorname* { l i m } _ { n \to \infty } \mathbb { P } _ { \mathcal { H } _ { 1 } } ^ { \frac { 1 } { n } } ( R ^ { c } ) = \operatorname* { l i m } _ { n \to \infty } \mathbb { P } _ { \mathcal { H } _ { 1 } } ^ { \frac { 1 } { n } } \left( \prod _ { t = 1 } ^ { n } X _ { t } < ( m - 1 ) ^ { n } \right) } \\ & { = \operatorname* { l i m } _ { n \to \infty } \mathbb { P } _ { \mathcal { H } _ { 1 } } ^ { \frac { 1 } { n } } \left( \sum _ { i = 1 } ^ { n } \ln X _ { t } < n \cdot \ln ( m - 1 ) \right) = \operatorname* { i n f } _ { t } \frac { \mathbb { E } _ { \mathcal { H } _ { 1 } } X _ { i } ^ { t } } { ( m - 1 ) ^ { t } } . } \end{array}
$$  

Under the condition that $a(n) \gg m \gg 0$, we have,  

$$
\begin{array} { r l } & { \mathbb { E } _ { \mathcal { H } _ { 1 } } X _ { t } ^ { s } = \Gamma _ { 1 } + \Gamma _ { 2 } + \Gamma _ { 3 } } \\ & { : = \frac { 1 } { 2 } \sum _ { x = 1 } ^ { m - 1 } \sum _ { y = x } ^ { m - 1 } ( a + 1 ) \int _ { 0 } ^ { 1 } \binom { m - 2 } { x - 1 } \binom { m - x - 1 } { y - x } \cdot ( r ^ { a } ) ^ { x } ( r ^ { \frac { 1 } { a } } - r ^ { a } ) ^ { y - x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - y - 1 } d r \cdot ( x + y ) ^ { s } } \\ & { \quad + \frac { 1 } { 2 } \sum _ { x = 1 } ^ { m - 1 } \frac { a + 1 } { a } \int _ { 0 } ^ { 1 } \binom { m - 2 } { x - 1 } ( r ^ { a } ) ^ { x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - x - 1 } d r \cdot ( 2 x ) ^ { s } } \\ & { \quad + \frac { 1 } { 2 } \sum _ { x = 0 } ^ { m - 1 } \sum _ { y = x + 1 } ^ { m - 1 } \frac { a + 1 } { a } \int _ { 0 } ^ { 1 } \binom { m - 2 } { x } \binom { m - x - 2 } { y - x - 1 } \cdot ( r ^ { a } ) ^ { x } ( r ^ { \frac { 1 } { a } } - r ^ { a } ) ^ { y - x } \cdot ( 1 - r ^ { \frac { 1 } { a } } ) ^ { m - y - 1 } d r \cdot ( x + y ) ^ { s } } \end{array}
$$  

Then, we have $r^{a} \rightarrow 0, r \in (0,1), r^{\frac{1}{a}} \rightarrow 1, r \in (0,1), \Gamma_{1} \rightarrow \frac{1}{2}m^{s}, \Gamma \rightarrow 0,$ and $\Gamma_{3} \rightarrow \frac{1}{2}(m-1)^{s}$.

Henceforth, we have $\mathbb{E}_{\mathcal{H}_{1}}X_{t}^{s}/(m-1)^{s} \rightarrow 1$. We also have $R_{\mathcal{P}}(h) \rightarrow 0, e^{-R_{\mathcal{P}}(h)} \rightarrow 1$. This result shows that the asymptotic sum of the two types of errors of our proposed test matches the lower bound of the asymptotic sum of the two types of errors of all tests.  

This completes the proof of Theorem 5.  

# C.5 Proof of Theorem 6  

By the definition that $Y = U_{\omega_t}$, we have.  

$$
\begin{array} { r l } & { \mathbb { P } ( Y \leq r ) = \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { N } \mathbb { P } \left( U _ { i } ^ { \frac { 1 } { p _ { i } } } \mathrm { i s ~ t h e ~ l a r g e s t ~ o n e } , U _ { j } \leq r \right) \cdot q _ { i j } } \\ & { \qquad = \sum _ { i = 1 } ^ { N } \sum _ { j \neq i } \frac { p _ { i } } { 1 - p _ { j } } ( r - r ^ { \frac { 1 } { p _ { j } } } \cdot p _ { j } ) \cdot q _ { i j } + \sum _ { i = 1 } ^ { N } p _ { i } r ^ { \frac { 1 } { p _ { i } } } \cdot q _ { i i } : = f ( r ) . } \end{array}
$$  

Furthermore,  

$$
\begin{array} { r l } & { f ^ { \prime } ( r ) = \displaystyle \sum _ { i = 1 } ^ { N } \displaystyle \sum _ { j \neq i } \frac { p _ { i } } { 1 - p _ { j } } p _ { j } \cdot ( - 1 ) \cdot q _ { i j } ( \frac { 1 } { p _ { j } } - 1 ) r ^ { \frac { 1 } { p _ { j } } - 2 } + \displaystyle \sum _ { i = 1 } ^ { N } p _ { i } ( \frac { 1 } { p _ { i } } - 1 ) r ^ { \frac { 1 } { p _ { i } } - 2 } q _ { i i } } \\ & { \quad = \displaystyle \sum _ { i = 1 } ^ { N } \displaystyle \sum _ { j \neq i } ( - 1 ) p _ { i } q _ { i j } r ^ { \frac { 1 } { p _ { j } } - 2 } + \displaystyle \sum _ { i = 1 } ^ { N } ( 1 - p _ { i } ) r ^ { \frac { 1 } { p _ { i } } - 2 } q _ { i i } } \\ & { \quad = \displaystyle \sum _ { i = 1 } ^ { N } \displaystyle \sum _ { j \neq i } ( - 1 ) p _ { j } q _ { j i } r ^ { \frac { 1 } { p _ { i } } - 2 } + \displaystyle \sum _ { i = 1 } ^ { N } ( 1 - p _ { i } ) r ^ { \frac { 1 } { p _ { i } } - 2 } q _ { i i } . } \end{array}
$$  

By the condition that $q_{ii} \geq \frac{\sum_{j \neq i} p_j q_{ji}}{1-p_i}$, the coefficients of $r^{\frac{1}{p_i}-2}$ are all positive. This implies that $f'(r) \geq 0$. Then, as $f(1) = 1$, we have  

$$
\mathbb { P } ( Y \leq r ) = r f ( r ) \leq r .
$$  

This completes the proof of Theorem 6.  

# C.6 Proof of Theorem 7  

We first present a number of supporting lemmas.  

Lemma 4. For any constant $r \in [0,1]$, $f(x) := (r - r^{\frac{1}{x}} \cdot x)/(1 - x)$ is a monotonically increasing function of $x \in [0,1]$.  

Proof. We have  

$$
f ^ { \prime } ( x ) = \left( r - r ^ { \frac { 1 } { x } } + \frac { 1 } { x } r ^ { \frac { 1 } { x } } \ln r - r ^ { \frac { 1 } { x } } \ln r \right) / ( 1 - x ) ^ { 2 } .
$$  

Using the inequality $\ln r^{1-\frac{1}{x}} \leq r^{1-\frac{1}{x}} - 1$, we have  

$\ln r \geq \frac{r^{1-\frac{1}{x}}-1}{1-\frac{1}{x}}$, and $f'(x)=r-r^{\frac{1}{x}}+(\frac{1}{x}-1)r^{\frac{1}{x}}\ln r \geq r-r^{\frac{1}{x}}+(\frac{1}{x}-1)r^{\frac{1}{x}}\cdot\frac{r^{1-\frac{1}{x}}-1}{1-\frac{1}{x}}=0$.

This completes the proof of Lemma 4. $\square$  

Lemma 5. For $\mathbf{P}$, suppose $p_1 \geq p_2 \geq \cdots \geq p_N$, $F_{1,\mathbf{P},\mathbf{Q}}(r) = \sum_{i=1}^{N} \sum_{j \neq i} \frac{p_i}{1-p_j} (r - r^{\frac{1}{p_j}} \cdot p_j) \cdot q_{ij} + \sum_{i=1}^{N} p_i r^{\frac{1}{p_i}} \cdot q_{ii}$. Then we have $\sup_{\mathbf{Q} \in \mathcal{Q}} F_{1,\mathbf{P},\mathbf{Q}}(r) = F_{1,\mathbf{P},\mathbf{Q}^*}(r)$, where  

$$
\mathbf { Q } ^ { * } = \left( \begin{array} { l l l l l l } { \theta } & { 1 - \theta } & { 0 } & { \cdots } & { 0 } \\ { 1 - \theta } & { \theta } & { 0 } & { \cdots } & { 0 } \\ { 1 - \theta } & { 0 } & { \theta } & { \cdots } & { 0 } \\ { \vdots } & { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { 1 - \theta } & { 0 } & { 0 } & { \cdots } & { \theta } \end{array} \right) .
$$  

Proof. By the inequality that  

$$
\frac { r - r ^ { \frac { 1 } { p _ { j } } } \cdot p _ { j } } { 1 - p _ { j } } \geq r \geq r ^ { \frac { 1 } { p _ { i } } } ,
$$  

as well as the monotonicity, we obtain the desired result, which completes the proof of Lemma 5. $\square$  

Lemma 6 ([16]). The set of extreme points of $\mathcal{P}_{\Delta}$, denoted by $Ext(\mathcal{P}_{\Delta})$, is given by  

$$
E x t ( \mathcal { P } _ { \Delta } ) = \left\{ \pi ( \mathbf { P } _ { \Delta } ^ { \star } ) : \pi \, i s \, a \, p e r m u t a t i o n \, o n \, \{ 1 , 2 , \ldots , | \mathcal { W } | \} \right\} ,
$$  

where $\pi(\mathbf{P})$ denotes the permuted NTP distribution whose ith coordinate is $P_{\pi(i)}$.  

Lemma 7 (Donsker-Varadhan representation).  

$$
D _ { K L } ( \mathbf { P } | | \mathbf { Q } ) = \operatorname* { s u p } _ { T : \mathcal { X } \to \mathbb { R } } \mathbb { E } _ { \mathbf { P } } [ T ( x ) ] - \log ( \mathbb { E } _ { \mathbf { Q } } [ e ^ { T ( x ) } ] ) .
$$  

Lemma 8. The Hessian matrix of $f(\mathbf{P},r)=f(p_2,p_3,\cdots,p_N)$ is positive semi-definite.  

Proof. For $i \geq 3$,  

$$
\frac { \partial ^ { 2 } f ( \mathbf { P } , r ) } { ( \partial p _ { i } ) ^ { 2 } } = r ^ { \frac { 1 } { p _ { i } } } \frac { \ln ^ { 2 } ( r ) } { p _ { i } ^ { 3 } } \geq 0 .
$$  

For $i=2$, we have  

$$
\begin{array} { r l } & { \frac { \partial ^ { 2 } f ( \mathbf { P } , r ) } { \left( \partial p _ { 2 } \right) ^ { 2 } } = ( 1 - \Delta ) \cdot ( 1 - \theta ) \Bigg \{ \left( r ^ { \frac { 1 } { p ^ { 2 } } } \ln ^ { 2 } r ^ { \frac { - 1 } { p _ { 2 } ^ { 3 } } } + r ^ { \frac { 1 } { p _ { 2 } } } \ln ^ { 2 } r ^ { \frac { 1 } { p _ { 2 } ^ { 2 } } } \right) \cdot ( 1 - p _ { 2 } ) ^ { 2 } } \\ & { \qquad + \left( \frac { 1 } { p _ { 2 } } r ^ { \frac { 1 } { p _ { 2 } } } \ln r - r ^ { \frac { 1 } { p _ { 2 } } } \ln r + r - r ^ { \frac { 1 } { p _ { 2 } } } \right) \cdot 2 ( 1 - p _ { 2 } ) \Bigg \} / ( 1 - p _ { 2 } ) ^ { 4 } + \theta r ^ { \frac { 1 } { p _ { 2 } } } \ln ^ { 2 } r ^ { \frac { 1 } { p _ { 2 } ^ { 3 } } } . } \end{array}
$$  

To show it is non-negative, it suffices to show that  

$$
2 ( r ^ { 1 - \frac { 1 } { x } } - 1 ) \geq ( \ln r ^ { 1 - \frac { 1 } { x } } ) ^ { 2 } + 2 \ln r ^ { 1 - \frac { 1 } { x } } , \; \; \mathrm { f o r } \; r ^ { 1 - \frac { 1 } { x } } > 1 ,
$$  

which can be verified directly. This completes the proof of Lemma 8.  

We now proceed with the proof of Theorem 7. First, we simplify $\mathbb{E}_{1,\mathbf{P},\mathbf{Q}}e^{-h(Y)}$ as follows.  

$$
\begin{array} { r l } & { \mathbb { E } _ { 1 , \mathbf { P } , \mathbf { Q } } e ^ { - h ( Y ) } = \int _ { 0 } ^ { 1 } e ^ { - h ( r ) } \cdot F _ { 1 , \mathbf { P } , \mathbf { Q } } ( d r ) } \\ & { = F _ { 1 , \mathbf { P } , \mathbf { Q } } ( r ) e ^ { - h ( r ) } | _ { 0 } ^ { 1 } + \int _ { 0 } ^ { 1 } F _ { 1 , \mathbf { P } , \mathbf { Q } } ( r ) e ^ { - h ( r ) } h ( d r ) = e ^ { - h ( 1 ) } + \int _ { 0 } ^ { 1 } F _ { 1 , \mathbf { P } , \mathbf { Q } } ( r ) e ^ { - h ( r ) } h ( d r ) . } \end{array}
$$  

This representation shows that, for a non-decreasing function $h$,  

$$
\arg \operatorname* { m a x } _ { \mathbf { P } , \mathbf { Q } } \mathbb { E } _ { 1 , \mathbf { P } , \mathbf { Q } } e ^ { - h ( Y ) } = \arg \operatorname* { m a x } _ { \mathbf { P } , \mathbf { Q } } F _ { 1 , \mathbf { P } , \mathbf { Q } } ( r ) .
$$  

Recall that  

$$
F _ { 1 , \mathbf { P } , \mathbf { Q } } ( r ) = \sum _ { i = 1 } ^ { N } \sum _ { j \neq i } \frac { p _ { i } } { 1 - p _ { j } } \left( r - r ^ { \frac { 1 } { p _ { j } } } \cdot p _ { j } \right) \cdot q _ { i j } + \sum _ { i = 1 } ^ { N } p _ { i } r ^ { \frac { 1 } { p _ { i } } } \cdot q _ { i i } .
$$  

Without loss of generality, suppose $p_1 \geq p_2 \geq \cdots \geq p_N$. By Lemma 5,  

$$
\begin{array} { r l } & { F _ { 1 , \mathbf { P } , \mathbf { Q } } ( r ) = \frac { p _ { 1 } } { 1 - p _ { 2 } } ( r - r ^ { \frac { 1 } { p _ { 2 } } } p _ { 2 } ) ( 1 - \theta ) + \sum _ { i = 2 } ^ { N } \frac { p _ { i } } { 1 - p _ { 1 } } ( r - r ^ { \frac { 1 } { p _ { 1 } } } p _ { 1 } ) ( 1 - \theta ) + \sum _ { i = 1 } ^ { N } p _ { i } r ^ { \frac { 1 } { p _ { i } } } \theta } \\ & { \quad = ( 1 - \theta ) \frac { p _ { 1 } } { 1 - p _ { 2 } } ( r - r ^ { \frac { 1 } { p _ { 2 } } } p _ { 2 } ) + ( 1 - \theta ) ( r - r ^ { \frac { 1 } { p _ { 1 } } } p _ { 1 } ) + \theta \sum _ { i = 1 } ^ { N } p _ { i } r ^ { \frac { 1 } { p _ { i } } } } \\ & { \quad = ( 1 - \theta ) \frac { p _ { 1 } } { 1 - p _ { 2 } } ( r - r ^ { \frac { 1 } { p _ { 2 } } } p _ { 2 } ) + ( 1 - \theta ) r + ( 2 \theta - 1 ) r ^ { \frac { 1 } { p _ { 1 } } } p _ { 1 } + \theta \sum _ { i = 2 } ^ { N } p _ { i } r ^ { \frac { 1 } { p _ { i } } } } \\ & { \quad : = f ( \mathbf { P } , r ) . } \end{array}
$$  

We note that for $i \geq 3$,  

$$
\frac { \partial ^ { 2 } f ( \mathbf { P } , r ) } { ( \partial p _ { i } ) ^ { 2 } } = \theta r ^ { \frac { 1 } { p _ { i } } } \frac { \ln ^ { 2 } ( r ) } { p _ { i } ^ { 3 } } > 0 .
$$  

Combining Lemma 6 and the convexity of $p_3, p_4, \cdots, p_N$, we have  

$$
\arg \operatorname* { m a x } _ { p _ { 3 } , \cdots , p _ { N } } f ( \mathbf { P } , r ) = ( p _ { 2 } , p _ { 2 } , \cdots , p _ { 2 } , \epsilon , 0 , 0 , \cdots , 0 ) ,
$$  

where $\epsilon < p_{2}$. Assuming there are $m$ times of $p_{2}$, then $p_{1} = 1 - mp_{2} - \epsilon$. Moreover,  

$$
\begin{array} { r l } & { f ( \mathbf { P } , r ) = ( 1 - \theta ) \frac { p _ { 1 } } { 1 - p _ { 2 } } ( r - r ^ { \frac { 1 } { p _ { 2 } } } p _ { 2 } ) + ( 1 - \theta ) r + ( 2 \theta - 1 ) r ^ { \frac { 1 } { p _ { 1 } } } p _ { 1 } + \theta \sum _ { i = 2 } ^ { N } p _ { i } r ^ { \frac { 1 } { p _ { i } } } } \\ & { \qquad = ( 1 - \theta ) \frac { 1 - m p _ { 2 } - \epsilon } { 1 - p _ { 2 } } r + p _ { 2 } r ^ { \frac { 1 } { p _ { 2 } } } \frac { m ( 1 - 2 \theta ) p _ { 2 } + \epsilon + \theta - 1 - \theta \epsilon + \theta m } { 1 - p _ { 2 } } + ( 1 - \theta ) r } \end{array}
$$  

$$
+ \, \theta \epsilon r ^ { \frac { 1 } { \epsilon } } + ( 2 \theta - 1 ) r ^ { \frac { 1 } { 1 - m p _ { 2 } - \epsilon } } ( 1 - m p _ { 2 } - \epsilon ) .
$$  

Fixing $\epsilon$, $m$, we have that,  

$$
\begin{array} { r } { \frac { d f ( \mathbf { P } , r ) } { d p _ { 2 } } = ( 1 - \Theta ) r \cdot \frac { - m + 1 - \epsilon } { ( 1 - p _ { 2 } ) ^ { 2 } } + r ^ { \frac { 1 } { p _ { 2 } } } \ln r \frac { - 1 } { p _ { 2 } } \frac { m ( 1 - 2 \Theta ) p _ { 2 } + \epsilon + \theta - 1 - \theta \epsilon + \theta m } { 1 - p _ { 2 } } } \\ { + r ^ { \frac { 1 } { p _ { 2 } } } \frac { m ( 1 - 2 \theta ) p _ { 2 } + \epsilon + \theta - 1 - \theta \epsilon + \theta m } { 1 - p _ { 2 } } + r ^ { \frac { 1 } { p _ { 2 } } } p _ { 2 } \frac { m - m \theta + \epsilon + \theta - 1 - \theta \epsilon } { ( 1 - p _ { 2 } ) ^ { 2 } } + A ( r ) , } \end{array}
$$  

where $A(r)$ is a component smaller than 0. By Lemma 3, we can show the above term is smaller than zero. As such, the smaller $p_2$ is, the larger $f(\mathbf{P},r)$ is, whereas the larger $p_1$ is, the larger $f(\mathbf{P},r)$ is. By fixing $p_1$ at $1-\Delta$, we can then calculate the Hessian matrix of $f(\mathbf{P},r)=f(p_2,p_3,\ldots,p_N)$.  

Combining the results above and Lemma 8, we obtain the following, for any non-decreasing function $h$,  

$$
\begin{array} { r } { \mathbf { Q } ^ { * } , \mathbf { P } ^ { * } = \underset { \mathbf { Q } , \mathbf { P } \in \mathcal { P } _ { \Delta } } { \arg \operatorname* { m a x } } \mathbb { E } _ { 1 , \mathbf { P } , \mathbf { Q } } e ^ { - h ( Y ) } , } \end{array}
$$  

where  

$$
\begin{array} { r l } & { \mathbf { Q } ^ { * } = \left( \begin{array} { l l l l l } { \theta } & { 1 - \theta } & { 0 } & { \cdots } & { 0 } \\ { 1 - \theta } & { \theta } & { 0 } & { \cdots } & { 0 } \\ { 1 - \theta } & { 0 } & { \theta } & { \cdots } & { 0 } \\ { \vdots } & { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { 1 - \theta } & { 0 } & { 0 } & { \cdots } & { \theta } \end{array} \right) ; } \\ & { \mathbf { P } ^ { * } = \left( 1 - \Delta , \cdots , 1 - \Delta , 1 - ( 1 - \Delta ) \cdot \left[ \frac { 1 } { 1 - \Delta } \right] , 0 , \ldots \right) . } \end{array}
$$  

Replacing $\theta h(\cdot)$ with $h(\cdot)$, we have that,  

$$
\begin{array} { r l } & { \underset { h } { \operatorname* { m i n } } \underset { \mathbf { Q } \in \mathcal { Q } } { \operatorname* { m a x } } \underset { \mathbf { P } \in \mathcal { P } } { \operatorname* { m a x } } \left( \mathbb { E } _ { 0 } \theta h ( Y ) + \log \mathbb { E } _ { 1 , \mathbf { P } , \mathbf { Q } } e ^ { - \theta h ( Y ) } \right) } \\ & { = \underset { h } { \operatorname* { m i n } } \underset { \mathbf { Q } \in \mathcal { Q } } { \operatorname* { m a x } } \underset { \mathbf { P } \in \mathcal { P } } { \operatorname* { m a x } } \left( \mathbb { E } _ { 0 } h ( Y ) + \log \mathbb { E } _ { 1 , \mathbf { P } , \mathbf { Q } } e ^ { - h ( Y ) } \right) } \\ & { \geq \underset { h } { \operatorname* { m i n } } \left( \mathbb { E } _ { 0 } h ( Y ) + \log \mathbb { E } _ { 1 , \mathbf { P } ^ { * } , \mathbf { Q } ^ { * } } e ^ { - h ( Y ) } \right) } \\ & { = - D _ { K L } \big ( \mu _ { 0 } , \mu _ { 1 , \mathbf { P } ^ { * } , \mathbf { Q } ^ { * } } \big ) . } \end{array}
$$  

Thanks to the uniqueness of the Donsker-Varadhan representation, $\mathbb{E}_0h(Y) + \log \mathbb{E}_{1,\mathbf{P},\mathbf{Q}}e^{-h(Y)}$ is strictly larger than $-D_{KL}(\mu_0, \mu_1,\mathbf{P}^*,\mathbf{Q}^*)$, unless we take the log-likelihood ratio  

$$
\begin{array} { r } { h _ { \Delta } ^ { * } ( r ) = \log \frac { d \mu _ { 1 , \mathbf { P } ^ { * } , \mathbf { Q } ^ { * } } } { d \mu _ { 0 } } ( r ) = \left\{ \begin{array} { l l } { \log \left( \frac { 1 - \theta } { \Delta } + \left( \lfloor \frac { 1 } { 1 - \Delta } \rfloor \, \theta + \frac { 1 } { \Delta } \theta - \frac { 1 } { \Delta } \right) r ^ { \frac { \Delta } { 1 - \Delta } } + \theta r ^ { \frac { \bar { \Delta } } { 1 - \bar { \Delta } } } \right) } & { \mathrm { i f ~ } \Delta \geq \frac { 1 } { 2 } } \\ { \log \left( 2 ( 1 - \theta ) + ( 2 \theta - 1 ) ( r ^ { \frac { 1 - \Delta } { \Delta } } + r ^ { \frac { \bar { \Delta } } { 1 - \bar { \Delta } } } ) \right) } & { \mathrm { i f ~ } \Delta < \frac { 1 } { 2 } } \end{array} \right. } \end{array}
$$  

which is non-decreasing in $r$. In this case, we can use Lemmas 4 to 8 to derive $\mathbf{P}^*$ and $\mathbf{Q}^*$. This completes the proof of Theorem 7.  

# C.7 Proof of Theorem 8  

This theorem can be proved in a similar fashion as that of Theorem 5. Here we provide the proof for the case when $\frac{1}{1-\Delta}$ is an integer.  

There are $(\frac{m}{1-\Delta})$ permutations of distribution $(\underbrace{1-\Delta,1-\Delta,\cdots,1-\Delta}_{\frac{1}{1-\Delta}times}}0,0,\cdots,0)$. Denote these multinomial distributions as $\mathbf{R}_i,i=1,\ldots,(\frac{m}{1-\Delta})$. For each $\mathbf{R}_i$, suppose the positions of $1-\Delta$ are $x_1,x_2,\cdots,x_{\frac{1}{1-\Delta}}$. Then we define its adjoint feature matrix as $T_i$, which satisfies that, for any $k\in[1,n]$, the $(k,k)$th-element is $\theta$, and each element in the column $x_1,x_2,\cdots,x_{\frac{1}{1-\Delta}}$ and the line $x_1,x_2,\ldots,x_{\frac{1}{1-\Delta}}$ but not on the diagonal is $(1-\theta)/(\frac{1}{1-\Delta}-1)$, each element in the column $x_1,x_2,\cdots,x_{\frac{1}{1-\Delta}}$ but not on the line $x_1,x_2,\cdots,x_{\frac{1}{1-\Delta}}$ and not on the diagonal is $(1-\Delta)(1-\theta)$. That is,  

$$
\begin{array} { r l } & { \mathbf { R } _ { 1 } = \underbrace { \left( 1 - \Delta , 1 - \Delta , \cdots , 1 - \Delta , 0 , 0 , \cdots , 0 \right) } _ { \frac { 1 } { 1 - \Delta } t i m e s } , \quad T _ { 1 } = \left( \begin{array} { l l } { A } & { B } \\ { C } & { D } \end{array} \right) } \\ & { A = \left( \begin{array} { l l l l } { \theta } & { \frac { 1 - \theta } { 1 - \Delta } } & { \cdots } & { \frac { 1 - \theta } { 1 - \Delta } } \\ { \frac { 1 - \theta } { 1 - \Delta } } & { \theta } & { \cdots } & { \frac { 1 - \theta } { 1 - \Delta } } \\ { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { \frac { 1 - \theta } { 1 - \Delta } } & { \frac { 1 - \theta } { 1 - \Delta } } & { \cdots } & { \theta } \end{array} \right) , \quad B = \mathbf { 0 } _ { \frac { 1 } { 1 - \Delta } \times \frac { 1 } { 1 - \Delta } } , } \\ & { C = \left( \begin{array} { l l l l } { ( 1 - \Delta ) ( 1 - \theta ) } & { ( 1 - \Delta ) ( 1 - \theta ) } & { \cdots } & { ( 1 - \Delta ) ( 1 - \theta ) } \\ { ( 1 - \Delta ) ( 1 - \theta ) } & { ( 1 - \Delta ) ( 1 - \theta ) } & { \cdots } & { ( 1 - \Delta ) ( 1 - \theta ) } \\ { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { ( 1 - \Delta ) ( 1 - \theta ) } & { ( 1 - \Delta ) ( 1 - \theta ) } & { \cdots } & { ( 1 - \Delta ) ( 1 - \theta ) } \end{array} \right) , \quad D = \theta \cdot \mathbf { 1 } _ { ( m - \frac { 1 } { 1 - \Delta } ) \times ( m - \frac { 1 } { 1 - \Delta } ) } . } \end{array}
$$  

In this setting, $\mathbf{P}_{t}^{0}$ is the probability distribution that is the mixture of those $\mathbf{Q}_{i}$ with $R_{i}$. The hypotheses are of the form,  

$$
\begin{array} { r } { \mathcal { H } _ { 0 } : \omega _ { t } \perp \zeta _ { t } , t = 1 , 2 , \cdots . N \quad \mathcal { H } _ { 1 } : \omega _ { t } = \mathcal { S } ( \mathbf { P } _ { t } ^ { 0 } , \zeta _ { t } ) , t = 1 , 2 , \cdots , n . } \end{array}
$$  

Then the best rejection region is,  

$$
\begin{array} { r l } & { R = \left\{ ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) : ( ( 1 - \Delta ) \cdot \left( \frac { m - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) ) ^ { n } \leq \prod _ { t = 1 } ^ { n } Y _ { t } \right\} } \\ & { Y _ { t } = \displaystyle \sum _ { S \subset \{ 1 , 2 , \cdots , m \} , | S | = \frac { 1 } { 1 - \Delta } } \theta \cdot \mathbf { 1 } \{ U _ { \omega _ { t } } \mathrm { ~ i s ~ t h e ~ l a r g e s t ~ o n e ~ a m o n g ~ } U _ { i | i \in S } \} } \\ & { \quad + \frac { 1 - \theta } { \frac { 1 } { 1 - \Delta } - 1 } \cdot \mathbf { 1 } \left\{ U _ { \omega _ { t } } \mathrm { ~ h a s ~ r a n k ~ b e t w e e n ~ 2 ~ a n d ~ } \frac { 1 } { 1 - \Delta } , \mathrm { ~ i n c l u s i v e l y } \right\} . } \end{array}
$$  

Under $\mathcal{H}_0$, we have  

$$
\begin{array} { r l } & { \mathbb { P } \left( Y _ { t } = \frac { 1 - \theta } { \frac { 1 } { 1 - \Delta } - 1 } \cdot \left( \frac { m - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) \right) = \frac { \frac { 1 } { 1 - \Delta } - 1 } { m } ; } \\ & { \mathbb { P } \left( Y _ { t } = \frac { 1 - \theta } { \frac { 1 } { 1 - \Delta } - 1 } \cdot \left( \frac { m - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) + \left( \theta - \frac { 1 - \theta } { \frac { 1 } { 1 - \Delta } - 1 } \right) \cdot \left( \frac { k - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) \right) = \frac { 1 } { m } , k \geq \frac { 1 } { 1 - \Delta } . } \end{array}
$$  

By the large deviation theory, the Type 1 error of the optimal test is,  

$$
\begin{array} { r l } & { \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \prod _ { t = 1 } ^ { n } Y _ { t } \geq \left( \Delta \cdot \left( \frac { m - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) \right) ^ { N } \right) } \\ & { = \operatorname* { i n f } _ { t } \frac { \mathbb { E } [ Y _ { t } ^ { t } ] } { \left( ( 1 - \Delta ) \cdot \left( \frac { m - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) \right) ^ { t } } } \\ & { = \frac { \frac { 1 } { 1 - \Delta } - 1 } { m } \cdot \left( \frac { 1 - \theta } { \frac { 1 } { 1 - \Delta } - 1 } \cdot \left( \frac { m - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) \right) ^ { t } } \\ & { + \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \frac { 1 } { m } \left[ \frac { 1 - \theta } { \frac { 1 } { 1 - \Delta } - 1 } \cdot \left( \frac { m - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) + \left( \theta - \frac { 1 - \theta } { \frac { 1 } { 1 - \Delta } - 1 } \right) \cdot \left( \frac { k - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \right) \right] ^ { t } } \\ & { = \frac { \frac { 1 } { 1 - \Delta } - 1 } { m } \cdot \left( \frac { 1 - \theta } { \frac { 1 } { 1 - \Delta } - 1 } \cdot m \right) ^ { t } } \\ & { + \sum _ { k = \frac { 1 } { 1 - \Delta } } ^ { m } \frac { 1 } { m } \cdot \left[ \frac { \frac { 1 } { 1 - \Delta } \cdot \theta - 1 } { \frac { 1 } { 1 - \Delta } - 1 } \cdot \frac { 1 } { 1 - \Delta } \cdot \frac { ( k - 1 ) \cdots ( k - \frac { 1 } { 1 - \Delta } + 1 ) } { ( m - 1 ) \cdots ( m - \frac { 1 } { 1 - \Delta } + 1 ) } + \frac { 1 - \theta } { \frac { 1 } { 1 - \Delta } - 1 } \cdot m \right] ^ { t } } \\ & { \geq \operatorname* { i n f } _ { \alpha } \int p ^ { \alpha } ( y ) \, d y - O \left( \frac { 1 } { m } \right) . } \end{array}
$$  

This completes the proof of Theorem 8.  

# D Proofs for Red-green-list Watermark in Section 5  

# D.1 Proof of Proposition 2  

or the feature matrix under $\mathcal{H}_0$, the NTP distribution is $\mathbf{P}_t$ no matter what the secret key $\zeta_t$ is. For the feature matrix under $\mathcal{H}_1$, and for the complete inheritance case, when $\zeta_t = A_i$,  

$$
\mathbb { P } ( \omega _ { t } = k | \mathbf { P } _ { t } , \zeta _ { t } = A _ { i } ) = p _ { t , k } \cdot \frac { \mathbf { 1 } \{ k \in A _ { i } \} } { \sum _ { i \in A _ { i } } p _ { t , i } }
$$  

As such, the feature matrix is $\begin{pmatrix} \mathbf{M}_{t,1} \\ \mathbf{M}_{t,2} \\ \vdots \\ \mathbf{M}_{t,k} \end{pmatrix}$, where $\mathbf{S}_{t,i} = \left(p_{t,1} \frac{\mathbf{1}\{1\in A_i\}}{\sum_{i\in A_i} p_{t,i}}, p_{t,2} \frac{\mathbf{1}\{2\in A_i\}}{\sum_{i\in A_i} p_{t,i}}, \cdots, p_{t,m} \frac{\mathbf{1}\{m\in A_i\}}{\sum_{i\in A_i} p_{t,i}}\right)$. For the partial inheritance case, by definition, the $t$th feature matrix belongs to the class $\begin{cases} \left(\begin{array}{c} \mathbf{Q}_{t,1} \\ \mathbf{Q}_{t,2} \\ \vdots \\ \mathbf{Q}_{t,k} \end{array}\right) : TV(\mathbf{Q}_{t,i}, \mathbf{M}_{t,i}) \leq 1 - \theta \end{cases}$.  

This completes the proof of Proposition 2.  

# D.2 Proof of Theorem 9  

Under $\mathcal{H}_0$, $\omega_t$ is independent of $\zeta_t$. So $Y_t = 1$ is equivalent to being painted as red. The painting style is random and the probability for each element to be pained as green is $\gamma$.  

Under $\mathcal{H}_1$, $\omega_t$ is sampled from the green list, so the probability of $\omega_t \in \zeta_t$ is 1. That is, $\mathbb{P}_{\mathcal{H}_1}(Y_t = 1) = 1$.  

This completes the proof of Theorem 9.  

# D.3 Proof of Theorem 10  

For (a), the result holds by directly applying the law of large numbers.  

For (b), under $\mathcal{H}_1$, the statistic $Y_t$ is a constant 1. Thus we have $\sum_{t=1}^{n} Y_t = n$. Choosing the threshold $\gamma_n$ at $n$ can minimize the type I error and keep the type II error at 0.  

This completes the proof of Theorem 10.  

# D.4 Proof of Theorem 11  

For (b), consider the case that for each $t$, the NTP distribution is $P := (1/m, \ldots, 1/m)$. Denote the asymptotic sum of type I error and type II error under $P$ as $g_P(\psi)$. Then we have  

$$
\operatorname* { i n f } _ { \psi } g ( \psi ) \geq \operatorname* { i n f } _ { \psi } g _ { P } ( \psi ) .
$$  

We next compute the likelihood of $(\omega_t, \zeta_t)_{1:n}$ under $\mathcal{H}_0$ and $\mathcal{H}_1$.  

$$
\begin{array} { r l } & { \mathbb { P } _ { \mathcal { H } _ { 0 } } ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) = \left( \frac { 1 } { \binom { m } { \gamma \cdot m } } \cdot \frac { 1 } { m } \right) ^ { n } , } \\ & { \mathbb { P } _ { \mathcal { H } _ { 1 } } ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) = \prod _ { t = 1 } ^ { n } \mathbf { 1 } ( \omega _ { t } \in \zeta _ { t } ) \cdot \frac { 1 } { \gamma \cdot m } \cdot \frac { 1 } { \binom { m } { \gamma \cdot m } } . } \end{array}
$$  

By Lemma 3, the optimal test is to reject the null hypothesis when $\mathbb{P}_{\mathcal{H}_0}(\omega_{1:n}, \zeta_{1:n}) \leq \mathbb{P}_{\mathcal{H}_1}(\omega_{1:n}, \zeta_{1:n})$. So the rejection region is,  

$$
R = \{ ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) | \omega _ { t } \in \zeta _ { t } , t = 1 , 2 , \cdots , n \} .
$$  

Henceforth, we have proved that $\inf_{\psi} g(\psi) \geq g(\psi_2')$. By definition, we have $g(\psi_2') \geq \inf_{\psi} g(\psi)$. This proves (b).  

For (a), similarly, consider the case that for each $t$, the NTP distribution is $P := (1/m, \ldots, 1/m)$ Denote the asymptotic type II error under $P$ as $f_P(\psi)$. Then we have  

$$
\operatorname* { i n f } _ { \psi } f ( \psi ) \geq \operatorname* { i n f } _ { \psi } f _ { P } ( \psi ) .
$$  

By the Neyman-Pearson lemma, the optimal test is the likelihood ratio test. Henceforth, the optimal rejection region is,  

$$
R = \left\{ ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) | \mathbb { P } _ { \mathcal { H } _ { 1 } } ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) \geq \eta \cdot \mathbb { P } _ { \mathcal { H } _ { 0 } } ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) \right\} .
$$  

The rejection rule is equivalent to,  

$$
\prod _ { t = 1 } ^ { n } \mathbf { 1 } _ { \{ \omega _ { t } \in \zeta _ { t } \} } \cdot \frac { 1 } { \gamma \cdot m } \cdot \frac { 1 } { \binom { m } { \gamma \cdot m } } \geq \eta \big ( \frac { 1 } { \binom { m } { \gamma \cdot m } } \cdot \frac { 1 } { m } \big ) ^ { n } \Leftrightarrow \prod _ { t = 1 } ^ { n } \mathbf { 1 } _ { \big ( \omega _ { t } \in \zeta _ { t } \big ) } \geq \eta \gamma ^ { n } .
$$  

Therefore, the optimal rejection rule should be $\{(\omega_{1:n},\zeta_{1:n})|\sum_{i=1}^{n}Y_{t}=n\}$. We note that choosing any threshold $\gamma_{n}\leq n$ leads to the same type II error, $\operatorname{soinf}_{\psi}f(\psi)=f(\psi_{1}^{'})$. The proves (a).  

Together it completes the proof of Theorem 11.  

# D.5 Proof of Theorem 12  

It is straightforward to derive the distribution of $Y_t$ under $\mathcal{H}_0$. Next, we consider $\mathcal{H}_1$.  

Note that $TV(\omega_t|\zeta_t, S(\mathbf{P}_t,\zeta_t)) \leq 1 - \theta$, with $\theta \geq 1/2$. So we can use the TV distance to derive $\mathbb{P}_{\mathcal{H}_1}(Y_t = 1)$. That is,  

$$
\begin{array} { r l } & { 1 - \theta \geq T V ( \omega _ { t } | \zeta _ { t } , \mathcal { S } ( \mathbf { P } _ { t } , \zeta _ { t } ) ) = \frac { 1 } { 2 } \sum _ { i = 1 } ^ { N } | p _ { i } \frac { \mathbf { 1 } \{ i \in D \} } { \sum _ { i \in D } p _ { i } } - q _ { i } | = \frac { 1 } { 2 } \sum _ { i \in D } | \frac { p _ { i } } { \sum _ { i \in D } p _ { i } } - q _ { i } | + \frac { 1 } { 2 } \sum _ { i \notin D } q _ { i } } \\ & { = \frac { 1 } { 2 } \sum _ { i \in D } | \frac { p _ { i } } { \sum _ { i \in D } p _ { i } } - q _ { i } | + \frac { 1 } { 2 } ( 1 - \sum _ { i \in D } q _ { i } ) = \frac { 1 } { 2 } + \frac { 1 } { 2 } \sum _ { i \in D } ( | \frac { p _ { i } } { \sum _ { i \in D } p _ { i } } - q _ { i } | - q _ { i } ) } \\ & { \geq \frac { 1 } { 2 } + \frac { 1 } { 2 } \sum _ { i \in D } ( \frac { p _ { i } } { \sum _ { i \in D } p _ { i } } - q _ { i } - q _ { i } ) = 1 - \sum _ { i \in D } q _ { i } . } \end{array}
$$  

Therefore, for each $i$, $q_{ij}$ satisfies that $\sum_{j\in D_i}q_{ij}\geq\theta$. Consequently,  

$$
\mathbb { P } ( Y _ { t } = 1 ) = \sum _ { i = 1 } ^ { \gamma \cdot m } \frac { 1 } { \binom { m } { \gamma \cdot m } } \sum _ { j \in D _ { i } } q _ { i , j } \geq \theta .
$$  

Accordingly, we have,  

$$
\begin{array} { r } { \mathbb { P } ( Y _ { t } = 0 ) = 1 - \mathbb { P } ( Y _ { t } = 1 ) \leq 1 - \theta . } \end{array}
$$  

This completes the proof of Theorem 12.  

# D.6 Proof of Theorem 13  

Denote the rejection region as $R=\left\{\left(\omega_{1:n}, \zeta_{1:n}\right): \sum_{t=1}^{n} Y_{t} \geq x\right\}$. Then type I error $=\mathbb{P}_{\mathcal{H}_{0}}(R)$, and type II error $=\mathbb{P}_{\mathcal{H}_{1}}(R^{c})$. The distribution of $Y_{t}$ under $\mathcal{H}_{0}$ and $\mathcal{H}_{1}$ is:  

$$
\begin{array} { r l } & { \mathbb { P } _ { \mathcal { H } _ { 0 } } ( Y _ { t } = 1 ) = \gamma , \quad \mathbb { P } _ { \mathcal { H } _ { 0 } } ( Y _ { t } = 0 ) = 1 - \gamma , } \\ & { \mathbb { P } _ { \mathcal { H } _ { 1 } } ( Y _ { t } = 1 ) = \theta , \quad \mathbb { P } _ { \mathcal { H } _ { 0 } } ( Y _ { t } = 0 ) = 1 - \theta . } \end{array}
$$  

Then the type I and type II errors can be calculated as,  

$$
\begin{array} { r } { \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \sum _ { t = 1 } ^ { n } Y _ { t } \geq x \right) = \sum _ { i = x } ^ { n } \binom { n } { i } \gamma ^ { i } \cdot ( 1 - \gamma ) ^ { n - i } } \\ { \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( \sum _ { t = 1 } ^ { n } Y _ { t } \leq x - 1 \right) = \sum _ { i = 0 } ^ { x - 1 } \binom { n } { i } \theta ^ { i } \cdot ( 1 - \theta ) ^ { n - i } . } \end{array}
$$  

So the optimal threshold can be represented by  

$$
x = \arg \operatorname* { m i n } _ { x \in [ 0 , n ] } \sum _ { i = x } ^ { n } { \binom { n } { i } } \gamma ^ { i } \cdot ( 1 - \gamma ) ^ { n - i } + \sum _ { i = 0 } ^ { x - 1 } { \binom { n } { i } } \theta ^ { i } \cdot ( 1 - \theta ) ^ { n - i } .
$$  

Denote $F(x):=\sum_{i=x}^{n}\binom{n}{i}\gamma^{i}\cdot(1-\gamma)^{n-i}+\sum_{i=0}^{x-1}\binom{n}{i}\theta^{i}\cdot(1-\theta)^{n-i}$. Then,  

$$
F ( x + 1 ) - F ( x ) = \binom { n } { x } \theta ^ { x } ( 1 - \theta ) ^ { n - x } - \binom { n } { x } \gamma ^ { x } ( 1 - \gamma ) ^ { n - x } .
$$  

Since  

$$
F ( x + 1 ) - F ( x ) \geq 0 \Leftrightarrow x \geq \left\lceil n \cdot { \frac { \log ( 1 - \gamma ) - \log ( 1 - \theta ) } { \log \theta + \log ( 1 - \gamma ) - \log \gamma - \log ( 1 - \theta ) } } \right\rceil ,
$$  

the optimal threshold is  

$$
x = \left[ n \cdot { \frac { \log ( 1 - \gamma ) - \log ( 1 - \theta ) } { \log \theta + \log ( 1 - \gamma ) - \log \gamma - \log ( 1 - \theta ) } } \right] .
$$  

This completes the proof of Theorem 13.  

# D.7 Proof of Theorem 14  

For (b), consider the case that for each $t$, the NTP distribution is $\mathbf{P}:=(1/m,1/m,\cdots,1/m)$. Then $\mathcal{S}(\mathbf{P},\zeta)=\left(\mathbf{1}\{1\in\zeta\}\cdot\frac{1}{\gamma\cdot m},\mathbf{1}\{2\in\zeta\}\cdot\frac{1}{\gamma\cdot m},\cdots,\mathbf{1}\{m\in\zeta\}\cdot\frac{1}{\gamma\cdot m}\right)$. Consider the case that $\mathbf{Q}=(q_1,\cdots,q_m),q_i=\mathbf{1}\{i\in\zeta\}\frac{1+\theta}{2\gamma\cdot m}+1\{i\notin\zeta\}\frac{1-\theta}{2(1-\gamma)\cdot m}$. Denote the asymptotic sum of type I error and type II error under $\mathbf{P}$ and $\mathbf{Q}$ as $f_{\mathbf{P},\mathbf{Q}}(\psi)$. Then we have  

$$
\operatorname* { i n f } _ { \psi } f ( \psi ) \geq \operatorname* { i n f } _ { \psi } f _ { \mathbf { P } , \mathbf { Q } } ( \psi ) .
$$  

We next compute the likelihood of $(\omega_t, \zeta_t)_{1:n}$ under $\mathcal{H}_0$ and $\mathcal{H}_1$ as,  

$$
\begin{array} { r l } & { \mathbb { P } _ { \mathcal { H } _ { 0 } } ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) = \left( \frac { 1 } { \binom { m } { \gamma \cdot m } } \cdot \frac { 1 } { m } \right) ^ { n } } \\ & { \mathbb { P } _ { \mathcal { H } _ { 1 } } ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) = \prod _ { t = 1 } ^ { n } \left( 1 \{ \omega _ { t } \in \zeta _ { t } \} \cdot \frac { 1 + \theta } { 2 \gamma \cdot m } + 1 \{ \omega _ { t } \notin \zeta _ { t } \} \cdot \frac { 1 - \theta } { 2 ( 1 - \gamma ) \cdot m } \right) \cdot \frac { 1 } { \binom { m } { \gamma \cdot m } } . } \end{array}
$$  

By Lemma 3, the optimal test is to reject the null hypothesis when $\mathbb{P}_{\mathcal{H}_0}(\omega_{1:n}, \zeta_{1:n}) \leq \mathbb{P}_{\mathcal{H}_1}(\omega_{1:n}, \zeta_{1:n})$. So the rejection region is,  

$$
R = \{ ( \omega _ { 1 : n } , \zeta _ { 1 : n } ) | \prod _ { t = 1 } ^ { n } ( 1 \{ \omega _ { t } \in \zeta _ { t } \} \cdot \frac { 1 + \theta } { 2 \gamma } + \mathbf { 1 } \{ \omega _ { t } \notin \zeta _ { t } \} \cdot \frac { 1 - \theta } { 2 ( 1 - \gamma ) } ) \geq 1 \} .
$$  

Henceforth, we have proved that $\inf_{\psi} f(\psi) \geq f(\psi_0)$. By definition, we have $f(\psi_0) \geq \inf_{\psi} f(\psi)$. This proves (b).  

For (a), consider the same P and Q, under which, the optimal rejection region is,  

$$
R = \left\{ \prod _ { t = 1 } ^ { n } ( 1 ( \omega _ { t } \in \zeta _ { t } ) \cdot \frac { 1 + \theta } { 2 \gamma m } + \mathbf { 1 } ( \omega _ { t } \notin \zeta _ { t } ) \frac { 1 - \theta } { 2 ( 1 - \gamma ) m } ) \geq \eta \cdot ( \frac { 1 } { m } ) ^ { n } \right\} ,
$$  

where $\eta$ is a constant to keep the type I error at $\alpha$. This rejection rule is equivalent to the rejection rule that $\sum_{t=1}^{n} Y_{t} \geq \eta^{\prime}$ for some $\eta^{\prime}$. This proves (a).  

Together, it completes the proof of Theorem 14.  

# E Additional Numerical Studies  

# E.1 Derivation of the optimal threshold for a general score function  

For a general score function $h$, we determine the optimal threshold $\gamma_n$, such that we minimize the sum of type I and type II errors, i.e.,  

$$
\operatorname* { m i n } _ { \gamma _ { n } } \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \sum _ { i = 1 } ^ { n } h ( Y _ { t } ) \geq \gamma _ { n } \right) + \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( \sum _ { i = 1 } ^ { n } h ( Y _ { t } ) < \gamma _ { n } \right) .
$$  

By the large deviation theory, the sum of type I and type II errors has a representation that is nearly tight when $n \to \infty$, i.e.,  

$$
\begin{array} { r l } & { \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \sum _ { i = 1 } ^ { n } h ( Y _ { t } ) \geq \gamma _ { n } \right) + \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( \sum _ { i = 1 } ^ { n } h ( Y _ { t } ) < \gamma _ { n } \right) } \\ & { \geq \operatorname* { i n f } _ { \theta _ { 1 } , \theta _ { 2 } } \frac { \exp \{ n \cdot \log \mathbb { E } _ { 0 } \exp \theta _ { 1 } h ( Y ) \} } { \exp ( \theta _ { 1 } \gamma ) } + \exp ( \theta _ { 2 } \gamma ) \cdot \exp \{ n \log \mathbb { E } _ { 1 } \exp - \theta _ { 2 } h ( Y ) \} . } \end{array}
$$  

Note that, given $\theta_{1}, \theta_{2}$, choosing  

$$
\gamma _ { n } = \frac { 1 } { \theta _ { 1 } + \theta _ { 2 } } \log \frac { \theta _ { 1 } \exp \{ n \cdot \log \mathbb { E } _ { 0 } \exp \theta _ { 1 } h ( Y ) \} } { \theta _ { 2 } \exp [ n \cdot \log \mathbb { E } _ { 1 } \exp \{ - \theta _ { 2 } h ( Y ) \} ] }
$$  

can minimize the sum of type I and type II errors. Therefore,  

$$
\begin{array} { r l } & { \operatorname* { i n f } _ { \gamma _ { n } } \mathbb { P } _ { \mathcal { H } _ { 0 } } \left( \sum _ { i = 1 } ^ { n } h ( Y _ { t } ) \geq \gamma _ { n } \right) + \mathbb { P } _ { \mathcal { H } _ { 1 } } \left( \sum _ { i = 1 } ^ { n } h ( Y _ { t } ) < \gamma _ { n } \right) } \\ & { \geq \operatorname* { i n f } _ { \theta _ { 1 } , \theta _ { 2 } } \exp \left( \frac { \theta _ { 2 } } { \theta _ { 1 } + \theta _ { 2 } } \log \mathbb { E } _ { 0 } \exp \theta _ { 1 } h ( Y ) \right) \cdot \exp \left( \frac { \theta _ { 1 } } { \theta _ { 1 } + \theta _ { 2 } } \log \mathbb { E } _ { 1 } \exp \{ - \theta _ { 2 } h ( Y ) \} \right) } \\ & { = \exp \left( \operatorname* { i n f } _ { \theta _ { 1 } , \theta _ { 2 } } \frac { \theta _ { 2 } } { \theta _ { 1 } + \theta _ { 2 } } \log \mathbb { E } _ { 0 } \exp \{ \theta _ { 1 } h ( Y ) \} + \frac { \theta _ { 1 } } { \theta _ { 1 } + \theta _ { 2 } } \log \mathbb { E } _ { 1 } \exp \{ - \theta _ { 2 } h ( Y ) \} \right) . } \end{array}
$$  

We then calculate the value of $\gamma_{n}$ using  

$$
\gamma _ { n } = \frac { 1 } { \theta _ { 1 } ^ { * } + \theta _ { 2 } ^ { * } } \left( \log \frac { \theta _ { 1 } ^ { * } } { \theta _ { 2 } ^ { * } } + n \log \mathbb { E } _ { 0 } \exp \{ \theta _ { 1 } ^ { * } h ( Y ) \} - n \log \mathbb { E } _ { 1 } \exp \{ - \theta _ { 2 } ^ { * } h ( Y ) \} \right) .
$$  

Since our deviation is tight only when $n$ is large enough, in our simulation, for convenience, we set  

$$
\hat { \gamma } _ { n } = \frac { n } { \theta _ { 1 } ^ { * } + \theta _ { 2 } ^ { * } } \left( \log \mathbb { E } _ { 0 } \exp \{ \theta _ { 1 } ^ { * } h ( Y ) \} - \log \mathbb { E } _ { 1 } \exp \{ - \theta _ { 2 } ^ { * } h ( Y ) \} \right) .
$$  

# E.2 Additional numerical results for Gumbel-max watermark  

We report additional simulation results for the Gumbel-max watermark, adopting the same setup as in Section 6.1, but considering different working values for $\Delta$ and $\theta$. Figure 6 reports the average type I and type II errors versus the text length, based on 5000 replications, under the setting of fixed type I error, where $\Delta = 0.01$ and $\theta = \{0.7, 0.8, 0.9, 0.95\}$. Figures 7 and fig:gm-sum-d10 report the average sum of type I and type II errors versus text length, based on 5,000 repetitions, under the setting of minimizing the sum of two errors, where $\Delta = 0.005$ and $\Delta = 0.01$, respectively, while $\theta = \{0.7, 0.8, 0.9, 0.95\}$. We have observed that our proposed tests remain effective and robust as long as $\Delta$ and $\theta$ are chosen within a reasonable range.  

![](http://61.160.97.222:59003/kb-paper-images/arxiv-2025-2501.02441/ed98953cd57cf0238dee6f3db9059f70221d641dc2f671b31ab37a839e4a1347.png)  
Figure 6: Average type I and type II errors versus text length for the Gumbel-max watermark under the setting of fixed type I error, where $\Delta = 0.01$ and $\theta = \{0.7, 0.8, 0.9, 0.95\}$.  

![](http://61.160.97.222:59003/kb-paper-images/arxiv-2025-2501.02441/82a52a942a4ce8774b342bb308695d1542b1b521838a5ec0993674984561c508.png)  
Figure 7: Average sum of type I and type II errors versus text length for the Gumbel-max watermark under the setting of minimizing the sum of type I and type II errors, where $\Delta = 0.005$ and $\theta = \{0.7, 0.8, 0.9, 0.95\}$.  

![](http://61.160.97.222:59003/kb-paper-images/arxiv-2025-2501.02441/4d4041586cf491f0f406e8fd28007d44e02b4adca2c56cc55abc75933df6ba65.png)  
Figure 8: Average sum of type I and type II errors versus text length for the Gumbel-max watermark under the setting of minimizing the sum of type I and type II errors, where $\Delta = 0.01$ and $\theta = \{0.7, 0.8, 0.9, 0.95\}$.  

# E.3 Additional numerical results for red-green-list watermark  

We report additional simulation results for the red-green-list watermark, adopting the same setup as in Section 6.2, but considering different true value for $\theta^{*}=\{0.6,0.7\}$ with the working value of $\theta=\{0.7,0.8,0.9,0.95\}$. Figure E.3, left panel, reports the average type I and type II errors versus the text length, based on 5000 replications, under the setting of fixed type I error. Figure E.3, right panel, reports the average sum of type I and type II errors versus the text length, based on 5,000 replications, under the setting of minimizing the sum of type I and type II errors. Again, we have observed that our proposed tests remain effective and robust as long as $\theta$ is chosen within a reasonable range.  

![](http://61.160.97.222:59003/kb-paper-images/arxiv-2025-2501.02441/e72d310fb2119f8e5c6aa1fb4555e721a669ad6529e008d5591d3839b731e548.png)  
Figure 9: Left panel: average type I and type II errors versus text length for the red-green-list watermark under the setting of fixed type I error; right panel: average sum of type I and type II errors versus text length for the red-green-list watermark under the setting of minimizing the sum of type I and type II errors, where $\theta^* = \{0.6, 0.7\}$ and $\theta = \{0.7, 0.8, 0.9, 0.95\}$.  